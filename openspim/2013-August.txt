From edgar.escobar.nieto at ipt.fraunhofer.de  Mon Aug  5 04:55:08 2013
From: edgar.escobar.nieto at ipt.fraunhofer.de (edgar.escobar.nieto at ipt.fraunhofer.de)
Date: Mon, 5 Aug 2013 11:55:08 +0200
Subject: [OpenSPIM] About the calibration of the lightsheet
Message-ID: <OF43BC6A67.1EC5993C-ONC1257BBE.00367CAB-C1257BBE.00367CAD@ipt.rwth-aachen.de>

 
 _________________________________________________________________________

Fraunhofer-Institut f?r Produktionstechnologie IPT 
Edgar Escobar Nieto  
 

 
 
Steinbachstra?e 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de 
http://www.ipt.fraunhofer.de
_________________________________________________________________________
 
 
 


-----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 05.08.2013 11:54 -----
An: openspim at openspim.org
Von: Edgar Escobar Nieto/Fraunhofer IPT
Datum: 31.07.2013 15:02
Betreff: (Unbenannt)

 Hi dear all,

I reached to the point where I need to calibrate the light-sheet. And I was wondering
what is the optical density of the ND filter recommended to perform the calibration.

I would like to know also what is the Frequency (lp/mm) recommended for the Opal Glass Ronchi Ruling Slides.
I think that high a frequency would be better, but there must be a limit.

Only to be sure, what I understood is that these Ruling Slides have already a reflective surface, so there is no
need to glue a mirror to the ruling slides.

Thank you in advance for your comments.

Kind regards,
Edgar Escobar Nieto
 _________________________________________________________________________

Fraunhofer-Institut f?r Produktionstechnologie IPT 
Edgar Escobar Nieto  
 

 
 
Steinbachstra?e 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de 
http://www.ipt.fraunhofer.de
_________________________________________________________________________
 
 
 
 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130805/91fdefea/attachment.html>

From edgar.escobar.nieto at ipt.fraunhofer.de  Tue Aug  6 10:08:39 2013
From: edgar.escobar.nieto at ipt.fraunhofer.de (edgar.escobar.nieto at ipt.fraunhofer.de)
Date: Tue, 6 Aug 2013 17:08:39 +0200
Subject: [OpenSPIM] About the calibration of the lightsheet
Message-ID: <OF15A84A94.5A9526D4-ONC1257BBF.005330CF-C1257BBF.005330D2@ipt.rwth-aachen.de>

 Hi dear All,

I would like to know how you could cut the ruling slides. I don't know if it is possible tu cut a thin part of glass using
a conventional glass cutter. 

Kind regards,
Edgar Escobar Nieto
 _________________________________________________________________________

Fraunhofer-Institut f?r Produktionstechnologie IPT 
Edgar Escobar Nieto  
 

 
 
Steinbachstra?e 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de 
http://www.ipt.fraunhofer.de
_________________________________________________________________________
 
 
 


-----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 06.08.2013 17:01 -----
An: openspim at openspim.org
Von: Edgar Escobar Nieto/Fraunhofer IPT
Datum: 05.08.2013 11:55
Betreff: About the calibration of the lightsheet

 
 _________________________________________________________________________

Fraunhofer-Institut f?r Produktionstechnologie IPT 
Edgar Escobar Nieto  
 

 
 
Steinbachstra?e 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de 
http://www.ipt.fraunhofer.de
_________________________________________________________________________
 
 
 


-----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 05.08.2013 11:54 -----
An: openspim at openspim.org
Von: Edgar Escobar Nieto/Fraunhofer IPT
Datum: 31.07.2013 15:02
Betreff: (Unbenannt)

 Hi dear all,

I reached to the point where I need to calibrate the light-sheet. And I was wondering
what is the optical density of the ND filter recommended to perform the calibration.

I would like to know also what is the Frequency (lp/mm) recommended for the Opal Glass Ronchi Ruling Slides.
I think that high a frequency would be better, but there must be a limit.

Only to be sure, what I understood is that these Ruling Slides have already a reflective surface, so there is no
need to glue a mirror to the ruling slides.

Thank you in advance for your comments.

Kind regards,
Edgar Escobar Nieto
 _________________________________________________________________________

Fraunhofer-Institut f?r Produktionstechnologie IPT 
Edgar Escobar Nieto  
 

 
 
Steinbachstra?e 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de 
http://www.ipt.fraunhofer.de
_________________________________________________________________________
 
 
 
 
 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130806/5dba57fd/attachment.html>

From pitrone at mpi-cbg.de  Tue Aug  6 11:22:13 2013
From: pitrone at mpi-cbg.de (Peter Gabriel Pitrone)
Date: Tue, 6 Aug 2013 18:22:13 +0200 (CEST)
Subject: [OpenSPIM] About the calibration of the lightsheet
In-Reply-To: <OF15A84A94.5A9526D4-ONC1257BBF.005330CF-C1257BBF.005330D2@ipt.rwth-aa
	chen.de>
References: <OF15A84A94.5A9526D4-ONC1257BBF.005330CF-C1257BBF.005330D2@ipt.rwth-aachen.de>
Message-ID: <60102.174.24.0.131.1375806133.squirrel@webmail.mpi-cbg.de>

Hello Edgar,

I'm sorry I didn't answer you till now, but I'm sometimes away from my
email as I am on vacation in the states.

To answer your questions, we broke the slide and found the best shard. It
is definitely not the most elegant way, but it works for us. If you want
to go about making it with more consistency with a glass or window
cutter, please write a protocol on our wiki on how to do it so others can
reproduce it.

If you need help with aligning the light sheet, we can make a skype
session tomorrow... just remember I have an 8 hour difference in
timezones. My Skype name is "petepitrone". send me an email as to the
time you would like to talk.

Pete

-- 
Peter Gabriel Pitrone - TechRMS
Microscopy/Imaging Specialist
Prof. Dr. Pavel Tomancak group
Max Planck Institute for
Molecular Biology and Genetics
Pfotenhauerstr. 108
01307 Dresden

"If a straight line fit is required, obtain only two data points." - Anon.


On Tue, August 6, 2013 5:08 pm, edgar.escobar.nieto at ipt.fraunhofer.de wrote:
<|>  Hi dear All,
<|>
<|> I would like to know how you could cut the ruling slides. I don't
know if
<|> it is possible tu cut a thin part of glass using
<|> a conventional glass cutter.
<|>
<|> Kind regards,
<|> Edgar Escobar Nieto
<|> 
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut f?r Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstra?e 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> -----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 06.08.2013
<|> 17:01 -----
<|> An: openspim at openspim.org
<|> Von: Edgar Escobar Nieto/Fraunhofer IPT
<|> Datum: 05.08.2013 11:55
<|> Betreff: About the calibration of the lightsheet
<|>
<|>
<|> 
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut f?r Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstra?e 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> -----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 05.08.2013
<|> 11:54 -----
<|> An: openspim at openspim.org
<|> Von: Edgar Escobar Nieto/Fraunhofer IPT
<|> Datum: 31.07.2013 15:02
<|> Betreff: (Unbenannt)
<|>
<|>  Hi dear all,
<|>
<|> I reached to the point where I need to calibrate the light-sheet. And I
<|> was wondering
<|> what is the optical density of the ND filter recommended to perform the
<|> calibration.
<|>
<|> I would like to know also what is the Frequency (lp/mm) recommended for
<|> the Opal Glass Ronchi Ruling Slides.
<|> I think that high a frequency would be better, but there must be a
limit.
<|>
<|> Only to be sure, what I understood is that these Ruling Slides have
<|> already a reflective surface, so there is no
<|> need to glue a mirror to the ruling slides.
<|>
<|> Thank you in advance for your comments.
<|>
<|> Kind regards,
<|> Edgar Escobar Nieto
<|> 
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut f?r Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstra?e 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> _______________________________________________
<|> OpenSPIM mailing list
<|> OpenSPIM at openspim.org
<|> http://openspim.org/mailman/listinfo/openspim
<|>




From huisken at mpi-cbg.de  Tue Aug  6 16:20:58 2013
From: huisken at mpi-cbg.de (Jan Huisken)
Date: Tue, 6 Aug 2013 23:20:58 +0200
Subject: [OpenSPIM] About the calibration of the lightsheet
In-Reply-To: <OF43BC6A67.1EC5993C-ONC1257BBE.00367CAB-C1257BBE.00367CAD@ipt.rwth-aachen.de>
References: <OF43BC6A67.1EC5993C-ONC1257BBE.00367CAB-C1257BBE.00367CAD@ipt.rwth-aachen.de>
Message-ID: <56A4F19F-C885-4041-BF78-BECDA36614E9@mpi-cbg.de>

Hi Edgar

> -----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 05.08.2013 11:54 -----
> An: openspim at openspim.org
> Von: Edgar Escobar Nieto/Fraunhofer IPT
> Datum: 31.07.2013 15:02
> Betreff: (Unbenannt)
> 
> Hi dear all,
> 
> I reached to the point where I need to calibrate the light-sheet. And I was wondering
> what is the optical density of the ND filter recommended to perform the calibration.

Since you are reflecting the laser right onto the camera you need to be sure that you can turn down the laser intensity far enough that you do not damage your camera. Therefore the ND depends on the lowest power you can set your laser to and the sensitivity of your camera. The ND filter can be introduced in the illumination or detection path. Some fluorescence emission filter may also do the job.

> 
> I would like to know also what is the Frequency (lp/mm) recommended for the Opal Glass Ronchi Ruling Slides.
> I think that high a frequency would be better, but there must be a limit.

The grid is used to (1) have some features on which you can focus and (2) to have some reflective parts the give you a profile of the beam. The distance does not matter very much. It is usually good if you have ca. 10 reflective lines across the entire FOV, which depends on your magnification, camera, etc. 
In addition it is nice if you also have an adjacent region on this piece of glass that is fully reflective where you can inspect the full beam profile (after focusing on it with the grid part). Here you can align for sheet thickness and uniformity.

> 
> Only to be sure, what I understood is that these Ruling Slides have already a reflective surface, so there is no
> need to glue a mirror to the ruling slides.

No. The whole piece needs to be fairly thin to fit between the lenses. So you would have to cut out a small piece anyway. Cutting is not so easy but the cut does not need to be precise.  Just smash it into pieces and pick the best piece ;-)

> 
> Thank you in advance for your comments.

Sure.

Best
Jan

Dr. Jan Huisken
MPI of Molecular Cell Biology and Genetics
Pfotenhauerstr. 108, 01307 Dresden, Germany

> 
> Kind regards,
> Edgar Escobar Nieto
> _________________________________________________________________________
> 
> Fraunhofer-Institut f?r Produktionstechnologie IPT 
> Edgar Escobar Nieto 
> 
> 
> 
> 
> Steinbachstra?e 17 
> 52074 Aachen 
> 
> edgar.escobar.nieto at ipt.fraunhofer.de 
> http://www.ipt.fraunhofer.de 
> _________________________________________________________________________ 
> 
> 
> 
> _______________________________________________
> OpenSPIM mailing list
> OpenSPIM at openspim.org
> http://openspim.org/mailman/listinfo/openspim

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130806/c9e091d8/attachment.html>

From edgar.escobar.nieto at ipt.fraunhofer.de  Wed Aug  7 05:13:39 2013
From: edgar.escobar.nieto at ipt.fraunhofer.de (edgar.escobar.nieto at ipt.fraunhofer.de)
Date: Wed, 7 Aug 2013 12:13:39 +0200
Subject: [OpenSPIM] About the calibration of the lightsheet
In-Reply-To: <60102.174.24.0.131.1375806133.squirrel@webmail.mpi-cbg.de>
References: <60102.174.24.0.131.1375806133.squirrel@webmail.mpi-cbg.de>,
	<OF15A84A94.5A9526D4-ONC1257BBF.005330CF-C1257BBF.005330D2@ipt.rwth-aachen.de>
Message-ID: <OF93E58D03.96F977B9-ONC1257BC0.00382E93-C1257BC0.00382E9A@ipt.rwth-aachen.de>

 Hi dear Peter,

I have not yet bought the Ronchi Ruler Slide, but I was thinking of use a different
material in order to perform the alignment of the lighstsheet. I was thinking of use
some fluorescence samples that are mounted in a glass http://www.bw-optik.de/laufband/fluoreszenz.php
and since I have already some of this mounted glass samples available I will try to make a similar procedure as the stated in the OpenSPIM wiki.
If that method doesn't work I will have to buy the Ronchi Ruler Slide anyway.

The think is that since the material is also glass, would be the same to cut it or to cut the Ronchi Ruler Slide. That's
why I was asking how the Ruler Slide was cut. I will try to find a way to cut the glass, the problem
is that only a thin piece, let's say from 1 to 2 mm wide is needed. I know that would be so difficul to perform a cut
of this magnitude using a conventional glass cutter, but I'll give a try, if I am successful I will write a protocol to the OpenSPIM wiki.

I think until I had all the necessary material ready and mounted and if I have troubles with the alignment of the lightsheet,
that would be nice to have the Skype session.

Thanks for your kind answer and for offering me your support. Enjoy your holidays in the U.S.

Kind regards,
Edgar
 _________________________________________________________________________

Fraunhofer-Institut f?r Produktionstechnologie IPT 
Edgar Escobar Nieto  
 

 
 
Steinbachstra?e 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de 
http://www.ipt.fraunhofer.de
_________________________________________________________________________
 
 
 


-----"Peter Gabriel Pitrone" <pitrone at mpi-cbg.de> schrieb: -----
An: edgar.escobar.nieto at ipt.fraunhofer.de
Von: "Peter Gabriel Pitrone" <pitrone at mpi-cbg.de>
Datum: 06.08.2013 18:22
Kopie: openspim at openspim.org
Betreff: Re: [OpenSPIM] About the calibration of the lightsheet

Hello Edgar,

I'm sorry I didn't answer you till now, but I'm sometimes away from my
email as I am on vacation in the states.

To answer your questions, we broke the slide and found the best shard. It
is definitely not the most elegant way, but it works for us. If you want
to go about making it with more consistency with a glass or window
cutter, please write a protocol on our wiki on how to do it so others can
reproduce it.

If you need help with aligning the light sheet, we can make a skype
session tomorrow... just remember I have an 8 hour difference in
timezones. My Skype name is "petepitrone". send me an email as to the
time you would like to talk.

Pete

-- 
Peter Gabriel Pitrone - TechRMS
Microscopy/Imaging Specialist
Prof. Dr. Pavel Tomancak group
Max Planck Institute for
Molecular Biology and Genetics
Pfotenhauerstr. 108
01307 Dresden

"If a straight line fit is required, obtain only two data points." - Anon.


On Tue, August 6, 2013 5:08 pm, edgar.escobar.nieto at ipt.fraunhofer.de wrote:
<|> ?Hi dear All,
<|>
<|> I would like to know how you could cut the ruling slides. I don't
know if
<|> it is possible tu cut a thin part of glass using
<|> a conventional glass cutter.
<|>
<|> Kind regards,
<|> Edgar Escobar Nieto
<|> 
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut f?r Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstra?e 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> -----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 06.08.2013
<|> 17:01 -----
<|> An: openspim at openspim.org
<|> Von: Edgar Escobar Nieto/Fraunhofer IPT
<|> Datum: 05.08.2013 11:55
<|> Betreff: About the calibration of the lightsheet
<|>
<|>
<|> 
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut f?r Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstra?e 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> -----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 05.08.2013
<|> 11:54 -----
<|> An: openspim at openspim.org
<|> Von: Edgar Escobar Nieto/Fraunhofer IPT
<|> Datum: 31.07.2013 15:02
<|> Betreff: (Unbenannt)
<|>
<|> ?Hi dear all,
<|>
<|> I reached to the point where I need to calibrate the light-sheet. And I
<|> was wondering
<|> what is the optical density of the ND filter recommended to perform the
<|> calibration.
<|>
<|> I would like to know also what is the Frequency (lp/mm) recommended for
<|> the Opal Glass Ronchi Ruling Slides.
<|> I think that high a frequency would be better, but there must be a
limit.
<|>
<|> Only to be sure, what I understood is that these Ruling Slides have
<|> already a reflective surface, so there is no
<|> need to glue a mirror to the ruling slides.
<|>
<|> Thank you in advance for your comments.
<|>
<|> Kind regards,
<|> Edgar Escobar Nieto
<|> 
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut f?r Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstra?e 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> _______________________________________________
<|> OpenSPIM mailing list
<|> OpenSPIM at openspim.org
<|> http://openspim.org/mailman/listinfo/openspim
<|>


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130807/c1de6f06/attachment-0001.html>

From edgar.escobar.nieto at ipt.fraunhofer.de  Wed Aug  7 05:48:03 2013
From: edgar.escobar.nieto at ipt.fraunhofer.de (edgar.escobar.nieto at ipt.fraunhofer.de)
Date: Wed, 7 Aug 2013 12:48:03 +0200
Subject: [OpenSPIM] About the calibration of the lightsheet
In-Reply-To: <56A4F19F-C885-4041-BF78-BECDA36614E9@mpi-cbg.de>
References: <56A4F19F-C885-4041-BF78-BECDA36614E9@mpi-cbg.de>,
	<OF43BC6A67.1EC5993C-ONC1257BBE.00367CAB-C1257BBE.00367CAD@ipt.rwth-aachen.de>
Message-ID: <OF6662349C.05F700AE-ONC1257BC0.003B54EB-C1257BC0.003B54F3@ipt.rwth-aachen.de>

 Dear Jan,

Thanks for your comments. I have already available some ND filters, so I will set the laser
power to the minimum and make some tests. I will determine my FOV and then to buy
the Ruler Slides accordingly if necessary. I think I will try first the method that Jan Krieger
and Alexis Maizel used with a scratched little mirror, (seems to be cheaper and very good too).

Kind regards,
Edgar
 _________________________________________________________________________

Fraunhofer-Institut f?r Produktionstechnologie IPT 
Edgar Escobar Nieto  
 

 
 
Steinbachstra?e 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de 
http://www.ipt.fraunhofer.de
_________________________________________________________________________
 
 
 


-----Jan Huisken <huisken at mpi-cbg.de> schrieb: -----
An: edgar.escobar.nieto at ipt.fraunhofer.de
Von: Jan Huisken <huisken at mpi-cbg.de>
Datum: 06.08.2013 23:21
Kopie: openspim at openspim.org
Betreff: Re: [OpenSPIM] About the calibration of the lightsheet

Hi Edgar

-----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 05.08.2013 11:54 ----- 
 An: openspim at openspim.org
Von: Edgar Escobar Nieto/Fraunhofer IPT
 Datum: 31.07.2013 15:02
Betreff: (Unbenannt)

  Hi dear all,

I reached to the point where I need to calibrate the light-sheet. And I was wondering
 what is the optical density of the ND filter recommended to perform the calibration.

Since you are reflecting the laser right onto the camera you need to be sure that you can turn down the laser intensity far enough that you do not damage your camera. Therefore the ND depends on the lowest power you can set your laser to and the sensitivity of your camera. The ND filter can be introduced in the illumination or detection path. Some fluorescence emission filter may also do the job.
 
I would like to know also what is the Frequency (lp/mm) recommended for the Opal Glass Ronchi Ruling Slides.
 I think that high a frequency would be better, but there must be a limit.

The grid is used to (1) have some features on which you can focus and (2) to have some reflective parts the give you a profile of the beam. The distance does not matter very much. It is usually good if you have ca. 10 reflective lines across the entire FOV, which depends on your magnification, camera, etc.?
In addition it is nice if you also have an adjacent region on this piece of glass that is fully reflective where you can inspect the full beam profile (after focusing on it with the grid part). Here you can align for sheet thickness and uniformity.
 
Only to be sure, what I understood is that these Ruling Slides have already a reflective surface, so there is no
 need to glue a mirror to the ruling slides.

No. The whole piece needs to be fairly thin to fit between the lenses. So you would have to cut out a small piece anyway. Cutting is not so easy but the cut does not need to be precise. ?Just smash it into pieces and pick the best piece ;-)

Thank you in advance for your comments.

Sure.

Best
Jan

Dr. Jan Huisken
MPI of Molecular Cell?Biology and Genetics
Pfotenhauerstr. 108, 01307 Dresden, Germany
 
Kind regards,
Edgar Escobar Nieto
  _________________________________________________________________________
 
Fraunhofer-Institut f?r Produktionstechnologie IPT 
Edgar Escobar Nieto  
  

 
 
Steinbachstra?e 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de  
 http://www.ipt.fraunhofer.de 
_________________________________________________________________________ 
 
  
 
 
_______________________________________________
OpenSPIM mailing list
OpenSPIM at openspim.org
http://openspim.org/mailman/listinfo/openspim

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130807/c039b851/attachment.html>

From edgar.escobar.nieto at ipt.fraunhofer.de  Wed Aug  7 07:00:14 2013
From: edgar.escobar.nieto at ipt.fraunhofer.de (edgar.escobar.nieto at ipt.fraunhofer.de)
Date: Wed, 7 Aug 2013 14:00:14 +0200
Subject: [OpenSPIM] About the calibration of the lightsheet
In-Reply-To: <4F91222C659D5B4E8C89CE4D710BB08EF1440C5FDB@DKFZEX01.ad.dkfz-heidelberg.de>
References: <4F91222C659D5B4E8C89CE4D710BB08EF1440C5FDB@DKFZEX01.ad.dkfz-heidelberg.de>,
	<60102.174.24.0.131.1375806133.squirrel@webmail.mpi-cbg.de>,
	<OF15A84A94.5A9526D4-ONC1257BBF.005330CF-C1257BBF.005330D2@ipt.rwth-aachen.de>,
	<OF93E58D03.96F977B9-ONC1257BC0.00382E93-C1257BC0.00382E9A@ipt.rwth-aachen.de>
Message-ID: <OF80980D01.3665800B-ONC1257BC0.0041F098-C1257BC0.0041F09F@ipt.rwth-aachen.de>

 Hi Jan,

Thanks a lot for your comments. 
I will search in the lab if there is a little mirror, otherwise I should buy it.
The method that you used is also very clever. That will be the one that I will
use since looks like it is cheaper to buy a little simple mirror.

Kind regards,
Edgar

 _________________________________________________________________________

Fraunhofer-Institut f?r Produktionstechnologie IPT 
Edgar Escobar Nieto  
 

 
 
Steinbachstra?e 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de 
http://www.ipt.fraunhofer.de
_________________________________________________________________________
 
 
 


-----"Krieger, Jan" <j.krieger at Dkfz-Heidelberg.de> schrieb: -----
An: "edgar.escobar.nieto at ipt.fraunhofer.de" <edgar.escobar.nieto at ipt.fraunhofer.de>
Von: "Krieger, Jan" <j.krieger at Dkfz-Heidelberg.de>
Datum: 07.08.2013 12:21
Betreff: AW: [OpenSPIM] About the calibration of the lightsheet

Hi!

we are using a simple but small silver mirror (5x5mm?, 1mm thick, bought from Melles-Griot) on a special mount, but you can glue it to anything that can be mounted inside the SPIM. First we focus the mirror, by illuminating it with a white LED or sinple lamp through the projection objective. The mirror is small enough to also fit into an openSPIM (I did that together with Alexis Maizel two weeks ago). Basically you can focus onto some dirk on the mirror surface, or a skratch that we make into it. Then you can switch to the laser and first bring it to the sharp line on the mirror and finally align it with the lenses of the illumination beam path. I think this is kind of the "classical" method to do that. 

The samples seem to be embedded in glass, right? Then you can focus them, but you won't have a focussed reflective surface then, that you can use to inspect the lightsheet.

Best,
JAN


Dipl.-Phys. Jan Krieger
German Cancer Research Center (dkfz)
Department B040 - Biophysics of Macromolecules (Prof. J. Langowski)
Im Neuenheimer Feld 580
69120 Heidelberg

fon: +49 / 6221 / 42-3395
fax: +49 / 6221 / 42-3391
e-mail: j.krieger at dkfz.de
www: http://www.dkfz.de/Macromol/
________________________________________
Von: openspim-bounces at openspim.org [openspim-bounces at openspim.org] im Auftrag von edgar.escobar.nieto at ipt.fraunhofer.de [edgar.escobar.nieto at ipt.fraunhofer.de]
Gesendet: Mittwoch, 7. August 2013 12:13
An: Peter Gabriel Pitrone
Cc: openspim at openspim.org
Betreff: Re: [OpenSPIM] About the calibration of the lightsheet

Hi dear Peter,

I have not yet bought the Ronchi Ruler Slide, but I was thinking of use a different
material in order to perform the alignment of the lighstsheet. I was thinking of use
some fluorescence samples that are mounted in a glass http://www.bw-optik.de/laufband/fluoreszenz.php
and since I have already some of this mounted glass samples available I will try to make a similar procedure as the stated in the OpenSPIM wiki.
If that method doesn't work I will have to buy the Ronchi Ruler Slide anyway.

The think is that since the material is also glass, would be the same to cut it or to cut the Ronchi Ruler Slide. That's
why I was asking how the Ruler Slide was cut. I will try to find a way to cut the glass, the problem
is that only a thin piece, let's say from 1 to 2 mm wide is needed. I know that would be so difficul to perform a cut
of this magnitude using a conventional glass cutter, but I'll give a try, if I am successful I will write a protocol to the OpenSPIM wiki.

I think until I had all the necessary material ready and mounted and if I have troubles with the alignment of the lightsheet,
that would be nice to have the Skype session.

Thanks for your kind answer and for offering me your support. Enjoy your holidays in the U.S.

Kind regards,
Edgar
_________________________________________________________________________

Fraunhofer-Institut f?r Produktionstechnologie IPT
Edgar Escobar Nieto




Steinbachstra?e 17
52074 Aachen

edgar.escobar.nieto at ipt.fraunhofer.de
http://www.ipt.fraunhofer.de?<http://www.ipt.fraunhofer.de/>
_________________________________________________________________________







-----"Peter Gabriel Pitrone" <pitrone at mpi-cbg.de> schrieb: -----
An: edgar.escobar.nieto at ipt.fraunhofer.de
Von: "Peter Gabriel Pitrone" <pitrone at mpi-cbg.de>
Datum: 06.08.2013 18:22
Kopie: openspim at openspim.org
Betreff: Re: [OpenSPIM] About the calibration of the lightsheet

Hello Edgar,

I'm sorry I didn't answer you till now, but I'm sometimes away from my
email as I am on vacation in the states.

To answer your questions, we broke the slide and found the best shard. It
is definitely not the most elegant way, but it works for us. If you want
to go about making it with more consistency with a glass or window
cutter, please write a protocol on our wiki on how to do it so others can
reproduce it.

If you need help with aligning the light sheet, we can make a skype
session tomorrow... just remember I have an 8 hour difference in
timezones. My Skype name is "petepitrone". send me an email as to the
time you would like to talk.

Pete

--
Peter Gabriel Pitrone - TechRMS
Microscopy/Imaging Specialist
Prof. Dr. Pavel Tomancak group
Max Planck Institute for
Molecular Biology and Genetics
Pfotenhauerstr. 108
01307 Dresden

"If a straight line fit is required, obtain only two data points." - Anon.


On Tue, August 6, 2013 5:08 pm, edgar.escobar.nieto at ipt.fraunhofer.de wrote:
<|> ?Hi dear All,
<|>
<|> I would like to know how you could cut the ruling slides. I don't
know if
<|> it is possible tu cut a thin part of glass using
<|> a conventional glass cutter.
<|>
<|> Kind regards,
<|> Edgar Escobar Nieto
<|>
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut f?r Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstra?e 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> -----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 06.08.2013
<|> 17:01 -----
<|> An: openspim at openspim.org
<|> Von: Edgar Escobar Nieto/Fraunhofer IPT
<|> Datum: 05.08.2013 11:55
<|> Betreff: About the calibration of the lightsheet
<|>
<|>
<|>
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut f?r Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstra?e 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> -----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 05.08.2013
<|> 11:54 -----
<|> An: openspim at openspim.org
<|> Von: Edgar Escobar Nieto/Fraunhofer IPT
<|> Datum: 31.07.2013 15:02
<|> Betreff: (Unbenannt)
<|>
<|> ?Hi dear all,
<|>
<|> I reached to the point where I need to calibrate the light-sheet. And I
<|> was wondering
<|> what is the optical density of the ND filter recommended to perform the
<|> calibration.
<|>
<|> I would like to know also what is the Frequency (lp/mm) recommended for
<|> the Opal Glass Ronchi Ruling Slides.
<|> I think that high a frequency would be better, but there must be a
limit.
<|>
<|> Only to be sure, what I understood is that these Ruling Slides have
<|> already a reflective surface, so there is no
<|> need to glue a mirror to the ruling slides.
<|>
<|> Thank you in advance for your comments.
<|>
<|> Kind regards,
<|> Edgar Escobar Nieto
<|>
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut f?r Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstra?e 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> _______________________________________________
<|> OpenSPIM mailing list
<|> OpenSPIM at openspim.org
<|> http://openspim.org/mailman/listinfo/openspim
<|>


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130807/5a377aa4/attachment-0001.html>

From Alexis.Maizel at cos.uni-heidelberg.de  Tue Aug 13 13:02:55 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Tue, 13 Aug 2013 20:02:55 +0200
Subject: [OpenSPIM] Software hangs during stack capture
In-Reply-To: <24140450-E3E7-4F2F-8163-7ACFCDED68D2@cos.uni-heidelberg.de>
References: <3FEEC96F-6E46-4B4F-8A50-141F6E9E169E@cos.uni-heidelberg.de>
	<alpine.DEB.1.00.1307301757130.24252@s15462909.onlinehome-server.info>
	<24140450-E3E7-4F2F-8163-7ACFCDED68D2@cos.uni-heidelberg.de>
Message-ID: <942DA4AF-230A-43B7-8196-D8B269D08B06@cos.uni-heidelberg.de>

Answering my own post?
well after a few more days and more recordings, it seems that the problem did not appear again. No more hangs during acquisition.
I keep an eye open.

Alexis
 
On 31 Jul 2013, at 09:38, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:

> Hi Johannes & Luke,
>>> I noticed that upon acquisition of a stack  written to disk, OpenSPIM
>>> occasionally pauses in the middle of the acquisition for  some seconds
>>> to several minutes. I have only observed this when the stack is written
>>> to disk, not when the same stack is left in RAM and opened immediately.
>>> I tried to turn ON/OFF the asynchronous writing option but that did not
>>> change anything. 
>> 
>> Are you running an anti-virus software that could be at fault?
> 
> Not that I am aware of, but there might be a lot of crap (updates, power manager, blabla?) that could interfere. I will see at cleaning this mess a bit.
> However, I did not notice anything happening @ the GUI level and correlating with these hangs.
> 
>> I've been getting some weird hangings as each slice is written out (affecting the entire operating system, not just the SPIM software), but typically they were lasting no more than a half-second, and never on the order of minutes. They're recent, and I've been doing some timing tests to try and determine the cause to greater accuracy than 'when writing'.
>> 
>> 
>> I take it the disk is local to the computer?
> 
> Yep, internal HD. 
> 
>>> Also I have noticed that I can not increase the memory allocated to Fiji
>>> beyond 1024M as it causes the HamamatsuHam driver to not load properly. 
>> 
>> Yes, that is unfortunately not something we can fix until we switch to a
>> 64-bit version (which should be doable soon, given the hardware).
> 
> Let's wait then.
> 
> With my best regards,
> 
> Alexis
> 
>> 
>> Ciao,
>> Johannes
> 
> 
> _______________________________________________
> OpenSPIM mailing list
> OpenSPIM at openspim.org
> http://openspim.org/mailman/listinfo/openspim

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130813/dd5a56b7/attachment.pgp>

From Alexis.Maizel at cos.uni-heidelberg.de  Tue Aug 13 13:14:47 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Tue, 13 Aug 2013 20:14:47 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
Message-ID: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>

Hi,

I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.

 I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
 
You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip

With my best regards,

Alexis

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130813/a6fdd0c2/attachment.pgp>

From tomancak at mpi-cbg.de  Tue Aug 13 17:30:24 2013
From: tomancak at mpi-cbg.de (Pavel Tomancak)
Date: Wed, 14 Aug 2013 00:30:24 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
Message-ID: <B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>

Hi Alexis,

That is very strange. I have never seen that. We made some acquisitions today and nothing like that was going on. Its obviously a serious issue. Does it happen only when you have the asynchronous writing enabled? I assume you are running on Windows 7.

All the best

PAvel

-----------------------------------------------------------------------------------
Pavel Tomancak, Ph.D.

Group Leader
Max Planck Institute of Molecular Cell Biology and Genetics
Pfotenhauerstr. 108
D-01307 Dresden				Tel.: +49 351 210 2670
Germany						Fax: +49 351 210 2020
tomancak at mpi-cbg.de
http://www.mpi-cbg.de
-----------------------------------------------------------------------------------



On Aug 13, 2013, at 8:14 PM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:

> Hi,
> 
> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
> 
> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
> 
> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
> 
> With my best regards,
> 
> Alexis
> 
> _______________________________________________
> OpenSPIM mailing list
> OpenSPIM at openspim.org
> http://openspim.org/mailman/listinfo/openspim

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130814/5ec66939/attachment.html>

From huisken at mpi-cbg.de  Wed Aug 14 03:11:31 2013
From: huisken at mpi-cbg.de (Jan Huisken)
Date: Wed, 14 Aug 2013 10:11:31 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
Message-ID: <8C2EDD65-84C3-4E2A-9996-E1265BFC498F@mpi-cbg.de>

Hi Alexis,

this is clearly a camera issue. Sometimes this goes along with missing or partial frames. We have seen it happening when something goes wrong with the spooling, e.g. when the hard drive is too slow or too full or too many files in one folder.

BTW the images are not in focus and you seem to collect quite a bit of laser speckles. Clearly a sign for a bad emission filter or that your laser needs a clean-up filter.

Best
Jan

Dr. Jan Huisken
MPI of Molecular Cell Biology and Genetics
Pfotenhauerstr. 108, 01307 Dresden, Germany

On Aug 13, 2013, at 8:14 PM, Alexis Maizel wrote:

> Hi,
> 
> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
> 
> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
> 
> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
> 
> With my best regards,
> 
> Alexis
> 
> _______________________________________________
> OpenSPIM mailing list
> OpenSPIM at openspim.org
> http://openspim.org/mailman/listinfo/openspim

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130814/2c94cc6e/attachment.html>

From Alexis.Maizel at cos.uni-heidelberg.de  Wed Aug 14 03:11:01 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Wed, 14 Aug 2013 10:11:01 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
Message-ID: <698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>

Hi Pavel,

I did some more tests this morning. 
The asynchronous writing is indeed the culprit. You can get two stacks here: http://dl.dropbox.com/u/484859/async_ON_vs_OFF.zip

With async OFF: the images are written to the disk as soon as they are acquired; it is slow but the planes are in the right order.

With async ON: the stack is acquired and written 'in bursts' to the disk. Sometimes there is a gap of several seconds between two bursts of disk writing and I have the feeling this corresponds to points when the planes are written in the wrong order. I have also noticed that irrespective of the time delay between two time lapse acquisition, the last image(s) of time point N are written to the disk instant before the time point N+1 starts to be acquired. Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk,  then there is a warning windows displayed (which is empty?) after some seconds, the window disappear and the recording has been aborted. 
So I do not know, what's wrong but something does not seem right in this 'asynchronous writing' option, at least on our config:
Windows 7 pro
1024Mb of RAM allocated to Fiji (we can not allocate more, otherwise the Orca won't operate)
JRE 1.6.0
everything 32bits version

Would be great to get that fixed :-)

With my best regards,

Alexis


On 14 Aug 2013, at 00:30, Pavel Tomancak <tomancak at mpi-cbg.de> wrote:

> Hi Alexis,
> 
> That is very strange. I have never seen that. We made some acquisitions today and nothing like that was going on. Its obviously a serious issue. Does it happen only when you have the asynchronous writing enabled? I assume you are running on Windows 7.
> 
> All the best
> 
> PAvel
> 
> -----------------------------------------------------------------------------------
> Pavel Tomancak, Ph.D.
> 
> Group Leader
> Max Planck Institute of Molecular Cell Biology and Genetics
> Pfotenhauerstr. 108
> D-01307 Dresden				Tel.: +49 351 210 2670
> Germany						Fax: +49 351 210 2020
> tomancak at mpi-cbg.de
> http://www.mpi-cbg.de
> -----------------------------------------------------------------------------------
> 
> 
> 
> On Aug 13, 2013, at 8:14 PM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:
> 
>> Hi,
>> 
>> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
>> 
>> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
>> 
>> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
>> 
>> With my best regards,
>> 
>> Alexis
>> 
>> _______________________________________________
>> OpenSPIM mailing list
>> OpenSPIM at openspim.org
>> http://openspim.org/mailman/listinfo/openspim
> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130814/b83aa6d8/attachment-0001.pgp>

From Alexis.Maizel at cos.uni-heidelberg.de  Wed Aug 14 03:20:58 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Wed, 14 Aug 2013 10:20:58 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <8C2EDD65-84C3-4E2A-9996-E1265BFC498F@mpi-cbg.de>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<8C2EDD65-84C3-4E2A-9996-E1265BFC498F@mpi-cbg.de>
Message-ID: <12EEF5BD-10F2-4328-9929-7C8CDAAFEF9E@cos.uni-heidelberg.de>

Hi Jan,

> BTW the images are not in focus and you seem to collect quite a bit of laser speckles. Clearly a sign for a bad emission filter or that your laser needs a clean-up filter.

Yes, the laser does need a clean up filter quite a bit of green light in it, especially at low power; it is ordered. The emission filter (http://www.semrock.com/FilterDetails.aspx?id=FF03-525/50-25) seems right, no? 

As for focus, this is the best we could get so far :-/
Our LS is ~10?m thick and nicely shaped. The excitation lens is a Nikon CFI FLUOR 10x/0.30.
Do you have suggestions to improve the focus?

With my best regards,

Alexis


On 14 Aug 2013, at 10:11, Jan Huisken <huisken at mpi-cbg.de> wrote:

> Hi Alexis,
> 
> this is clearly a camera issue. Sometimes this goes along with missing or partial frames. We have seen it happening when something goes wrong with the spooling, e.g. when the hard drive is too slow or too full or too many files in one folder.
> 
> 
> Best
> Jan
> 
> Dr. Jan Huisken
> MPI of Molecular Cell Biology and Genetics
> Pfotenhauerstr. 108, 01307 Dresden, Germany
> 
> On Aug 13, 2013, at 8:14 PM, Alexis Maizel wrote:
> 
>> Hi,
>> 
>> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
>> 
>> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
>> 
>> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
>> 
>> With my best regards,
>> 
>> Alexis
>> 
>> _______________________________________________
>> OpenSPIM mailing list
>> OpenSPIM at openspim.org
>> http://openspim.org/mailman/listinfo/openspim
> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130814/02b2c788/attachment.pgp>

From weber at mpi-cbg.de  Wed Aug 14 07:53:09 2013
From: weber at mpi-cbg.de (Michael Weber)
Date: Wed, 14 Aug 2013 14:53:09 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <12EEF5BD-10F2-4328-9929-7C8CDAAFEF9E@cos.uni-heidelberg.de>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<8C2EDD65-84C3-4E2A-9996-E1265BFC498F@mpi-cbg.de>
	<12EEF5BD-10F2-4328-9929-7C8CDAAFEF9E@cos.uni-heidelberg.de>
Message-ID: <5C2B0E2F-DEEA-4B52-AF34-93AAF49B00E5@mpi-cbg.de>

Hi Alexis,

the light sheet might simply be too thick, I think that's still an issue with OpenSPIM v1. If you have laser power left you could try and extend the beam further in the first place.

Regarding the stack issue - you mentioned that it takes about 3 min to write 400 MB to disk, that might be an issue on the computer hardware side. A simple hard drive should give you around 100 MB/s, with smaller data packages it might be less but 3 min sounds way to slow. Can you reduce the data rate and see if that helps? For example by reducing the frame rate or using binning while keeping the frame rate constant. Or if you can, try an SSD drive or a RAID system for data acquisition. Streaming data can also be very sensitive to background activities, such as virus scanners, cloud syncing, update scanners.

Best regards,
Michael


On Aug 14, 2013, at 10:20 AM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:

> Hi Jan,
> 
>> BTW the images are not in focus and you seem to collect quite a bit of laser speckles. Clearly a sign for a bad emission filter or that your laser needs a clean-up filter.
> 
> Yes, the laser does need a clean up filter quite a bit of green light in it, especially at low power; it is ordered. The emission filter (http://www.semrock.com/FilterDetails.aspx?id=FF03-525/50-25) seems right, no? 
> 
> As for focus, this is the best we could get so far :-/
> Our LS is ~10?m thick and nicely shaped. The excitation lens is a Nikon CFI FLUOR 10x/0.30.
> Do you have suggestions to improve the focus?
> 
> With my best regards,
> 
> Alexis
> 
> 
> On 14 Aug 2013, at 10:11, Jan Huisken <huisken at mpi-cbg.de> wrote:
> 
>> Hi Alexis,
>> 
>> this is clearly a camera issue. Sometimes this goes along with missing or partial frames. We have seen it happening when something goes wrong with the spooling, e.g. when the hard drive is too slow or too full or too many files in one folder.
>> 
>> 
>> Best
>> Jan
>> 
>> Dr. Jan Huisken
>> MPI of Molecular Cell Biology and Genetics
>> Pfotenhauerstr. 108, 01307 Dresden, Germany
>> 
>> On Aug 13, 2013, at 8:14 PM, Alexis Maizel wrote:
>> 
>>> Hi,
>>> 
>>> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
>>> 
>>> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
>>> 
>>> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
>>> 
>>> With my best regards,
>>> 
>>> Alexis
>>> 
>>> _______________________________________________
>>> OpenSPIM mailing list
>>> OpenSPIM at openspim.org
>>> http://openspim.org/mailman/listinfo/openspim
>> 
> 
> _______________________________________________
> OpenSPIM mailing list
> OpenSPIM at openspim.org
> http://openspim.org/mailman/listinfo/openspim

_____________

Michael Weber
PhD Student, Huisken lab
Max Planck Institute of Molecular Cell Biology and Genetics
Pfotenhauerstrasse 108, 01307 Dresden
Tel. 0049 351/2102837

http://www.mpi-cbg.de/huisken

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130814/3106b327/attachment.html>

From Alexis.Maizel at cos.uni-heidelberg.de  Wed Aug 14 09:54:59 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Wed, 14 Aug 2013 16:54:59 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <5C2B0E2F-DEEA-4B52-AF34-93AAF49B00E5@mpi-cbg.de>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<8C2EDD65-84C3-4E2A-9996-E1265BFC498F@mpi-cbg.de>
	<12EEF5BD-10F2-4328-9929-7C8CDAAFEF9E@cos.uni-heidelberg.de>
	<5C2B0E2F-DEEA-4B52-AF34-93AAF49B00E5@mpi-cbg.de>
Message-ID: <F58F755A-F260-4B0E-A467-B7D4E241A459@cos.uni-heidelberg.de>

hi,
> Regarding the stack issue - you mentioned that it takes about 3 min to write 400 MB to disk, that might be an issue on the computer hardware side. A simple hard drive should give you around 100 MB/s, with smaller data packages it might be less but 3 min sounds way to slow. Can you reduce the data rate and see if that helps?For example by reducing the frame rate or using binning while keeping the frame rate constant. Or if you can, try an SSD drive or a RAID system for data acquisition. Streaming data can also be very sensitive to background activities, such as virus scanners, cloud syncing, update scanners.

I do not think that the problem comes from the hard drive:
1) reducing the file size by a factor of 2 or 4 (binning) did not make the writing to disk faster. With the asynchronous writing ON, the planes are always written to disk in a peppered manner between two consecutive time lapse, with the last image(s)  written right before the next time point.

2) When  asynchronous writing os OFF (i-e writing to disk is occurring immediately, which should be a good readout of the hard drive speed) the stacks are written in about 20sec (for ~200Mb)

With my best regards,

Alexis


> Best regards,
> Michael
> 
> 
> On Aug 14, 2013, at 10:20 AM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:
> 
>> Hi Jan,
>> 
>>> BTW the images are not in focus and you seem to collect quite a bit of laser speckles. Clearly a sign for a bad emission filter or that your laser needs a clean-up filter.
>> 
>> Yes, the laser does need a clean up filter quite a bit of green light in it, especially at low power; it is ordered. The emission filter (http://www.semrock.com/FilterDetails.aspx?id=FF03-525/50-25) seems right, no? 
>> 
>> As for focus, this is the best we could get so far :-/
>> Our LS is ~10?m thick and nicely shaped. The excitation lens is a Nikon CFI FLUOR 10x/0.30.
>> Do you have suggestions to improve the focus?
>> 
>> With my best regards,
>> 
>> Alexis
>> 
>> 
>> On 14 Aug 2013, at 10:11, Jan Huisken <huisken at mpi-cbg.de> wrote:
>> 
>>> Hi Alexis,
>>> 
>>> this is clearly a camera issue. Sometimes this goes along with missing or partial frames. We have seen it happening when something goes wrong with the spooling, e.g. when the hard drive is too slow or too full or too many files in one folder.
>>> 
>>> 
>>> Best
>>> Jan
>>> 
>>> Dr. Jan Huisken
>>> MPI of Molecular Cell Biology and Genetics
>>> Pfotenhauerstr. 108, 01307 Dresden, Germany
>>> 
>>> On Aug 13, 2013, at 8:14 PM, Alexis Maizel wrote:
>>> 
>>>> Hi,
>>>> 
>>>> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
>>>> 
>>>> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
>>>> 
>>>> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
>>>> 
>>>> With my best regards,
>>>> 
>>>> Alexis
>>>> 
>>>> _______________________________________________
>>>> OpenSPIM mailing list
>>>> OpenSPIM at openspim.org
>>>> http://openspim.org/mailman/listinfo/openspim
>>> 
>> 
>> _______________________________________________
>> OpenSPIM mailing list
>> OpenSPIM at openspim.org
>> http://openspim.org/mailman/listinfo/openspim
> 
> _____________
> 
> Michael Weber
> PhD Student, Huisken lab
> Max Planck Institute of Molecular Cell Biology and Genetics
> Pfotenhauerstrasse 108, 01307 Dresden
> Tel. 0049 351/2102837
> 
> http://www.mpi-cbg.de/huisken
> 
> _______________________________________________
> OpenSPIM mailing list
> OpenSPIM at openspim.org
> http://openspim.org/mailman/listinfo/openspim

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130814/614a38d3/attachment.pgp>

From tomancak at mpi-cbg.de  Sun Aug 18 14:42:07 2013
From: tomancak at mpi-cbg.de (Pavel Tomancak)
Date: Sun, 18 Aug 2013 21:42:07 +0200
Subject: [OpenSPIM] OpenSPIM
In-Reply-To: <2CF004C2D6E4BD4A871084E05627BCAD2916A85C@serv-mailbox3.igbmc.u-strasbg.fr>
References: <2CF004C2D6E4BD4A871084E05627BCAD2916A85C@serv-mailbox3.igbmc.u-strasbg.fr>
Message-ID: <D1F5E274-CFC8-469D-884E-2942CAED8C2B@mpi-cbg.de>

Dear Yusuke,

We are working on organising a course about light sheet microscopy. Its an EMBO practical course application, if it works out, you will hear about it. That would be the easiest way to start.

Otherwise I recommend to search for resources on the internet. Open hardware is an increasingly active movement. Lots of things are discussed on twitter where you can easily connect with people with the same interests. The keywords to search for are things like 3d printing, arduino, raspberry pi etc. There is a lot out there.

There is also a primordial idea to organise a course about open hardware in general but its too soon to talk about it.

All the best

PAvel

-----------------------------------------------------------------------------------
Pavel Tomancak, Ph.D.

Group Leader
Max Planck Institute of Molecular Cell Biology and Genetics
Pfotenhauerstr. 108
D-01307 Dresden				Tel.: +49 351 210 2670
Germany						Fax: +49 351 210 2020
tomancak at mpi-cbg.de
http://www.mpi-cbg.de
-----------------------------------------------------------------------------------



On Aug 14, 2013, at 4:52 PM, Yusuke MIYANARI <miyanari at igbmc.fr> wrote:

> Dear Pavel and people in OpenSPIM project
> 
> I'm Yusuke, a posdoc in IGBMC, Strasbourg. First of all, I appreciate you and people involved in OpenSpim project, since it makes me so motivated to build own SPIM, whereas I am just a user of microscopy in the meantime. I really enjoyed the paper published in Nature method and the website. This is really helpful especially for biologist like me to learn how to assemble microscopy. I would like to learn how to design microscopy, including assemble of optical path, choice of each components (laser, beam expander, relay lens, etc), and practical skill or difficulties. I'm wondering whether you could tell me some text books, manuscripts, websites, or workshops, which are useful to understand how to design and assemble microscopy. 
> 
> Thank you for your help,
> 
> Yusuke
> 
> 
> Yusuke Miyanari
> INSTITUT DE GENETIQUE ET DE BIOLOGIE MOLECULAIRE ET CELLULAIRE (IGBMC)
> Parc d'Innovation
> 1, rue Laurent Fries,
> 67404 ILLKIRCH Cedex,
> C.U. de STRASBOURG, FRANCE
> Tel: 00 33 (0)3 88 65 33 58 (lab) 
> Fax: 00 33 (0)3 88 65 32 01
> Maria-Elena Torres-Padilla lab
> 
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130818/f9da2e99/attachment.html>

From miyanari at igbmc.fr  Mon Aug 19 09:25:11 2013
From: miyanari at igbmc.fr (Yusuke MIYANARI)
Date: Mon, 19 Aug 2013 14:25:11 +0000
Subject: [OpenSPIM] OpenSPIM
In-Reply-To: <D1F5E274-CFC8-469D-884E-2942CAED8C2B@mpi-cbg.de>
References: <2CF004C2D6E4BD4A871084E05627BCAD2916A85C@serv-mailbox3.igbmc.u-strasbg.fr>,
	<D1F5E274-CFC8-469D-884E-2942CAED8C2B@mpi-cbg.de>
Message-ID: <2CF004C2D6E4BD4A871084E05627BCAD29185725@serv-mailbox3.igbmc.u-strasbg.fr>

Hello Pavel,

Thank you for your reply. If you could organize the EMBO practical course, it sounds great.

All the best,

Yusuke




Yusuke Miyanari
INSTITUT DE GENETIQUE ET DE BIOLOGIE MOLECULAIRE ET CELLULAIRE (IGBMC)
Parc d'Innovation
1, rue Laurent Fries,
67404 ILLKIRCH Cedex,
C.U. de STRASBOURG, FRANCE
Tel: 00 33 (0)3 88 65 33 58 (lab)
Fax: 00 33 (0)3 88 65 32 01
Maria-Elena Torres-Padilla lab

________________________________
From: Pavel Tomancak [tomancak at mpi-cbg.de]
Sent: 18 August 2013 21:42
To: Yusuke MIYANARI
Cc: OpenSPIM at openspim.org
Subject: Re: OpenSPIM

Dear Yusuke,

We are working on organising a course about light sheet microscopy. Its an EMBO practical course application, if it works out, you will hear about it. That would be the easiest way to start.

Otherwise I recommend to search for resources on the internet. Open hardware is an increasingly active movement. Lots of things are discussed on twitter where you can easily connect with people with the same interests. The keywords to search for are things like 3d printing, arduino, raspberry pi etc. There is a lot out there.

There is also a primordial idea to organise a course about open hardware in general but its too soon to talk about it.

All the best

PAvel

-----------------------------------------------------------------------------------
Pavel Tomancak, Ph.D.

Group Leader
Max Planck Institute of Molecular Cell Biology and Genetics
Pfotenhauerstr. 108
D-01307 Dresden Tel.: +49 351 210 2670
Germany Fax: +49 351 210 2020
tomancak at mpi-cbg.de<mailto:tomancak at mpi-cbg.de>
http://www.mpi-cbg.de<http://www.mpi-cbg.de/>
-----------------------------------------------------------------------------------



On Aug 14, 2013, at 4:52 PM, Yusuke MIYANARI <miyanari at igbmc.fr<mailto:miyanari at igbmc.fr>> wrote:

Dear Pavel and people in OpenSPIM project

I'm Yusuke, a posdoc in IGBMC, Strasbourg. First of all, I appreciate you and people involved in OpenSpim project, since it makes me so motivated to build own SPIM, whereas I am just a user of microscopy in the meantime. I really enjoyed the paper published in Nature method and the website. This is really helpful especially for biologist like me to learn how to assemble microscopy. I would like to learn how to design microscopy, including assemble of optical path, choice of each components (laser, beam expander, relay lens, etc), and practical skill or difficulties. I'm wondering whether you could tell me some text books, manuscripts, websites, or workshops, which are useful to understand how to design and assemble microscopy.

Thank you for your help,

Yusuke


Yusuke Miyanari
INSTITUT DE GENETIQUE ET DE BIOLOGIE MOLECULAIRE ET CELLULAIRE (IGBMC)
Parc d'Innovation
1, rue Laurent Fries,
67404 ILLKIRCH Cedex,
C.U. de STRASBOURG, FRANCE
Tel: 00 33 (0)3 88 65 33 58 (lab)
Fax: 00 33 (0)3 88 65 32 01
Maria-Elena Torres-Padilla lab



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130819/13e2eefd/attachment.html>

From stuyvenberg at wisc.edu  Mon Aug 19 12:22:08 2013
From: stuyvenberg at wisc.edu (Luke Stuyvenberg)
Date: Mon, 19 Aug 2013 12:22:08 -0500
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <75f08c45483b1.52125421@wiscmail.wisc.edu>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<76209bad4c9b2.52122355@wiscmail.wisc.edu>
	<7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
Message-ID: <7530b15b4bd9a.52120df0@wiscmail.wisc.edu>

Hi Alexis,

Sorry for the delay in my reply; I was moving into a new apartment last week.

On 08/14/13, Alexis Maizel wrote:
> Hi Pavel,
>?
> I did some more tests this morning.?
> The asynchronous writing is indeed the culprit. You can get two stacks here: http://dl.dropbox.com/u/484859/async_ON_vs_OFF.zip
Did you re-save this data, or are these stacks fresh from the OpenSPIM plugin? I ask because the OME-TIFF metadata have been clobbered in both stacks (you can check using the Bio-Formats importer); if this is fresh output, the plugin may have a serious error in its metadata generation. Alternatively, Bio-Formats might be out-of-date or acting up. At any rate, the image slices obviously shouldn't be written out of order, but even if they were to be written in the wrong order, were the OME metadata intact, Bio-Formats should be able to display the slices in the correct order (because now it has a table relating the slice's index in the file to its physical Z position).


On 08/14/13, Alexis Maizel wrote:
> With async ON: the stack is acquired and written 'in bursts' to the disk. Sometimes there is a gap of several seconds between two bursts of disk writing and I have the feeling this corresponds to points when the planes are written in the wrong order. I have also noticed that irrespective of the time delay between two time lapse acquisition, the last image(s) of time point N are written to the disk instant before the time point N+1 starts to be acquired.
The data isn't truly meant to be written in bursts -- ideally it would just chug along in the background... I'll try tweaking the async a little more; probably, the code is doing something incorrectly leading to this. As for the timepoint images, this too is very unusual -- I specifically have the writer finish writing any pending images before finalizing a stack. I'll look into this as well.


On 08/14/13, Alexis Maizel wrote:
>?Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty?) after some seconds, the window disappear and the recording has been aborted.
I'm aware of this; I've never been able to determine what that message box is trying to display. No warnings should be appearing during an abort, but this one mysteriously shows up. It's a low priority for me, however; it doesn't seem to affect the abort or change the data that was written (well, any more than aborting mid-acquisition already does).


On 08/14/13, Alexis Maizel wrote:
> So I do not know, what's wrong but something does not seem right in this 'asynchronous writing' option, at least on our config:
> Windows 7 pro
> 1024Mb of RAM allocated to Fiji (we can not allocate more, otherwise the Orca won't operate)
> JRE 1.6.0
> everything 32bits version
Despite the many imaging sessions Julie Last and I have been performing over here, I haven't been able to reproduce this bug. I also tried imaging while stress-testing the CPU and RAM with Prime95; although this made the queue build up a few more images than normal use, I didn't notice any actual problems while writing, and the OME metadata came out fine. I've also been doing some CPU time sampling using jvisualvm, though all I discovered is that snapImage is *very* slow on our machine for some reason. I'll see what more I can find out soon.

Is everything completely up-to-date, including Bio-Formats? (Note that I haven't yet tried the Bio-Formats daily builds -- whether they will fix or break the plugin has yet to be seen.) If so, the next time you're running an acquisition, could you try monitoring Fiji's memory usage (Plugins > Utilities > Memory Monitor) as the sequence progresses? I suspect that the bursts you are observing happen because the queue is filling up, forcing the output handler to synchronously write out one or more slices; you should see the memory use climb in a stair-step pattern until one of these bursts, when it should plummet back to nearly nothing.


Thanks,
Luke

On 08/14/13, Alexis Maizel wrote:
> Hi Pavel,
> 
> I did some more tests this morning. 
> The asynchronous writing is indeed the culprit. You can get two stacks here: http://dl.dropbox.com/u/484859/async_ON_vs_OFF.zip
> 
> With async OFF: the images are written to the disk as soon as they are acquired; it is slow but the planes are in the right order.
> 
> With async ON: the stack is acquired and written 'in bursts' to the disk. Sometimes there is a gap of several seconds between two bursts of disk writing and I have the feeling this corresponds to points when the planes are written in the wrong order. I have also noticed that irrespective of the time delay between two time lapse acquisition, the last image(s) of time point N are written to the disk instant before the time point N+1 starts to be acquired. Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty?) after some seconds, the window disappear and the recording has been aborted. 
> So I do not know, what's wrong but something does not seem right in this 'asynchronous writing' option, at least on our config:
> Windows 7 pro
> 1024Mb of RAM allocated to Fiji (we can not allocate more, otherwise the Orca won't operate)
> JRE 1.6.0
> everything 32bits version
> 
> Would be great to get that fixed :-)
> 
> With my best regards,
> 
> Alexis
> 
> 
> On 14 Aug 2013, at 00:30, Pavel Tomancak <tomancak at mpi-cbg.de> wrote:
> 
> > Hi Alexis,
> > 
> > That is very strange. I have never seen that. We made some acquisitions today and nothing like that was going on. Its obviously a serious issue. Does it happen only when you have the asynchronous writing enabled? I assume you are running on Windows 7.
> > 
> > All the best
> > 
> > PAvel
> > 
> > -----------------------------------------------------------------------------------
> > Pavel Tomancak, Ph.D.
> > 
> > Group Leader
> > Max Planck Institute of Molecular Cell Biology and Genetics
> > Pfotenhauerstr. 108
> > D-01307 Dresden Tel.: +49 351 210 2670
> > Germany Fax: +49 351 210 2020
> > tomancak at mpi-cbg.de
> > http://www.mpi-cbg.de
> > -----------------------------------------------------------------------------------
> > 
> > 
> > 
> > On Aug 13, 2013, at 8:14 PM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:
> > 
> >> Hi,
> >> 
> >> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
> >> 
> >> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
> >> 
> >> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
> >> 
> >> With my best regards,
> >> 
> >> Alexis
> >> 
> >> _______________________________________________
> >> OpenSPIM mailing list
> >> OpenSPIM at openspim.org
> >> http://openspim.org/mailman/listinfo/openspim
> >


From Alexis.Maizel at cos.uni-heidelberg.de  Tue Aug 20 08:54:51 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Tue, 20 Aug 2013 15:54:51 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<76209bad4c9b2.52122355@wiscmai!
	l.wisc.edu> <7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
Message-ID: <F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>

Hi Luke,

> Did you re-save this data, or are these stacks fresh from the OpenSPIM plugin?

I scaled them down to 8bits and saved them on my Mac. You can get an original "shuffled" stack here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiff

> I ask because the OME-TIFF metadata have been clobbered in both stacks (you can check using the Bio-Formats importer); if this is fresh output, the plugin may have a serious error in its metadata generation. Alternatively, Bio-Formats might be out-of-date or acting up. At any rate, the image slices obviously shouldn't be written out of order, but even if they were to be written in the wrong order, were the OME metadata intact, Bio-Formats should be able to display the slices in the correct order (because now it has a table relating the slice's index in the file to its physical Z position).

I forced Fiji to open the fresh-from-openSPIM stacks on my mac using the Bio-format importer (updated) and the slices are still in the wrong order.

> The data isn't truly meant to be written in bursts -- ideally it would just chug along in the background... I'll try tweaking the async a little more; probably, the code is doing something incorrectly leading to this. As for the timepoint images, this too is very unusual -- I specifically have the writer finish writing any pending images before finalizing a stack. I'll look into this as well.

Definitively the writing to disk finishes way after the stack has finished to be acquired. 

>>  Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty?) after some seconds, the window disappear and the recording has been aborted.
> I'm aware of this; I've never been able to determine what that message box is trying to display. No warnings should be appearing during an abort, but this one mysteriously shows up. It's a low priority for me, however; it doesn't seem to affect the abort or change the data that was written (well, any more than aborting mid-acquisition already does).

I concur that it is not critical, I just wanted to point it out. Glad to read that at least some of what I am experiencing can be reproduced :-)

> Is everything completely up-to-date, including Bio-Formats? (Note that I haven't yet tried the Bio-Formats daily builds -- whether they will fix or break the plugin has yet to be seen.)

I will update everything and perform more tests.

> If so, the next time you're running an acquisition, could you try monitoring Fiji's memory usage (Plugins > Utilities > Memory Monitor) as the sequence progresses? I suspect that the bursts you are observing happen because the queue is filling up, forcing the output handler to synchronously write out one or more slices; you should see the memory use climb in a stair-step pattern until one of these bursts, when it should plummet back to nearly nothing.

I'll do that and get back to you.

Thanks for your help,

Alexis



> 
> 
> Thanks,
> Luke
> 
> On 08/14/13, Alexis Maizel wrote:
>> Hi Pavel,
>> 
>> I did some more tests this morning. 
>> The asynchronous writing is indeed the culprit. You can get two stacks here: http://dl.dropbox.com/u/484859/async_ON_vs_OFF.zip
>> 
>> With async OFF: the images are written to the disk as soon as they are acquired; it is slow but the planes are in the right order.
>> 
>> With async ON: the stack is acquired and written 'in bursts' to the disk. Sometimes there is a gap of several seconds between two bursts of disk writing and I have the feeling this corresponds to points when the planes are written in the wrong order. I have also noticed that irrespective of the time delay between two time lapse acquisition, the last image(s) of time point N are written to the disk instant before the time point N+1 starts to be acquired. Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty?) after some seconds, the window disappear and the recording has been aborted. 
>> So I do not know, what's wrong but something does not seem right in this 'asynchronous writing' option, at least on our config:
>> Windows 7 pro
>> 1024Mb of RAM allocated to Fiji (we can not allocate more, otherwise the Orca won't operate)
>> JRE 1.6.0
>> everything 32bits version
>> 
>> Would be great to get that fixed :-)
>> 
>> With my best regards,
>> 
>> Alexis
>> 
>> 
>> On 14 Aug 2013, at 00:30, Pavel Tomancak <tomancak at mpi-cbg.de> wrote:
>> 
>>> Hi Alexis,
>>> 
>>> That is very strange. I have never seen that. We made some acquisitions today and nothing like that was going on. Its obviously a serious issue. Does it happen only when you have the asynchronous writing enabled? I assume you are running on Windows 7.
>>> 
>>> All the best
>>> 
>>> PAvel
>>> 
>>> -----------------------------------------------------------------------------------
>>> Pavel Tomancak, Ph.D.
>>> 
>>> Group Leader
>>> Max Planck Institute of Molecular Cell Biology and Genetics
>>> Pfotenhauerstr. 108
>>> D-01307 Dresden Tel.: +49 351 210 2670
>>> Germany Fax: +49 351 210 2020
>>> tomancak at mpi-cbg.de
>>> http://www.mpi-cbg.de
>>> -----------------------------------------------------------------------------------
>>> 
>>> 
>>> 
>>> On Aug 13, 2013, at 8:14 PM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:
>>> 
>>>> Hi,
>>>> 
>>>> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
>>>> 
>>>> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
>>>> 
>>>> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
>>>> 
>>>> With my best regards,
>>>> 
>>>> Alexis
>>>> 
>>>> _______________________________________________
>>>> OpenSPIM mailing list
>>>> OpenSPIM at openspim.org
>>>> http://openspim.org/mailman/listinfo/openspim
>>> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130820/4db14a08/attachment.pgp>

From stuyvenberg at wisc.edu  Tue Aug 20 13:24:23 2013
From: stuyvenberg at wisc.edu (Luke Stuyvenberg)
Date: Tue, 20 Aug 2013 13:24:23 -0500
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <75f0fb794fc8f.5213b44f@wiscmail.wisc.edu>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<76209bad4c9b2.52122355@wiscmai!> <l.wisc.edu@wiscmail.wisc.edu>
	<7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
	<F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>
	<77509ad34a859.5213a0a0@wiscmail.wisc.edu>
	<747093234c4f4.5213a0dc@wiscmail.wisc.edu>
	<76f0c89648b4c.5213a119@wiscmail.wisc.edu>
	<77809ad24bb9a.5213a155@wiscmail.wisc.edu>
	<7790f9e84a27f.5213a192@wiscmail.wisc.edu>
	<7780ef964d852.5213a1ce@wiscmail.wisc.edu>
	<740096c04dc83.5213a20b@wiscmail.wisc.edu>
	<75e0ec504df4a.5213a247@wiscmail.wisc.edu>
	<75b0a0ad4a219.5213a284@wiscmail.wisc.edu>
	<7530ccce48e90.5213a2c0@wiscmail.wisc.edu>
	<75b0891f4e792.5213a2fd@wiscmail.wisc.edu>
	<7790f2f44e93e.5213a33a@wiscmail.wisc.edu>
	<7780c8764e553.5213a3b5@wiscmail.wisc.edu>
	<7530f9bc4b916.5213a651@wiscmail.wisc.edu>
	<7470ae67490d8.5213a68d@wiscmail.wisc.edu>
	<7780e5eb4ee99.5213a6ca@wiscmail.wisc.edu>
	<77809dea4f72c.5213a706@wiscmail.wisc.edu>
	<7780b8994cb3a.5213a743@wiscmail.wisc.edu>
	<7630b03b495d0.5213a77f@wiscmail.wisc.edu>
	<7470aae6496f4.5213a7bb@wiscmail.wisc.edu>
	<7790f4014969d.5213a7f8@wiscmail.wisc.edu>
	<753095434e34e.5213a834@wiscmail.wisc.edu>
	<74008841498a1.5213a8ad@wiscmail.wisc.edu>
	<75f0b9f14da0c.5213a8e9@wiscmail.wisc.edu>
	<7630b9b54a789.5213a926@wiscmail.wisc.edu>
	<7470f2314fddf.5213a962@wiscmail.wisc.edu>
	<7530c2b9492c6.5213a99f@wiscmail.wisc.edu>
	<7470dd924c0cd.5213b0fd@wiscmail.wisc.edu>
	<7470f76d4f5d8.5213b139@wiscmail.wisc.edu>
	<75b098c04e484.5213b176@wiscmail.wisc.edu>
	<7530cbb0480cf.5213b1b2@wiscmail.wisc.edu>
	<74709e354a777.5213b1ef@wiscmail.wisc.edu>
	<75f0ca114f6fc.5213b22e@wiscmail.wisc.edu>
	<7400fe1c492c0.5213b26c@wiscmail.wisc.edu>
	<7630cd1b4ae42.5213b2a9@wiscmail.wisc.edu>
	<76f0f0df4f0fa.5213b2e5@wiscmail.wisc.edu>
	<7790bef14ad82.5213b39a@wiscmail.wisc.edu>
	<778084b74be17.5213b3d6@wiscmail.wisc.edu>
	<75f0bc25493ed.5213b413@wiscmail.wisc.edu>
	<75f0fb794fc8f.5213b44f@wiscmail.wisc.edu>
Message-ID: <75f0fb424cace.52136e07@wiscmail.wisc.edu>

Hi Alexis,

> I scaled them down to 8bits and saved them on my Mac. You can get an original "shuffled" stack here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiffUnfortunately this stack, too, seems to have no metadata attached to it. If the plugin is outputting this kind of data, I may have a serious problem. :-)


Are you by any chance clicking the Abort! button at the end of an acquisition? Although acquiring the data may finish, the text will keep reading Abort! until the writing is finished; with large data sets, it may seem the process is done and the button has just gotten stuck, when in fact the OME-TIFF writer is updating the metadata in each of its images (I explain this in more detail below.). Clicking Abort! at such a time would stop the metadata from being written.


> Definitively the writing to disk finishes way after the stack has finished to be acquired. 
Upon review, this makes more sense than it did. The basic OME-TIFF specification requires that each file in the dataset have the complete metadata. Unfortunately, there's no way to know what the complete metadata is until all the files have been written. So as the output handler writes each file, it builds up the metadata, until the end of the acquisition. At this point, it reopens each file in the dataset and rewrites the comment to have the correct metadata (this is the long delay I mentioned above -- the more files you write, the longer it takes to put the finished metadata in place). The question, then, is whether or not the last *slice* of each is actually being held back -- I suspect not; the part of the code that appends a slice to the file is unambiguous in its function, while the metadata can easily reach such a size that writing it seems much like writing a new slice. In essence, there's not much we can presently do about this particular quirk, though now that Bio-Formats is outputting daily builds, there may be other options soon.


> > If so, the next time you're running an acquisition, could you try monitoring Fiji's memory usage (Plugins > Utilities > Memory Monitor) as the sequence progresses? I suspect that the bursts you are observing happen because the queue is filling up, forcing the output handler to synchronously write out one or more slices; you should see the memory use climb in a stair-step pattern until one of these bursts, when it should plummet back to nearly nothing.

Actually, thinking on it, the plummeting most likely will not coincide with the writes -- more likely, the images will stick around in memory until the garbage collector decides to run. So the aforementioned pattern should appear in some form or other, though the timing may not tell us very much.


A few other thoughts:
- Do any other programs make regular use of your hard drive? Perhaps Windows 7's automatic (background) defragmentation, or a filesystem-filter antivirus (like avast!, which scans files as they are read and written)? The writing thread is given a low priority so the acquisition can run freely, but this might cause it to be blocked outright by low-level processes making regular use of the hard drive.
- Is your RAM mostly free when you start the acquisition? The maximum length of the image queue is decided based on the available memory at the beginning of an acquisition; I don't yet have a display for it, but it might be being set too low...


I'm working on a slight rework of the async handler which might help to fix these issues; I'll upload it to our update server once it's done. Of course, it's hard to gauge its efficacy until I can reproduce this behavior, so I'll see what I can do on that front as well.


Thanks again,
Luke


On 08/20/13, Alexis Maizel?wrote:
> Hi Luke,
> 
> > Did you re-save this data, or are these stacks fresh from the OpenSPIM plugin?
> 
> I scaled them down to 8bits and saved them on my Mac. You can get an original "shuffled" stack here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiff
> 
> > I ask because the OME-TIFF metadata have been clobbered in both stacks (you can check using the Bio-Formats importer); if this is fresh output, the plugin may have a serious error in its metadata generation. Alternatively, Bio-Formats might be out-of-date or acting up. At any rate, the image slices obviously shouldn't be written out of order, but even if they were to be written in the wrong order, were the OME metadata intact, Bio-Formats should be able to display the slices in the correct order (because now it has a table relating the slice's index in the file to its physical Z position).
> 
> I forced Fiji to open the fresh-from-openSPIM stacks on my mac using the Bio-format importer (updated) and the slices are still in the wrong order.
> 
> > The data isn't truly meant to be written in bursts -- ideally it would just chug along in the background... I'll try tweaking the async a little more; probably, the code is doing something incorrectly leading to this. As for the timepoint images, this too is very unusual -- I specifically have the writer finish writing any pending images before finalizing a stack. I'll look into this as well.
> 
> Definitively the writing to disk finishes way after the stack has finished to be acquired. 
> 
> >> Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty?) after some seconds, the window disappear and the recording has been aborted.
> > I'm aware of this; I've never been able to determine what that message box is trying to display. No warnings should be appearing during an abort, but this one mysteriously shows up. It's a low priority for me, however; it doesn't seem to affect the abort or change the data that was written (well, any more than aborting mid-acquisition already does).
> 
> I concur that it is not critical, I just wanted to point it out. Glad to read that at least some of what I am experiencing can be reproduced :-)
> 
> > Is everything completely up-to-date, including Bio-Formats? (Note that I haven't yet tried the Bio-Formats daily builds -- whether they will fix or break the plugin has yet to be seen.)
> 
> I will update everything and perform more tests.
> 
> > If so, the next time you're running an acquisition, could you try monitoring Fiji's memory usage (Plugins > Utilities > Memory Monitor) as the sequence progresses? I suspect that the bursts you are observing happen because the queue is filling up, forcing the output handler to synchronously write out one or more slices; you should see the memory use climb in a stair-step pattern until one of these bursts, when it should plummet back to nearly nothing.
> 
> I'll do that and get back to you.
> 
> Thanks for your help,
> 
> Alexis
> 
> 
> 
> > 
> > 
> > Thanks,
> > Luke
> > 
> > On 08/14/13, Alexis Maizel wrote:
> >> Hi Pavel,
> >> 
> >> I did some more tests this morning. 
> >> The asynchronous writing is indeed the culprit. You can get two stacks here: http://dl.dropbox.com/u/484859/async_ON_vs_OFF.zip
> >> 
> >> With async OFF: the images are written to the disk as soon as they are acquired; it is slow but the planes are in the right order.
> >> 
> >> With async ON: the stack is acquired and written 'in bursts' to the disk. Sometimes there is a gap of several seconds between two bursts of disk writing and I have the feeling this corresponds to points when the planes are written in the wrong order. I have also noticed that irrespective of the time delay between two time lapse acquisition, the last image(s) of time point N are written to the disk instant before the time point N+1 starts to be acquired. Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty?) after some seconds, the window disappear and the recording has been aborted. 
> >> So I do not know, what's wrong but something does not seem right in this 'asynchronous writing' option, at least on our config:
> >> Windows 7 pro
> >> 1024Mb of RAM allocated to Fiji (we can not allocate more, otherwise the Orca won't operate)
> >> JRE 1.6.0
> >> everything 32bits version
> >> 
> >> Would be great to get that fixed :-)
> >> 
> >> With my best regards,
> >> 
> >> Alexis
> >> 
> >> 
> >> On 14 Aug 2013, at 00:30, Pavel Tomancak <tomancak at mpi-cbg.de> wrote:
> >> 
> >>> Hi Alexis,
> >>> 
> >>> That is very strange. I have never seen that. We made some acquisitions today and nothing like that was going on. Its obviously a serious issue. Does it happen only when you have the asynchronous writing enabled? I assume you are running on Windows 7.
> >>> 
> >>> All the best
> >>> 
> >>> PAvel
> >>> 
> >>> -----------------------------------------------------------------------------------
> >>> Pavel Tomancak, Ph.D.
> >>> 
> >>> Group Leader
> >>> Max Planck Institute of Molecular Cell Biology and Genetics
> >>> Pfotenhauerstr. 108
> >>> D-01307 Dresden Tel.: +49 351 210 2670
> >>> Germany Fax: +49 351 210 2020
> >>> tomancak at mpi-cbg.de
> >>> http://www.mpi-cbg.de
> >>> -----------------------------------------------------------------------------------
> >>> 
> >>> 
> >>> 
> >>> On Aug 13, 2013, at 8:14 PM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:
> >>> 
> >>>> Hi,
> >>>> 
> >>>> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
> >>>> 
> >>>> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
> >>>> 
> >>>> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
> >>>> 
> >>>> With my best regards,
> >>>> 
> >>>> Alexis
> >>>> 
> >>>> _______________________________________________
> >>>> OpenSPIM mailing list
> >>>> OpenSPIM at openspim.org
> >>>> http://openspim.org/mailman/listinfo/openspim
> >>>


From Alexis.Maizel at cos.uni-heidelberg.de  Wed Aug 21 05:18:49 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Wed, 21 Aug 2013 12:18:49 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <75f0fb424cace.52136e07@wiscmail.wisc.edu>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<76209bad4c9b2.52122355@wiscmai! !> <l.wisc.edu@wiscmail.wisc.edu>
	<7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
	<F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>
	<77509ad34a859.5213a0a0@wiscmail.wisc.edu>
	<747093234c4f4.5213a0dc@wiscmail.wisc.edu>
	<76f0c89648b4c.5213a119@wiscmail.wisc.edu>
	<77809ad24bb9a.5213a155@wiscmail.!
	wisc.edu> <7790f9e84a27f.5213a192@wiscmail.wisc.edu> <7780ef96!
	4d852.5213a1ce@wiscmail.wisc.edu>
	<740096c04dc83.5213a20b@wiscmail.wisc.edu>
	<75e0ec504df4a.5213a247@wiscmail.wisc.edu>
	<75b0a0ad4a219.5213a284@wiscmail.wisc.edu>
	<7530ccce48e90.5213a2c0@wiscmail.wisc.edu>
	<75b0891f4e792.5213a2fd@wiscmail.wisc.edu> <7790f2f44e93e.
Message-ID: <599178A9-1BFA-4F1D-895B-76A66D43DAB3@cos.uni-heidelberg.de>

Hi Luke,

some more tests and hopefully some progresses. I scanned a 82Mb stack (1024*1024*41 @ 16bits)

- RAM usage by Fiji and stack writing to disk: 
	~20Mb used at start (out of  1Gb allocated) 
	as the scan progresses in ramps up to ~110Mb and then slowly decreases as the size of the file on disk increases; finally the garbage collector resets to 20Mb
	it takes up to 2 minutes  to write the 84Mb and at least 1 minute more to write the remaining 2Mb (the metadata, I presume...)
- Metadata: 
	I have updated Fiji
	I have tried to update LOCI to the trunk/daily/stable versions but after downloading some files I get an error message ("there was an error updating LOCI")
	It takes a very long time to write the metadata, much longer than the images. You can retrieve one stack with metadata here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiff
	If you open that stack & metadata, you will see that the planes are not written in the right order. Look at the sequence (toward the end of the file above):

<Plane DeltaT="18.37296911600015" PositionX="3205.0" PositionY="2628.0" PositionZ="3085.0" TheT="0" TheZ="36">
<Plane DeltaT="12.293132814000273" PositionX="3205.0" PositionY="2628.0" PositionZ="3025.0" TheT="0" TheZ="37">
<Plane DeltaT="18.884906363000027" PositionX="3205.0" PositionY="2628.0" PositionZ="3090.0" TheT="0" TheZ="38">
 The plane with the index TheZ=37 (physical position in Z 3025 and the timestamp 12.29)  is intercalated between two obviously consecutive slices (TheZ=36 and TheZ=38). This is obviously wrong?

I have the feeling that my problems are somehow linked to Bio-Format	not behaving properly. 

With my best regards,

Alexis
 

On 20 Aug 2013, at 20:24, Luke Stuyvenberg <stuyvenberg at wisc.edu> wrote:

> Hi Alexis,
> 
>> I scaled them down to 8bits and saved them on my Mac. You can get an original "shuffled" stack here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiffUnfortunately this stack, too, seems to have no metadata attached to it. If the plugin is outputting this kind of data, I may have a serious problem. :-)
> 
> 
> Are you by any chance clicking the Abort! button at the end of an acquisition? Although acquiring the data may finish, the text will keep reading Abort! until the writing is finished; with large data sets, it may seem the process is done and the button has just gotten stuck, when in fact the OME-TIFF writer is updating the metadata in each of its images (I explain this in more detail below.). Clicking Abort! at such a time would stop the metadata from being written.
> 
> 
>> Definitively the writing to disk finishes way after the stack has finished to be acquired. 
> Upon review, this makes more sense than it did. The basic OME-TIFF specification requires that each file in the dataset have the complete metadata. Unfortunately, there's no way to know what the complete metadata is until all the files have been written. So as the output handler writes each file, it builds up the metadata, until the end of the acquisition. At this point, it reopens each file in the dataset and rewrites the comment to have the correct metadata (this is the long delay I mentioned above -- the more files you write, the longer it takes to put the finished metadata in place). The question, then, is whether or not the last *slice* of each is actually being held back -- I suspect not; the part of the code that appends a slice to the file is unambiguous in its function, while the metadata can easily reach such a size that writing it seems much like writing a new slice. In essence, there's not much we can presently do about this particular quirk, though now that Bio-Formats is outputting daily builds, there may be other options soon.
> 
> 
>>> If so, the next time you're running an acquisition, could you try monitoring Fiji's memory usage (Plugins > Utilities > Memory Monitor) as the sequence progresses? I suspect that the bursts you are observing happen because the queue is filling up, forcing the output handler to synchronously write out one or more slices; you should see the memory use climb in a stair-step pattern until one of these bursts, when it should plummet back to nearly nothing.
> 
> Actually, thinking on it, the plummeting most likely will not coincide with the writes -- more likely, the images will stick around in memory until the garbage collector decides to run. So the aforementioned pattern should appear in some form or other, though the timing may not tell us very much.
> 
> 
> A few other thoughts:
> - Do any other programs make regular use of your hard drive? Perhaps Windows 7's automatic (background) defragmentation, or a filesystem-filter antivirus (like avast!, which scans files as they are read and written)? The writing thread is given a low priority so the acquisition can run freely, but this might cause it to be blocked outright by low-level processes making regular use of the hard drive.
> - Is your RAM mostly free when you start the acquisition? The maximum length of the image queue is decided based on the available memory at the beginning of an acquisition; I don't yet have a display for it, but it might be being set too low...
> 
> 
> I'm working on a slight rework of the async handler which might help to fix these issues; I'll upload it to our update server once it's done. Of course, it's hard to gauge its efficacy until I can reproduce this behavior, so I'll see what I can do on that front as well.
> 
> 
> Thanks again,
> Luke
> 
> 
> On 08/20/13, Alexis Maizel wrote:
>> Hi Luke,
>> 
>>> Did you re-save this data, or are these stacks fresh from the OpenSPIM plugin?
>> 
>> I scaled them down to 8bits and saved them on my Mac. You can get an original "shuffled" stack here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiff
>> 
>>> I ask because the OME-TIFF metadata have been clobbered in both stacks (you can check using the Bio-Formats importer); if this is fresh output, the plugin may have a serious error in its metadata generation. Alternatively, Bio-Formats might be out-of-date or acting up. At any rate, the image slices obviously shouldn't be written out of order, but even if they were to be written in the wrong order, were the OME metadata intact, Bio-Formats should be able to display the slices in the correct order (because now it has a table relating the slice's index in the file to its physical Z position).
>> 
>> I forced Fiji to open the fresh-from-openSPIM stacks on my mac using the Bio-format importer (updated) and the slices are still in the wrong order.
>> 
>>> The data isn't truly meant to be written in bursts -- ideally it would just chug along in the background... I'll try tweaking the async a little more; probably, the code is doing something incorrectly leading to this. As for the timepoint images, this too is very unusual -- I specifically have the writer finish writing any pending images before finalizing a stack. I'll look into this as well.
>> 
>> Definitively the writing to disk finishes way after the stack has finished to be acquired. 
>> 
>>>> Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty?) after some seconds, the window disappear and the recording has been aborted.
>>> I'm aware of this; I've never been able to determine what that message box is trying to display. No warnings should be appearing during an abort, but this one mysteriously shows up. It's a low priority for me, however; it doesn't seem to affect the abort or change the data that was written (well, any more than aborting mid-acquisition already does).
>> 
>> I concur that it is not critical, I just wanted to point it out. Glad to read that at least some of what I am experiencing can be reproduced :-)
>> 
>>> Is everything completely up-to-date, including Bio-Formats? (Note that I haven't yet tried the Bio-Formats daily builds -- whether they will fix or break the plugin has yet to be seen.)
>> 
>> I will update everything and perform more tests.
>> 
>>> If so, the next time you're running an acquisition, could you try monitoring Fiji's memory usage (Plugins > Utilities > Memory Monitor) as the sequence progresses? I suspect that the bursts you are observing happen because the queue is filling up, forcing the output handler to synchronously write out one or more slices; you should see the memory use climb in a stair-step pattern until one of these bursts, when it should plummet back to nearly nothing.
>> 
>> I'll do that and get back to you.
>> 
>> Thanks for your help,
>> 
>> Alexis
>> 
>> 
>> 
>>> 
>>> 
>>> Thanks,
>>> Luke
>>> 
>>> On 08/14/13, Alexis Maizel wrote:
>>>> Hi Pavel,
>>>> 
>>>> I did some more tests this morning. 
>>>> The asynchronous writing is indeed the culprit. You can get two stacks here: http://dl.dropbox.com/u/484859/async_ON_vs_OFF.zip
>>>> 
>>>> With async OFF: the images are written to the disk as soon as they are acquired; it is slow but the planes are in the right order.
>>>> 
>>>> With async ON: the stack is acquired and written 'in bursts' to the disk. Sometimes there is a gap of several seconds between two bursts of disk writing and I have the feeling this corresponds to points when the planes are written in the wrong order. I have also noticed that irrespective of the time delay between two time lapse acquisition, the last image(s) of time point N are written to the disk instant before the time point N+1 starts to be acquired. Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty?) after some seconds, the window disappear and the recording has been aborted. 
>>>> So I do not know, what's wrong but something does not seem right in this 'asynchronous writing' option, at least on our config:
>>>> Windows 7 pro
>>>> 1024Mb of RAM allocated to Fiji (we can not allocate more, otherwise the Orca won't operate)
>>>> JRE 1.6.0
>>>> everything 32bits version
>>>> 
>>>> Would be great to get that fixed :-)
>>>> 
>>>> With my best regards,
>>>> 
>>>> Alexis
>>>> 
>>>> 
>>>> On 14 Aug 2013, at 00:30, Pavel Tomancak <tomancak at mpi-cbg.de> wrote:
>>>> 
>>>>> Hi Alexis,
>>>>> 
>>>>> That is very strange. I have never seen that. We made some acquisitions today and nothing like that was going on. Its obviously a serious issue. Does it happen only when you have the asynchronous writing enabled? I assume you are running on Windows 7.
>>>>> 
>>>>> All the best
>>>>> 
>>>>> PAvel
>>>>> 
>>>>> -----------------------------------------------------------------------------------
>>>>> Pavel Tomancak, Ph.D.
>>>>> 
>>>>> Group Leader
>>>>> Max Planck Institute of Molecular Cell Biology and Genetics
>>>>> Pfotenhauerstr. 108
>>>>> D-01307 Dresden Tel.: +49 351 210 2670
>>>>> Germany Fax: +49 351 210 2020
>>>>> tomancak at mpi-cbg.de
>>>>> http://www.mpi-cbg.de
>>>>> -----------------------------------------------------------------------------------
>>>>> 
>>>>> 
>>>>> 
>>>>> On Aug 13, 2013, at 8:14 PM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:
>>>>> 
>>>>>> Hi,
>>>>>> 
>>>>>> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
>>>>>> 
>>>>>> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
>>>>>> 
>>>>>> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
>>>>>> 
>>>>>> With my best regards,
>>>>>> 
>>>>>> Alexis
>>>>>> 
>>>>>> _______________________________________________
>>>>>> OpenSPIM mailing list
>>>>>> OpenSPIM at openspim.org
>>>>>> http://openspim.org/mailman/listinfo/openspim
>>>>> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130821/9454bea2/attachment-0001.pgp>

From stuyvenberg at wisc.edu  Wed Aug 21 10:36:15 2013
From: stuyvenberg at wisc.edu (Luke Stuyvenberg)
Date: Wed, 21 Aug 2013 10:36:15 -0500
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <7630f58a55128.5214de5b@wiscmail.wisc.edu>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<l.wisc.edu@wiscmail.wisc.edu>
	<7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
	<F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>
	<77509ad34a859.5213a0a0@wiscmail.wisc.edu>
	<747093234c4f4.5213a0dc@wiscmail.wisc.edu>
	<76f0c89648b4c.5213a119@wiscmail.wisc.edu>
	<77809ad24bb9a.5213a155@wiscmail.!>
	<wisc.edu@wiscmail.wisc.edu> <7790f9e84a27f.5213a192@wiscmail.wisc.edu>
	<740096c04dc83.5213a20b@wiscmail.wisc.edu>
	<75e0ec504df4a.5213a247@wiscmail.wisc.edu>
	<75b0a0ad4a219.5213a284@wiscmail.wisc.edu>
	<7530ccce48e90.5213a2c0@wiscmail.wisc.edu>
	<75b0891f4e792.5213a2fd@wiscmail.wisc.edu>
	<599178A9-1BFA-4F1D-895B-76A66D43DAB3@cos.uni-heidelberg.de>
	<75f0ddef53ce9.5214d523@wiscmail.wisc.edu>
	<75e0816d56602.5214d55f@wiscmail.wisc.edu>
	<75f0a2a851b6f.5214d59c@wiscmail.wisc.edu>
	<75f0a40057606.5214d5d8@wiscmail.wisc.edu>
	<75f0e3af52dad.5214d615@wiscmail.wisc.edu>
	<75f0e06155195.5214d651@wiscmail.wisc.edu>
	<75b0e6bb51516.5214d68d@wiscmail.wisc.edu>
	<7470deb554615.5214d6ca@wiscmail.wisc.edu>
	<7630f202501e0.5214d706@wiscmail.wisc.edu>
	<775092f1523b2.5214d743@wiscmail.wisc.edu>
	<7790d02e50e94.5214d77f@wiscmail.wisc.edu>
	<7790c21c574d1.5214d7bc@wiscmail.wisc.edu>
	<7630a4b953c61.5214d7f8@wiscmail.wisc.edu>
	<778083d557c6a.5214d835@wiscmail.wisc.edu>
	<75b0d75851ca2.5214d871@wiscmail.wisc.edu>
	<76309e0255df7.5214d8ae@wiscmail.wisc.edu>
	<7400e434506ad.5214db4b@wiscmail.wisc.edu>
	<7780ee1c522d5.5214dbc3@wiscmail.wisc.edu>
	<75f0c12f51490.5214dbff@wiscmail.wisc.edu>
	<7470df7753be2.5214dc3c@wiscmail.wisc.edu>
	<75b09fac570df.5214dda5@wiscmail.wisc.edu>
	<75e0d65b50858.5214dde2@wiscmail.wisc.edu>
	<7630adff547c9.5214de1e@wiscmail.wisc.edu>
	<7630f58a55128.5214de5b@wiscmail.wisc.edu>
Message-ID: <75f0ad0e57b91.5214981f@wiscmail.wisc.edu>

Hi Alexis,

> some more tests and hopefully some progresses. I scanned a 82Mb stack (1024*1024*41 @ 16bits)
>?
> - RAM usage by Fiji and stack writing to disk:?
> ~20Mb used at start (out of 1Gb allocated)?
> as the scan progresses in ramps up to ~110Mb and then slowly decreases as the size of the file on disk increases; finally the garbage collector resets to 20Mb
> it takes up to 2 minutes to write the 84Mb and at least 1 minute more to write the remaining 2Mb (the metadata, I presume...)
Okay; although the writing times are not at all ideal (my 400MB stacks were written synchronously and in their entirety in a little over a minute and a half), the RAM usage is more or less as expected.


> If you open that stack & metadata, you will see that the planes are not written in the right order. Look at the sequence (toward the end of the file above):
>?
> <Plane DeltaT="18.37296911600015" PositionX="3205.0" PositionY="2628.0" PositionZ="3085.0" TheT="0" TheZ="36">
> <Plane DeltaT="12.293132814000273" PositionX="3205.0" PositionY="2628.0" PositionZ="3025.0" TheT="0" TheZ="37">
> <Plane DeltaT="18.884906363000027" PositionX="3205.0" PositionY="2628.0" PositionZ="3090.0" TheT="0" TheZ="38">
> The plane with the index TheZ=37 (physical position in Z 3025 and the timestamp 12.29) is intercalated between two obviously consecutive slices (TheZ=36 and TheZ=38). This is obviously wrong?

I have some mixed feelings on this; this is the definitive proof I was looking for, but I'm genuinely at a loss as to how this may have come about. My best guess is that the synchronization is too loose, and somehow the acquisition ("main") thread is writing slices (because it feels it needs to) before the writing thread.


> I have the feeling that my problems are somehow linked to Bio-Format	not behaving properly.?

I've tried the new Bio-Formats daily builds update site through the Fiji updater, and haven't noticed any problems from it; perhaps try enabling that update site and downloading the very latest versions?


Finally, I have most of the promised plugin updates. There are two main additions:
- First, it does some timing over the acquisition, outputting the results to the log; although it isn't very detailed, it gives a breakdown of how time is used while acquiring (for example, if the async isn't appropriately asynchronous, the acquisition time will be roughly the same as though it were off).
- Second, and more importantly for you, the async writer has been completely revamped. It now queues the begin/end stack events as well as new slices, so these operations shouldn't *ever* get out of order. It also puts up a monitor showing the state of its queue, which will let us know if it's filling up or not.
If you're willing to play guinea pig again, please try replacing mmplugins/SPIMAcquisition.jar with the version from: https://dl.dropboxusercontent.com/u/57890359/SPIMAcquisition.jar
Anyone is free to try this JAR out, but I haven't put it on our Fiji update site for a few reasons -- the new features are only conditionally useful, and can't (yet) be disabled. That said, if the new async fixes your output problems, I'll polish these changes and incorporate them.


Thanks,
Luke


On 08/21/13, Alexis Maizel  wrote:
> Hi Luke,
> 
> some more tests and hopefully some progresses. I scanned a 82Mb stack (1024*1024*41 @ 16bits)
> 
> - RAM usage by Fiji and stack writing to disk: 
> ~20Mb used at start (out of 1Gb allocated) 
> as the scan progresses in ramps up to ~110Mb and then slowly decreases as the size of the file on disk increases; finally the garbage collector resets to 20Mb
> it takes up to 2 minutes to write the 84Mb and at least 1 minute more to write the remaining 2Mb (the metadata, I presume...)
> - Metadata: 
> I have updated Fiji
> I have tried to update LOCI to the trunk/daily/stable versions but after downloading some files I get an error message ("there was an error updating LOCI")
> It takes a very long time to write the metadata, much longer than the images. You can retrieve one stack with metadata here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiff
> If you open that stack & metadata, you will see that the planes are not written in the right order. Look at the sequence (toward the end of the file above):
> 
> <Plane DeltaT="18.37296911600015" PositionX="3205.0" PositionY="2628.0" PositionZ="3085.0" TheT="0" TheZ="36">
> <Plane DeltaT="12.293132814000273" PositionX="3205.0" PositionY="2628.0" PositionZ="3025.0" TheT="0" TheZ="37">
> <Plane DeltaT="18.884906363000027" PositionX="3205.0" PositionY="2628.0" PositionZ="3090.0" TheT="0" TheZ="38">
> The plane with the index TheZ=37 (physical position in Z 3025 and the timestamp 12.29) is intercalated between two obviously consecutive slices (TheZ=36 and TheZ=38). This is obviously wrong?
> 
> I have the feeling that my problems are somehow linked to Bio-Format	not behaving properly. 
> 
> With my best regards,
> 
> Alexis
> 
> 
> On 20 Aug 2013, at 20:24, Luke Stuyvenberg <stuyvenberg at wisc.edu> wrote:
> 
> > Hi Alexis,
> > 
> >> I scaled them down to 8bits and saved them on my Mac. You can get an original "shuffled" stack here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiffUnfortunately this stack, too, seems to have no metadata attached to it. If the plugin is outputting this kind of data, I may have a serious problem. :-)
> > 
> > 
> > Are you by any chance clicking the Abort! button at the end of an acquisition? Although acquiring the data may finish, the text will keep reading Abort! until the writing is finished; with large data sets, it may seem the process is done and the button has just gotten stuck, when in fact the OME-TIFF writer is updating the metadata in each of its images (I explain this in more detail below.). Clicking Abort! at such a time would stop the metadata from being written.
> > 
> > 
> >> Definitively the writing to disk finishes way after the stack has finished to be acquired. 
> > Upon review, this makes more sense than it did. The basic OME-TIFF specification requires that each file in the dataset have the complete metadata. Unfortunately, there's no way to know what the complete metadata is until all the files have been written. So as the output handler writes each file, it builds up the metadata, until the end of the acquisition. At this point, it reopens each file in the dataset and rewrites the comment to have the correct metadata (this is the long delay I mentioned above -- the more files you write, the longer it takes to put the finished metadata in place). The question, then, is whether or not the last *slice* of each is actually being held back -- I suspect not; the part of the code that appends a slice to the file is unambiguous in its function, while the metadata can easily reach such a size that writing it seems much like writing a new slice. In essence, there's not much we can presently do about this particular quirk, though now that Bio-Formats is outputting daily builds, there may be other options soon.
> > 
> > 
> >>> If so, the next time you're running an acquisition, could you try monitoring Fiji's memory usage (Plugins > Utilities > Memory Monitor) as the sequence progresses? I suspect that the bursts you are observing happen because the queue is filling up, forcing the output handler to synchronously write out one or more slices; you should see the memory use climb in a stair-step pattern until one of these bursts, when it should plummet back to nearly nothing.
> > 
> > Actually, thinking on it, the plummeting most likely will not coincide with the writes -- more likely, the images will stick around in memory until the garbage collector decides to run. So the aforementioned pattern should appear in some form or other, though the timing may not tell us very much.
> > 
> > 
> > A few other thoughts:
> > - Do any other programs make regular use of your hard drive? Perhaps Windows 7's automatic (background) defragmentation, or a filesystem-filter antivirus (like avast!, which scans files as they are read and written)? The writing thread is given a low priority so the acquisition can run freely, but this might cause it to be blocked outright by low-level processes making regular use of the hard drive.
> > - Is your RAM mostly free when you start the acquisition? The maximum length of the image queue is decided based on the available memory at the beginning of an acquisition; I don't yet have a display for it, but it might be being set too low...
> > 
> > 
> > I'm working on a slight rework of the async handler which might help to fix these issues; I'll upload it to our update server once it's done. Of course, it's hard to gauge its efficacy until I can reproduce this behavior, so I'll see what I can do on that front as well.
> > 
> > 
> > Thanks again,
> > Luke
> > 
> > 
> > On 08/20/13, Alexis Maizel wrote:
> >> Hi Luke,
> >> 
> >>> Did you re-save this data, or are these stacks fresh from the OpenSPIM plugin?
> >> 
> >> I scaled them down to 8bits and saved them on my Mac. You can get an original "shuffled" stack here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiff
> >> 
> >>> I ask because the OME-TIFF metadata have been clobbered in both stacks (you can check using the Bio-Formats importer); if this is fresh output, the plugin may have a serious error in its metadata generation. Alternatively, Bio-Formats might be out-of-date or acting up. At any rate, the image slices obviously shouldn't be written out of order, but even if they were to be written in the wrong order, were the OME metadata intact, Bio-Formats should be able to display the slices in the correct order (because now it has a table relating the slice's index in the file to its physical Z position).
> >> 
> >> I forced Fiji to open the fresh-from-openSPIM stacks on my mac using the Bio-format importer (updated) and the slices are still in the wrong order.
> >> 
> >>> The data isn't truly meant to be written in bursts -- ideally it would just chug along in the background... I'll try tweaking the async a little more; probably, the code is doing something incorrectly leading to this. As for the timepoint images, this too is very unusual -- I specifically have the writer finish writing any pending images before finalizing a stack. I'll look into this as well.
> >> 
> >> Definitively the writing to disk finishes way after the stack has finished to be acquired. 
> >> 
> >>>> Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty?) after some seconds, the window disappear and the recording has been aborted.
> >>> I'm aware of this; I've never been able to determine what that message box is trying to display. No warnings should be appearing during an abort, but this one mysteriously shows up. It's a low priority for me, however; it doesn't seem to affect the abort or change the data that was written (well, any more than aborting mid-acquisition already does).
> >> 
> >> I concur that it is not critical, I just wanted to point it out. Glad to read that at least some of what I am experiencing can be reproduced :-)
> >> 
> >>> Is everything completely up-to-date, including Bio-Formats? (Note that I haven't yet tried the Bio-Formats daily builds -- whether they will fix or break the plugin has yet to be seen.)
> >> 
> >> I will update everything and perform more tests.
> >> 
> >>> If so, the next time you're running an acquisition, could you try monitoring Fiji's memory usage (Plugins > Utilities > Memory Monitor) as the sequence progresses? I suspect that the bursts you are observing happen because the queue is filling up, forcing the output handler to synchronously write out one or more slices; you should see the memory use climb in a stair-step pattern until one of these bursts, when it should plummet back to nearly nothing.
> >> 
> >> I'll do that and get back to you.
> >> 
> >> Thanks for your help,
> >> 
> >> Alexis
> >> 
> >> 
> >> 
> >>> 
> >>> 
> >>> Thanks,
> >>> Luke
> >>> 
> >>> On 08/14/13, Alexis Maizel wrote:
> >>>> Hi Pavel,
> >>>> 
> >>>> I did some more tests this morning. 
> >>>> The asynchronous writing is indeed the culprit. You can get two stacks here: http://dl.dropbox.com/u/484859/async_ON_vs_OFF.zip
> >>>> 
> >>>> With async OFF: the images are written to the disk as soon as they are acquired; it is slow but the planes are in the right order.
> >>>> 
> >>>> With async ON: the stack is acquired and written 'in bursts' to the disk. Sometimes there is a gap of several seconds between two bursts of disk writing and I have the feeling this corresponds to points when the planes are written in the wrong order. I have also noticed that irrespective of the time delay between two time lapse acquisition, the last image(s) of time point N are written to the disk instant before the time point N+1 starts to be acquired. Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty?) after some seconds, the window disappear and the recording has been aborted. 
> >>>> So I do not know, what's wrong but something does not seem right in this 'asynchronous writing' option, at least on our config:
> >>>> Windows 7 pro
> >>>> 1024Mb of RAM allocated to Fiji (we can not allocate more, otherwise the Orca won't operate)
> >>>> JRE 1.6.0
> >>>> everything 32bits version
> >>>> 
> >>>> Would be great to get that fixed :-)
> >>>> 
> >>>> With my best regards,
> >>>> 
> >>>> Alexis
> >>>> 
> >>>> 
> >>>> On 14 Aug 2013, at 00:30, Pavel Tomancak <tomancak at mpi-cbg.de> wrote:
> >>>> 
> >>>>> Hi Alexis,
> >>>>> 
> >>>>> That is very strange. I have never seen that. We made some acquisitions today and nothing like that was going on. Its obviously a serious issue. Does it happen only when you have the asynchronous writing enabled? I assume you are running on Windows 7.
> >>>>> 
> >>>>> All the best
> >>>>> 
> >>>>> PAvel
> >>>>> 
> >>>>> -----------------------------------------------------------------------------------
> >>>>> Pavel Tomancak, Ph.D.
> >>>>> 
> >>>>> Group Leader
> >>>>> Max Planck Institute of Molecular Cell Biology and Genetics
> >>>>> Pfotenhauerstr. 108
> >>>>> D-01307 Dresden Tel.: +49 351 210 2670
> >>>>> Germany Fax: +49 351 210 2020
> >>>>> tomancak at mpi-cbg.de
> >>>>> http://www.mpi-cbg.de
> >>>>> -----------------------------------------------------------------------------------
> >>>>> 
> >>>>> 
> >>>>> 
> >>>>> On Aug 13, 2013, at 8:14 PM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:
> >>>>> 
> >>>>>> Hi,
> >>>>>> 
> >>>>>> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
> >>>>>> 
> >>>>>> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
> >>>>>> 
> >>>>>> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
> >>>>>> 
> >>>>>> With my best regards,
> >>>>>> 
> >>>>>> Alexis
> >>>>>> 
> >>>>>> _______________________________________________
> >>>>>> OpenSPIM mailing list
> >>>>>> OpenSPIM at openspim.org
> >>>>>> http://openspim.org/mailman/listinfo/openspim
> >>>>>


From stuyvenberg at wisc.edu  Thu Aug 22 08:17:54 2013
From: stuyvenberg at wisc.edu (Luke Stuyvenberg)
Date: Thu, 22 Aug 2013 08:17:54 -0500
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <77108c785febc.52160f81@wiscmail.wisc.edu>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<l.wisc.edu@wiscmail.wisc.edu>
	<7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
	<F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>
	<77509ad34a859.5213a0a0@wiscmail.wisc.edu>
	<747093234c4f4.5213a0dc@wiscmail.wisc.edu>
	<76f0c89648b4c.5213a119@wiscmail.wisc.edu>
	<77809ad24bb9a.5213a155@wiscmail.!>
	<wisc.edu@wiscmail.wisc.edu> <7790f9e84a27f.5213a192@wiscmail.wisc.edu>
	<740096c04dc83.5213a20b@wiscmail.wisc.edu>
	<75e0ec504df4a.5213a247@wiscmail.wisc.edu>
	<75b0a0ad4a219.5213a284@wiscmail.wisc.edu>
	<7530ccce48e90.5213a2c0@wiscmail.wisc.edu>
	<75b0891f4e792.5213a2fd@wiscmail.wisc.edu>
	<599178A9-1BFA-4F1D-895B-76A66D43DAB3@cos.uni-heidelberg.de>
	<75f0ddef53ce9.5214d523@wiscmail.wisc.edu>
	<75e0816d56602.5214d55f@wiscmail.wisc.edu>
	<75f0a2a851b6f.5214d59c@wiscmail.wisc.edu>
	<75f0a40057606.5214d5d8@wiscmail.wisc.edu>
	<75f0e3af52dad.5214d615@wiscmail.wisc.edu>
	<75f0e06155195.5214d651@wiscmail.wisc.edu>
	<75b0e6bb51516.5214d68d@wiscmail.wisc.edu>
	<7470deb554615.5214d6ca@wiscmail.wisc.edu>
	<7630f202501e0.5214d706@wiscmail.wisc.edu>
	<775092f1523b2.5214d743@wiscmail.wisc.edu>
	<7790d02e50e94.5214d77f@wiscmail.wisc.edu>
	<7790c21c574d1.5214d7bc@wiscmail.wisc.edu>
	<7630a4b953c61.5214d7f8@wiscmail.wisc.edu>
	<778083d557c6a.5214d835@wiscmail.wisc.edu>
	<75b0d75851ca2.5214d871@wiscmail.wisc.edu>
	<76309e0255df7.5214d8ae@wiscmail.wisc.edu>
	<7400e434506ad.5214db4b@wiscmail.wisc.edu>
	<7780ee1c522d5.5214dbc3@wiscmail.wisc.edu>
	<75f0c12f51490.5214dbff@wiscmail.wisc.edu>
	<7470df7753be2.5214dc3c@wiscmail.wisc.edu>
	<75b09fac570df.5214dda5@wiscmail.wisc.edu>
	<75e0d65b50858.5214dde2@wiscmail.wisc.edu>
	<7630adff547c9.5214de1e@wiscmail.wisc.edu>
	<7630f58a55128.5214de5b@wiscmail.wisc.edu>
	<75f0ad0e57b91.5214981f@wiscmail.wisc.edu>
	<7790bc795f1c7.52160ecc@wiscmail.wisc.edu>
	<76d0b5005ff34.52160f08@wiscmail.wisc.edu>
	<7640976e5c55d.52160f44@wiscmail.wisc.edu>
	<77108c785febc.52160f81@wiscmail.wisc.edu>
Message-ID: <7690dfdf5d53d.5215c932@wiscmail.wisc.edu>

Hi Alexis,

Here's the email I sent, sans the rest of the conversation thread (which may have been the cause of the rejection).


Thanks,
Luke


P.S. The source code for the linked plugin is available, but split up into two branches on github -- async-fix and profiling -- in case anyone is looking for it. At the moment, neither merges cleanly into the main branch, since I've just merged the new device manager code yesterday.

On 08/21/13, "Luke Stuyvenberg"  wrote:
> Hi Alexis,
> 
> > some more tests and hopefully some progresses. I scanned a 82Mb stack (1024*1024*41 @ 16bits)
> >?
> > - RAM usage by Fiji and stack writing to disk:?
> > ~20Mb used at start (out of 1Gb allocated)?
> > as the scan progresses in ramps up to ~110Mb and then slowly decreases as the size of the file on disk increases; finally the garbage collector resets to 20Mb
> > it takes up to 2 minutes to write the 84Mb and at least 1 minute more to write the remaining 2Mb (the metadata, I presume...)
> Okay; although the writing times are not at all ideal (my 400MB stacks were written synchronously and in their entirety in a little over a minute and a half), the RAM usage is more or less as expected.
> 
> 
> > If you open that stack & metadata, you will see that the planes are not written in the right order. Look at the sequence (toward the end of the file above):
> 
> I have some mixed feelings on this; this is the definitive proof I was looking for, but I'm genuinely at a loss as to how this may have come about. My best guess is that the synchronization is too loose, and somehow the acquisition ("main") thread is writing slices (because it feels it needs to) before the writing thread.
> 
> 
> > I have the feeling that my problems are somehow linked to Bio-Format	not behaving properly.?
> 
> I've tried the new Bio-Formats daily builds update site through the Fiji updater, and haven't noticed any problems from it; perhaps try enabling that update site and downloading the very latest versions?
> 
> 
> Finally, I have most of the promised plugin updates. There are two main additions:
> - First, it does some timing over the acquisition, outputting the results to the log; although it isn't very detailed, it gives a breakdown of how time is used while acquiring (for example, if the async isn't appropriately asynchronous, the acquisition time will be roughly the same as though it were off).
> - Second, and more importantly for you, the async writer has been completely revamped. It now queues the begin/end stack events as well as new slices, so these operations shouldn't *ever* get out of order. It also puts up a monitor showing the state of its queue, which will let us know if it's filling up or not.
> If you're willing to play guinea pig again, please try replacing mmplugins/SPIMAcquisition.jar with the version from: https://dl.dropboxusercontent.com/u/57890359/SPIMAcquisition.jar
> Anyone is free to try this JAR out, but I haven't put it on our Fiji update site for a few reasons -- the new features are only conditionally useful, and can't (yet) be disabled. That said, if the new async fixes your output problems, I'll polish these changes and incorporate them.
> 
> 
> Thanks,
> Luke
> 
>


From Alexis.Maizel at cos.uni-heidelberg.de  Thu Aug 22 10:31:30 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Thu, 22 Aug 2013 17:31:30 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <7690dfdf5d53d.5215c932@wiscmail.wisc.edu>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<l.wisc.edu@wiscmail.wisc.edu> !
	<7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
	<F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>
	<77509ad34a859.5213a0a0@wiscmail.wisc.edu>
	<747093234c4f4.5213a0dc@wiscmail.wisc.edu>
	<76f0c89648b4c.5213a119@wiscmail.wisc.edu>
	<77809ad24bb9a.5213a155@wiscmail.!>
	<wisc.edu@wiscmail.wisc.edu> <7!
	790f9e84a27f.5213a192@wiscmail.wisc.edu> <740096c04dc83.5213a2!
	0b@wiscmail.wisc.edu> <75e0ec504df4a.5213a247@wiscmail.wisc.edu>
	<75b0a0ad4a219.5213a284@wiscmail.wisc.edu>
	<7530ccce48e90.5213a2c0@wiscmail.wisc.edu>
	<75b0891f4e792.5213a2fd@wiscmail.wisc.edu>
	<599178A9-1BFA-4F1D-895B-76A66D43DAB3@cos.uni-heidelberg.de>
	<75f0ddef
Message-ID: <82A3F18D-4C6E-40C0-B674-37C4F04C2DA0@cos.uni-heidelberg.de>

Hi Luke,
thanks a lot! I'll test the new SPIMAcquisition and let you know. 
With my best regards,

Alexis

On 22 Aug 2013, at 15:17, Luke Stuyvenberg <stuyvenberg at wisc.edu> wrote:

> Hi Alexis,
> 
> Here's the email I sent, sans the rest of the conversation thread (which may have been the cause of the rejection).
> 
> 
> Thanks,
> Luke
> 
> 
> P.S. The source code for the linked plugin is available, but split up into two branches on github -- async-fix and profiling -- in case anyone is looking for it. At the moment, neither merges cleanly into the main branch, since I've just merged the new device manager code yesterday.
> 
> On 08/21/13, "Luke Stuyvenberg"  wrote:
>> Hi Alexis,
>> 
>>> some more tests and hopefully some progresses. I scanned a 82Mb stack (1024*1024*41 @ 16bits)
>>>  
>>> - RAM usage by Fiji and stack writing to disk: 
>>> ~20Mb used at start (out of 1Gb allocated) 
>>> as the scan progresses in ramps up to ~110Mb and then slowly decreases as the size of the file on disk increases; finally the garbage collector resets to 20Mb
>>> it takes up to 2 minutes to write the 84Mb and at least 1 minute more to write the remaining 2Mb (the metadata, I presume...)
>> Okay; although the writing times are not at all ideal (my 400MB stacks were written synchronously and in their entirety in a little over a minute and a half), the RAM usage is more or less as expected.
>> 
>> 
>>> If you open that stack & metadata, you will see that the planes are not written in the right order. Look at the sequence (toward the end of the file above):
>> 
>> I have some mixed feelings on this; this is the definitive proof I was looking for, but I'm genuinely at a loss as to how this may have come about. My best guess is that the synchronization is too loose, and somehow the acquisition ("main") thread is writing slices (because it feels it needs to) before the writing thread.
>> 
>> 
>>> I have the feeling that my problems are somehow linked to Bio-Format	not behaving properly. 
>> 
>> I've tried the new Bio-Formats daily builds update site through the Fiji updater, and haven't noticed any problems from it; perhaps try enabling that update site and downloading the very latest versions?
>> 
>> 
>> Finally, I have most of the promised plugin updates. There are two main additions:
>> - First, it does some timing over the acquisition, outputting the results to the log; although it isn't very detailed, it gives a breakdown of how time is used while acquiring (for example, if the async isn't appropriately asynchronous, the acquisition time will be roughly the same as though it were off).
>> - Second, and more importantly for you, the async writer has been completely revamped. It now queues the begin/end stack events as well as new slices, so these operations shouldn't *ever* get out of order. It also puts up a monitor showing the state of its queue, which will let us know if it's filling up or not.
>> If you're willing to play guinea pig again, please try replacing mmplugins/SPIMAcquisition.jar with the version from: https://dl.dropboxusercontent.com/u/57890359/SPIMAcquisition.jar
>> Anyone is free to try this JAR out, but I haven't put it on our Fiji update site for a few reasons -- the new features are only conditionally useful, and can't (yet) be disabled. That said, if the new async fixes your output problems, I'll polish these changes and incorporate them.
>> 
>> 
>> Thanks,
>> Luke
>> 
>> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130822/0feec6aa/attachment.pgp>

From Alexis.Maizel at cos.uni-heidelberg.de  Fri Aug 23 04:34:18 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Fri, 23 Aug 2013 11:34:18 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <7690dfdf5d53d.5215c932@wiscmail.wisc.edu>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<l.wisc.edu@wiscmail.wisc.edu> !
	<7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
	<F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>
	<77509ad34a859.5213a0a0@wiscmail.wisc.edu>
	<747093234c4f4.5213a0dc@wiscmail.wisc.edu>
	<76f0c89648b4c.5213a119@wiscmail.wisc.edu>
	<77809ad24bb9a.5213a155@wiscmail.!>
	<wisc.edu@wiscmail.wisc.edu> <7!
	790f9e84a27f.5213a192@wiscmail.wisc.edu> <740096c04dc83.5213a2!
	0b@wiscmail.wisc.edu> <75e0ec504df4a.5213a247@wiscmail.wisc.edu>
	<75b0a0ad4a219.5213a284@wiscmail.wisc.edu>
	<7530ccce48e90.5213a2c0@wiscmail.wisc.edu>
	<75b0891f4e792.5213a2fd@wiscmail.wisc.edu>
	<599178A9-1BFA-4F1D-895B-76A66D43DAB3@cos.uni-heidelberg.de>
	<75f0ddef
Message-ID: <C652514F-B972-49DE-81BC-0ADBFD36AFEB@cos.uni-heidelberg.de>

Hi Luke,

I did some test and have good news: the new SPIMAcquisition.jar solves the issue. The slices are now reliably written in the right order. I also have the feeling it is a bit faster than the 'original/old' one and the metadata a properly written. 

You can get the stacks and logs of my tests here: http://dl.dropbox.com/u/484859/2013-08-23.zip

I did 3 acquisitions:
- test 1: A  stack of 40 slices (1024*1024)  with the original  SPIMAcquisition.jar; the slices are mixed up. You will also notice in the log.txt file the message "Warning: asynchronous writer may have been cancelled before completing. (0)" I had that everytime the metadata are not written. I did not hit "abort" at any time; after acquisition I simply waited for the file to finish writing to disk (a couple of minutes)  and opened it in Fiji (then the message appeared in the log).

-test2: same thing, but with the new SPIMAcquisition.jar. Everything is fine (slices in the right order & metadata present). I observed the async monitor: it climbed to 27/240 (writing) then slowly decreased to 0/240 (Idle). Overall it felt faster.

-test3: time lapse (5 time steps) of  a 40-slices stack; one stack every 120sec. Absolutely no problems. The async monitor showed the same behaviour as for test 2.    

I'll keep on using this version until the next update. 

Thanks a lot for your help.

With my best regards,

Alexis


>> - First, it does some timing over the acquisition, outputting the results to the log; although it isn't very detailed, it gives a breakdown of how time is used while acquiring (for example, if the async isn't appropriately asynchronous, the acquisition time will be roughly the same as though it were off).
>> - Second, and more importantly for you, the async writer has been completely revamped. It now queues the begin/end stack events as well as new slices, so these operations shouldn't *ever* get out of order. It also puts up a monitor showing the state of its queue, which will let us know if it's filling up or not.
>> If you're willing to play guinea pig again, please try replacing mmplugins/SPIMAcquisition.jar with the version from: https://dl.dropboxusercontent.com/u/57890359/SPIMAcquisition.jar
>> Anyone is free to try this JAR out, but I haven't put it on our Fiji update site for a few reasons -- the new features are only conditionally useful, and can't (yet) be disabled. That said, if the new async fixes your output problems, I'll polish these changes and incorporate them.
>> 
>> 
>> Thanks,
>> Luke
>> 
>> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130823/aee6240a/attachment.pgp>

From stuyvenberg at wisc.edu  Fri Aug 23 14:49:30 2013
From: stuyvenberg at wisc.edu (LUKE ADAM STUYVENBERG)
Date: Fri, 23 Aug 2013 14:49:30 -0500
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <C652514F-B972-49DE-81BC-0ADBFD36AFEB@cos.uni-heidelberg.de>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
	<F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>
	<77509ad34a859.5213a0a0@wiscmail.wisc.edu>
	<747093234c4f4.5213a0dc@wiscmail.wisc.edu>
	<76f0c89648b4c.5213a119@wiscmail.wisc.edu>
	<77809ad24bb9a.5213a155@wiscmail.!>
	<wisc.edu@wiscmail.wisc.edu> <75e0ec504df4a.5213a247@wiscmail.wisc.edu>
	<75b0a0ad4a219.5213a284@wiscmail.wisc.edu>
	<7530ccce48e90.5213a2c0@wiscmail.wisc.edu>
	<75b0891f4e792.5213a2fd@wiscmail.wisc.edu>
	<599178A9-1BFA-4F1D-895B-76A66D43DAB3@cos.uni-heidelberg.de>
	<75f0ddef@wiscmail.wisc.edu>
	<C652514F-B972-49DE-81BC-0ADBFD36AFEB@cos.uni-heidelberg.de>
Message-ID: <4b0909c42b9b27fdb84a5be0b48f742e@wiscmail.wisc.edu>

Hi Alexis,

> I did some test and have good news: the new SPIMAcquisition.jar solves the
> issue. The slices are now reliably written in the right order. I also have
> the feeling it is a bit faster than the 'original/old' one and the metadata a
> properly written.
Excellent!

> -test2: same thing, but with the new SPIMAcquisition.jar. Everything is fine
> (slices in the right order & metadata present). I observed the async monitor:
> it climbed to 27/240 (writing) then slowly decreased to 0/240 (Idle). Overall
> it felt faster.
My async monitor still rarely exceeds 1 before writing it out, though I've
noticed that sometimes it snowballs (i.e. if it doesn't get one written quickly
enough, they start to pile up until a break in the acquisition). Still, 27/240
gives lots of room, and that limit is pretty conservative. It's good to know
things are working properly now, and the associated speed boost is also a big
plus.

> I'll keep on using this version until the next update.
If I don't get the update uploaded today, it will almost certainly be Monday. At
this point my biggest obstacle is deciding where to put the checkboxes to make
it show the monitor. ;-)

I should warn you in advance, though I'll try to make it clear when I announce
the update: the next update *will* obsolete the changes you made to the Cobalt
adapter. The reason is because it will be designed to work with the Cobalt
adapter in MM's nightly build -- I'm afraid you will need to revert your
changes.

Thanks again for experimenting, and for bringing this to light!

Luke

On Fri, 23 Aug 2013 11:34:18 +0200 Alexis Maizel
<Alexis.Maizel at cos.uni-heidelberg.de> wrote

> Hi Luke,
> 
> I did some test and have good news: the new SPIMAcquisition.jar solves the
> issue. The slices are now reliably written in the right order. I also have
> the feeling it is a bit faster than the 'original/old' one and the metadata a
> properly written.
> 
> You can get the stacks and logs of my tests here:
> http://dl.dropbox.com/u/484859/2013-08-23.zip
> 
> I did 3 acquisitions:
> - test 1: A  stack of 40 slices (1024*1024)  with the original 
> SPIMAcquisition.jar; the slices are mixed up. You will also notice in the
> log.txt file the message "Warning: asynchronous writer may have been
> cancelled before completing. (0)" I had that everytime the metadata are not
> written. I did not hit "abort" at any time; after acquisition I simply waited
> for the file to finish writing to disk (a couple of minutes)  and opened it
> in Fiji (then the message appeared in the log).
> 
> -test2: same thing, but with the new SPIMAcquisition.jar. Everything is fine
> (slices in the right order & metadata present). I observed the async monitor:
> it climbed to 27/240 (writing) then slowly decreased to 0/240 (Idle). Overall
> it felt faster.
> 
> -test3: time lapse (5 time steps) of  a 40-slices stack; one stack every
> 120sec. Absolutely no problems. The async monitor showed the same behaviour
> as for test 2.
> 
> I'll keep on using this version until the next update.
> 
> Thanks a lot for your help.
> 
> With my best regards,
> 
> Alexis
> 
> 
> >> - First, it does some timing over the acquisition, outputting the results
> >> to the log; although it isn't very detailed, it gives a breakdown of how
time
> >> is used while acquiring (for example, if the async isn't appropriately
> >> asynchronous, the acquisition time will be roughly the same as though it
were
> >> off).
> >> - Second, and more importantly for you, the async writer has been
> >> completely revamped. It now queues the begin/end stack events as well as
new
> >> slices, so these operations shouldn't *ever* get out of order. It also puts
> >> up a monitor showing the state of its queue, which will let us know if it's
> >> filling up or not.
> >> If you're willing to play guinea pig again, please try replacing
> >> mmplugins/SPIMAcquisition.jar with the version from:
> >> https://dl.dropboxusercontent.com/u/57890359/SPIMAcquisition.jar
> >> Anyone is free to try this JAR out, but I haven't put it on our Fiji
> >> update site for a few reasons -- the new features are only conditionally
> >> useful, and can't (yet) be disabled. That said, if the new async fixes your
> >> output problems, I'll polish these changes and incorporate them.
> >>
> >>
> >> Thanks,
> >> Luke
> >>
> >>




From stuyvenberg at wisc.edu  Mon Aug 26 08:59:00 2013
From: stuyvenberg at wisc.edu (LUKE ADAM STUYVENBERG)
Date: Mon, 26 Aug 2013 08:59:00 -0500
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <7CD33490-933E-4455-805D-11F755A6C170@cos.uni-heidelberg.de>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<7720e3744fa3d.52122391@wiscmai!> <l.wisc.edu@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
	<F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>
	<77509ad34a859.5213a0a0@wiscmail.wisc.edu>
	<747093234c4f4.5213a0dc@wiscmail.wisc.edu>
	<76f0c89648b4c.5213a119@wiscmail.wisc.edu>
	<77809ad24bb9a.5213a155@wiscmail.!>
	<wisc.edu@wiscmail.wisc.edu> <75e0ec504df4a.5213a247@wiscmail.!>
	<wisc.edu@wiscmail.wisc.edu> <75b0a0ad4a219.5213a284@wiscmail.wisc.edu>
	<75b0891f4e792.5213a2fd@wiscmail.wisc.edu>
	<599178A9-1BFA-4F1D-895B-76A66D43DAB3@cos.uni-heidelberg.de>
	<75f0ddef@wiscmail.wisc.edu>
	<C652514F-B972-49DE-81BC-0ADBFD36AFEB@cos.uni-heidelberg.de>
	<4b0909c42b9b27fdb84a5be0b48f742e@wi>
	<7CD33490-933E-4455-805D-11F755A6C170@cos.uni-heidelberg.de>
Message-ID: <1e1d0fea36bf731d5f52f2bb936256c1@wiscmail.wisc.edu>

Hi Alexis,

On Mon, 26 Aug 2013 08:44:26 0200 Alexis Maizel wrote
> The next OpenSPIM update will use a different code base than the current one
> (1.4.x dev)? So unless I commit my update to the MM repository and get them
> part of the nightly build, it won't work, right?
Actually, what I meant was that from now on, the OpenSPIM plugin will try to
interact with the Cobolt device as though it were unchanged -- it will not
treat it as a shutter, and when altering laser power, it will attempt to set
the "Power" property, not "PowerSetpoint" (and so, having the property be named
"PowerSetpoint" even though the laser is a Cobolt will greatly confuse the
plugin). Essentially, the changes I'm making mean that the plugin will interact
with each device adapter differently.

By all means, though, commit your changes -- adapting the plugin to changes in
device adapters is very little work (which is the part of the point of this
update), so if your changes become 'canonical', I can easily change the
SPIMAcquisition plugin to support them.

I'm not planning to update the MM code base just yet -- as far as I am aware,
the nightly build's device adapters still work with the OpenSPIM version.

Thanks,
Luke

On Mon, 26 Aug 2013 08:44:26 0200 Alexis Maizel wrote

> Hi Luke,
> 
> > I should warn you in advance, though I'll try to make it clear when I
> > announce
> > the update: the next update *will* obsolete the changes you made to the
> > Cobalt
> > adapter. The reason is because it will be designed to work with the Cobalt
> > adapter in MM's nightly build -- I'm afraid you will need to revert your
> > changes.
> 
> I am not sure, I am following you here
> The next OpenSPIM update will use a different code base than the current one
> (1.4.x dev)? So unless I commit my update to the MM repository and get them
> part of the nightly build, it won't work, right?
> My amendments to the Cobolt code are now tested and stable, so I feel
> confident to have them added to the repository. I'll talk to Karl Llave.
> 
> With my best regards,
> 
> Alexis




From j.krieger at dkfz-heidelberg.de  Tue Aug 27 08:05:15 2013
From: j.krieger at dkfz-heidelberg.de (Jan Krieger)
Date: Tue, 27 Aug 2013 15:05:15 +0200
Subject: [OpenSPIM] beadscans
Message-ID: <521CA40B.3020606@dkfz-heidelberg.de>

HI everybody!

I put some details on our homepage, how we perform beadscans to look at 
the PSF of our SPIM (we do SPIM-FCS, so that's rather important) in our 
group:

    http://www.dkfz.de/Macromol/quickfit/beadscan.html

We do not have an openSPIM, but a homebuilt, standard-SPIM 
(http://www.dkfz.de/Macromol/research/spim.html), specialized for 
single-cell measurements, but still the script should also work for data 
from an openSPIM.

I also also added a link to the openSPIM wiki.

Best from Heidelberg,
JAN




From stuyvenberg at wisc.edu  Wed Aug 28 08:49:25 2013
From: stuyvenberg at wisc.edu (Luke Stuyvenberg)
Date: Wed, 28 Aug 2013 08:49:25 -0500
Subject: [OpenSPIM] Plugin update
Message-ID: <8e80694b79413fd56a5e25c8259ffeda@wiscmail.wisc.edu>

Hi everyone,

 I've just uploaded a new version of the plugin to the Fiji update site. Some
quick features:

 - Rebuilt device interactions to be less dependent on specific
devices/properties/et cetera
 - Overhauled asynchronous output to be more reliable and transparent
 - Added some rudimentary acquisition profiling, for the curious

 As always, I do my best to make sure that the updates are free of any
use-impairing bugs or issues. However, and especially with this update, this
is not always possible. Therefore, if you have any project-critical imaging to
do, please do not update until you have finished.

 I mention this because this update completely revises the way the plugin
interacts with Micro-Manager devices, letting us easily add new features and
supported hardware. However, this also means that, for certain features, the
plugin now expects a limited subset of devices, and might cause problems with
others -- in particular, anyone not using the OpenSPIM 1.0 specification might
run into trouble.

 On the other hand, if you _don't_ have any project-critical imaging, I would
greatly appreciate it if you could test the plugin on your setup. As I can
personally only test the OpenSPIM 1.0 setup, I don't yet know if the plugin
will operate seamlessly with variants on the setup. If you can, please test
the new plugin and get back to me!

 The rest of this e-mail describes the pros, cons, capabilities, and
limitations of the new code, and might only be of interest if the update is
giving you trouble (or if you're just curious about these sorts of things. ;-)
).

 Thanks,
 Luke Stuyvenberg

-------------------------

The plugin should still work with any hardware compatible with Micro-Manager,
but it will not be able to use any features not provided by the MM API. For
example, it will not be able to determine the maximum position of most stages,
and will be unable to control their velocity, or the laser power of most
lasers. (These will need to be controlled another way, i.e. through the device
property browser in Micro-Manager.)

 _What it _can_ do_: The plugin knows the attributes (minimum, maximum, and
step size) of Picard stages, and can control the Z-stage's velocity. It also
knows the minimum and maximum laser power of Coherent Cube laser devices, and
can control that.

 _What it _should_ be able to do_: Additionally, it should be able to control
the laser power of Cobolt laser devices (although the range is hard-coded to 0
- 50 mW).

 I don't mention cameras because, as yet, the plugin doesn't take advantage of
any camera features not available through the MM API. As a result, it should
be able to use any camera MM can use without trouble.

 If your hardware is having problems with the update, feel free to e-mail the
list with whatever issues you're running into. You can also add support for
your device yourself: download our development environment from
http://openspim.org/How_to_build_the_software -- from there, take a look at
plugins/SPIMAcquisition/src/main/java/spim/setup/PicardStage.java for an
example of the code the new device manager runs on. I only ask that such
daring individuals as do so eventually submit a pull request on github, or
contact the list with their new code, so we can extend this support to
everyone using the software.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130828/29561f6e/attachment.html>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: message_1.1.txt
URL: <http://openspim.org/pipermail/openspim/attachments/20130828/29561f6e/attachment.txt>

From stuyvenberg at wisc.edu  Wed Aug 28 09:02:43 2013
From: stuyvenberg at wisc.edu (Luke Stuyvenberg)
Date: Wed, 28 Aug 2013 09:02:43 -0500
Subject: [OpenSPIM] Plugin update (again)
Message-ID: <521E0303.50704@wisc.edu>

Sorry about the cut-off email; here is (hopefully) the message in its 
entirety:
------------------------------------------------------------------------
Hi everyone,

I've just uploaded a new version of the plugin to the Fiji update site. 
Some quick features:

- Rebuilt device interactions to be less dependent on specific 
devices/properties/et cetera
- Overhauled asynchronous output to be more reliable and transparent
- Added some rudimentary acquisition profiling, for the curious

As always, I do my best to make sure that the updates are free of any 
use-impairing bugs or issues. However, and especially with this update, 
this is not always possible. Therefore, if you have any project-critical 
imaging to do, please do not update until you have finished.

I mention this because this plugin update completely revises the way 
OpenSPIM interacts with Micro-Manager devices, letting us easily add new 
features and supported hardware. However, this also means that, for 
certain features, the plugin now expects a limited subset of devices, 
and might cause problems with others -- in particular, anyone not using 
the OpenSPIM 1.0 specification on the site might run into trouble.

On the other hand, if you /don't/ have any project-critical imaging, I 
would greatly appreciate it if you could test the plugin on your setup. 
As I can personally only test the OpenSPIM 1.0 setup, I don't yet know 
if the plugin will operate seamlessly with variants on the setup. If you 
can, please test the new plugin and get back to me!

The rest of this e-mail describes the pros, cons, capabilities, and 
limitations of the new code, and might only be of interest if the update 
is giving you trouble (or if you're just curious about these sorts of 
things. ;-) ).

Thanks,
Luke Stuyvenberg
------------------------------------------------------------------------
The plugin should still work with any hardware compatible with 
Micro-Manager, but it will not be able to use any features not provided 
by the MM API. For example, it will not be able to determine the maximum 
position of most stages, and will be unable to control their velocity, 
or the laser power of most lasers. (These will need to be controlled 
another way, i.e. through the device property browser in Micro-Manager.)

/What it /can/do/: The plugin knows the attributes (minimum, maximum, 
and step size) of Picard stages, and can control the Z-stage's velocity. 
It also knows the minimum and maximum laser power of Coherent Cube laser 
devices, and can control that.

/What it /should/be able to do/: Additionally, it should be able to 
control the laser power of Cobolt laser devices (although the range is 
hard-coded to 0 - 50 mW).

I don't mention cameras because, as yet, the plugin doesn't take 
advantage of any camera features not available through the MM API. As a 
result, it should be able to use any camera MM can use without trouble.

If your hardware is having problems with the update, feel free to e-mail 
the list with whatever issues you're running into. You can also add 
support for your device yourself: download our development environment 
from http://openspim.org/How_to_build_the_software -- from there, take a 
look at 
plugins/SPIMAcquisition/src/main/java/spim/setup/PicardStage.java for an 
example of the code the new device manager runs on. I only ask that such
daring individuals as do so eventually submit a pull request on github, 
or contact the list with their new code, so we can extend this support 
to everyone using the software.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130828/6cc9f960/attachment.html>

From j.krieger at dkfz-heidelberg.de  Mon Aug 26 10:56:51 2013
From: j.krieger at dkfz-heidelberg.de (Jan Krieger)
Date: Mon, 26 Aug 2013 17:56:51 +0200
Subject: [OpenSPIM] beadscans
Message-ID: <521B7AC3.4010409@dkfz-heidelberg.de>

HI everybody!

I put some details on our homepage, how we perform beadscans to look at 
the PSF of our SPIM (we do SPIM-FCS, so that's rather important) in our 
group:

   http://www.dkfz.de/Macromol/quickfit/beadscan.html

We do not have an openSPIM, but a homebuilt, standard-SPIM 
(http://www.dkfz.de/Macromol/research/spim.html), specialized for 
single-cell measurements, but still the script should also work for data 
from an openSPIM.

I also also added a link to the openSPIM wiki.

Best from Heidelberg,
JAN

-------------- next part --------------
A non-text attachment was scrubbed...
Name: j_krieger.vcf
Type: text/x-vcard
Size: 408 bytes
Desc: not available
URL: <http://openspim.org/pipermail/openspim/attachments/20130826/53d1728a/attachment.vcf>

From j.krieger at Dkfz-Heidelberg.de  Sat Aug 31 02:22:52 2013
From: j.krieger at Dkfz-Heidelberg.de (Krieger, Jan)
Date: Sat, 31 Aug 2013 09:22:52 +0200
Subject: [OpenSPIM] lasers for SPIM
Message-ID: <4F91222C659D5B4E8C89CE4D710BB08EF1440C604F@DKFZEX01.ad.dkfz-heidelberg.de>

Hi everybody!

as one of our lasers (491nm) died down yesterday, I wanted to ask, which lasers you prefer for the openSPIM:

- So we use Cobolt DPSS lasers, which can not be modulated
- If we can't repair our laser, I thought about getting one of these:
   1. a Cobolt MLD http://www.cobolt.se/coboltmld.html
   2. Coherent Cube http://www.coherent.com/products/?1007/CUBE
   3. Coherent Saphire http://www.coherent.com/products/?1638/Sapphire-Lasers
   4. Coherent OBIS http://www.coherent.com/products/?1884/OBIS-Lasers

I would appreciate any help/hints you can give me on these (and possibly other) lasers, that you think suitable for SPIM.

Best,
JAN


Dipl.-Phys. Jan Krieger
German Cancer Research Center (dkfz)
Department B040 - Biophysics of Macromolecules (Prof. J. Langowski)
Im Neuenheimer Feld 580
69120 Heidelberg

fon: +49 / 6221 / 42-3395
fax: +49 / 6221 / 42-3391
e-mail: j.krieger at dkfz.de
www: http://www.dkfz.de/Macromol/

From edgar.escobar.nieto at ipt.fraunhofer.de  Mon Aug  5 04:55:08 2013
From: edgar.escobar.nieto at ipt.fraunhofer.de (edgar.escobar.nieto at ipt.fraunhofer.de)
Date: Mon, 5 Aug 2013 11:55:08 +0200
Subject: [OpenSPIM] About the calibration of the lightsheet
Message-ID: <OF43BC6A67.1EC5993C-ONC1257BBE.00367CAB-C1257BBE.00367CAD@ipt.rwth-aachen.de>

 
 _________________________________________________________________________

Fraunhofer-Institut fr Produktionstechnologie IPT 
Edgar Escobar Nieto  
 

 
 
Steinbachstrae 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de 
http://www.ipt.fraunhofer.de
_________________________________________________________________________
 
 
 


-----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 05.08.2013 11:54 -----
An: openspim at openspim.org
Von: Edgar Escobar Nieto/Fraunhofer IPT
Datum: 31.07.2013 15:02
Betreff: (Unbenannt)

 Hi dear all,

I reached to the point where I need to calibrate the light-sheet. And I was wondering
what is the optical density of the ND filter recommended to perform the calibration.

I would like to know also what is the Frequency (lp/mm) recommended for the Opal Glass Ronchi Ruling Slides.
I think that high a frequency would be better, but there must be a limit.

Only to be sure, what I understood is that these Ruling Slides have already a reflective surface, so there is no
need to glue a mirror to the ruling slides.

Thank you in advance for your comments.

Kind regards,
Edgar Escobar Nieto
 _________________________________________________________________________

Fraunhofer-Institut fr Produktionstechnologie IPT 
Edgar Escobar Nieto  
 

 
 
Steinbachstrae 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de 
http://www.ipt.fraunhofer.de
_________________________________________________________________________
 
 
 
 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130805/91fdefea/attachment-0002.html>

From edgar.escobar.nieto at ipt.fraunhofer.de  Tue Aug  6 10:08:39 2013
From: edgar.escobar.nieto at ipt.fraunhofer.de (edgar.escobar.nieto at ipt.fraunhofer.de)
Date: Tue, 6 Aug 2013 17:08:39 +0200
Subject: [OpenSPIM] About the calibration of the lightsheet
Message-ID: <OF15A84A94.5A9526D4-ONC1257BBF.005330CF-C1257BBF.005330D2@ipt.rwth-aachen.de>

 Hi dear All,

I would like to know how you could cut the ruling slides. I don't know if it is possible tu cut a thin part of glass using
a conventional glass cutter. 

Kind regards,
Edgar Escobar Nieto
 _________________________________________________________________________

Fraunhofer-Institut fr Produktionstechnologie IPT 
Edgar Escobar Nieto  
 

 
 
Steinbachstrae 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de 
http://www.ipt.fraunhofer.de
_________________________________________________________________________
 
 
 


-----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 06.08.2013 17:01 -----
An: openspim at openspim.org
Von: Edgar Escobar Nieto/Fraunhofer IPT
Datum: 05.08.2013 11:55
Betreff: About the calibration of the lightsheet

 
 _________________________________________________________________________

Fraunhofer-Institut fr Produktionstechnologie IPT 
Edgar Escobar Nieto  
 

 
 
Steinbachstrae 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de 
http://www.ipt.fraunhofer.de
_________________________________________________________________________
 
 
 


-----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 05.08.2013 11:54 -----
An: openspim at openspim.org
Von: Edgar Escobar Nieto/Fraunhofer IPT
Datum: 31.07.2013 15:02
Betreff: (Unbenannt)

 Hi dear all,

I reached to the point where I need to calibrate the light-sheet. And I was wondering
what is the optical density of the ND filter recommended to perform the calibration.

I would like to know also what is the Frequency (lp/mm) recommended for the Opal Glass Ronchi Ruling Slides.
I think that high a frequency would be better, but there must be a limit.

Only to be sure, what I understood is that these Ruling Slides have already a reflective surface, so there is no
need to glue a mirror to the ruling slides.

Thank you in advance for your comments.

Kind regards,
Edgar Escobar Nieto
 _________________________________________________________________________

Fraunhofer-Institut fr Produktionstechnologie IPT 
Edgar Escobar Nieto  
 

 
 
Steinbachstrae 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de 
http://www.ipt.fraunhofer.de
_________________________________________________________________________
 
 
 
 
 
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130806/5dba57fd/attachment-0002.html>

From pitrone at mpi-cbg.de  Tue Aug  6 11:22:13 2013
From: pitrone at mpi-cbg.de (Peter Gabriel Pitrone)
Date: Tue, 6 Aug 2013 18:22:13 +0200 (CEST)
Subject: [OpenSPIM] About the calibration of the lightsheet
In-Reply-To: <OF15A84A94.5A9526D4-ONC1257BBF.005330CF-C1257BBF.005330D2@ipt.rwth-aa
	chen.de>
References: <OF15A84A94.5A9526D4-ONC1257BBF.005330CF-C1257BBF.005330D2@ipt.rwth-aachen.de>
Message-ID: <60102.174.24.0.131.1375806133.squirrel@webmail.mpi-cbg.de>

Hello Edgar,

I'm sorry I didn't answer you till now, but I'm sometimes away from my
email as I am on vacation in the states.

To answer your questions, we broke the slide and found the best shard. It
is definitely not the most elegant way, but it works for us. If you want
to go about making it with more consistency with a glass or window
cutter, please write a protocol on our wiki on how to do it so others can
reproduce it.

If you need help with aligning the light sheet, we can make a skype
session tomorrow... just remember I have an 8 hour difference in
timezones. My Skype name is "petepitrone". send me an email as to the
time you would like to talk.

Pete

-- 
Peter Gabriel Pitrone - TechRMS
Microscopy/Imaging Specialist
Prof. Dr. Pavel Tomancak group
Max Planck Institute for
Molecular Biology and Genetics
Pfotenhauerstr. 108
01307 Dresden

"If a straight line fit is required, obtain only two data points." - Anon.


On Tue, August 6, 2013 5:08 pm, edgar.escobar.nieto at ipt.fraunhofer.de wrote:
<|>  Hi dear All,
<|>
<|> I would like to know how you could cut the ruling slides. I don't
know if
<|> it is possible tu cut a thin part of glass using
<|> a conventional glass cutter.
<|>
<|> Kind regards,
<|> Edgar Escobar Nieto
<|> 
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut fr Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstrae 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> -----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 06.08.2013
<|> 17:01 -----
<|> An: openspim at openspim.org
<|> Von: Edgar Escobar Nieto/Fraunhofer IPT
<|> Datum: 05.08.2013 11:55
<|> Betreff: About the calibration of the lightsheet
<|>
<|>
<|> 
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut fr Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstrae 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> -----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 05.08.2013
<|> 11:54 -----
<|> An: openspim at openspim.org
<|> Von: Edgar Escobar Nieto/Fraunhofer IPT
<|> Datum: 31.07.2013 15:02
<|> Betreff: (Unbenannt)
<|>
<|>  Hi dear all,
<|>
<|> I reached to the point where I need to calibrate the light-sheet. And I
<|> was wondering
<|> what is the optical density of the ND filter recommended to perform the
<|> calibration.
<|>
<|> I would like to know also what is the Frequency (lp/mm) recommended for
<|> the Opal Glass Ronchi Ruling Slides.
<|> I think that high a frequency would be better, but there must be a
limit.
<|>
<|> Only to be sure, what I understood is that these Ruling Slides have
<|> already a reflective surface, so there is no
<|> need to glue a mirror to the ruling slides.
<|>
<|> Thank you in advance for your comments.
<|>
<|> Kind regards,
<|> Edgar Escobar Nieto
<|> 
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut fr Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstrae 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> _______________________________________________
<|> OpenSPIM mailing list
<|> OpenSPIM at openspim.org
<|> http://openspim.org/mailman/listinfo/openspim
<|>





From huisken at mpi-cbg.de  Tue Aug  6 16:20:58 2013
From: huisken at mpi-cbg.de (Jan Huisken)
Date: Tue, 6 Aug 2013 23:20:58 +0200
Subject: [OpenSPIM] About the calibration of the lightsheet
In-Reply-To: <OF43BC6A67.1EC5993C-ONC1257BBE.00367CAB-C1257BBE.00367CAD@ipt.rwth-aachen.de>
References: <OF43BC6A67.1EC5993C-ONC1257BBE.00367CAB-C1257BBE.00367CAD@ipt.rwth-aachen.de>
Message-ID: <56A4F19F-C885-4041-BF78-BECDA36614E9@mpi-cbg.de>

Hi Edgar

> -----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 05.08.2013 11:54 -----
> An: openspim at openspim.org
> Von: Edgar Escobar Nieto/Fraunhofer IPT
> Datum: 31.07.2013 15:02
> Betreff: (Unbenannt)
> 
> Hi dear all,
> 
> I reached to the point where I need to calibrate the light-sheet. And I was wondering
> what is the optical density of the ND filter recommended to perform the calibration.

Since you are reflecting the laser right onto the camera you need to be sure that you can turn down the laser intensity far enough that you do not damage your camera. Therefore the ND depends on the lowest power you can set your laser to and the sensitivity of your camera. The ND filter can be introduced in the illumination or detection path. Some fluorescence emission filter may also do the job.

> 
> I would like to know also what is the Frequency (lp/mm) recommended for the Opal Glass Ronchi Ruling Slides.
> I think that high a frequency would be better, but there must be a limit.

The grid is used to (1) have some features on which you can focus and (2) to have some reflective parts the give you a profile of the beam. The distance does not matter very much. It is usually good if you have ca. 10 reflective lines across the entire FOV, which depends on your magnification, camera, etc. 
In addition it is nice if you also have an adjacent region on this piece of glass that is fully reflective where you can inspect the full beam profile (after focusing on it with the grid part). Here you can align for sheet thickness and uniformity.

> 
> Only to be sure, what I understood is that these Ruling Slides have already a reflective surface, so there is no
> need to glue a mirror to the ruling slides.

No. The whole piece needs to be fairly thin to fit between the lenses. So you would have to cut out a small piece anyway. Cutting is not so easy but the cut does not need to be precise.  Just smash it into pieces and pick the best piece ;-)

> 
> Thank you in advance for your comments.

Sure.

Best
Jan

Dr. Jan Huisken
MPI of Molecular Cell Biology and Genetics
Pfotenhauerstr. 108, 01307 Dresden, Germany

> 
> Kind regards,
> Edgar Escobar Nieto
> _________________________________________________________________________
> 
> Fraunhofer-Institut fr Produktionstechnologie IPT 
> Edgar Escobar Nieto 
> 
> 
> 
> 
> Steinbachstrae 17 
> 52074 Aachen 
> 
> edgar.escobar.nieto at ipt.fraunhofer.de 
> http://www.ipt.fraunhofer.de 
> _________________________________________________________________________ 
> 
> 
> 
> _______________________________________________
> OpenSPIM mailing list
> OpenSPIM at openspim.org
> http://openspim.org/mailman/listinfo/openspim

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130806/c9e091d8/attachment-0002.html>

From edgar.escobar.nieto at ipt.fraunhofer.de  Wed Aug  7 05:13:39 2013
From: edgar.escobar.nieto at ipt.fraunhofer.de (edgar.escobar.nieto at ipt.fraunhofer.de)
Date: Wed, 7 Aug 2013 12:13:39 +0200
Subject: [OpenSPIM] About the calibration of the lightsheet
In-Reply-To: <60102.174.24.0.131.1375806133.squirrel@webmail.mpi-cbg.de>
References: <60102.174.24.0.131.1375806133.squirrel@webmail.mpi-cbg.de>,
	<OF15A84A94.5A9526D4-ONC1257BBF.005330CF-C1257BBF.005330D2@ipt.rwth-aachen.de>
Message-ID: <OF93E58D03.96F977B9-ONC1257BC0.00382E93-C1257BC0.00382E9A@ipt.rwth-aachen.de>

 Hi dear Peter,

I have not yet bought the Ronchi Ruler Slide, but I was thinking of use a different
material in order to perform the alignment of the lighstsheet. I was thinking of use
some fluorescence samples that are mounted in a glass http://www.bw-optik.de/laufband/fluoreszenz.php
and since I have already some of this mounted glass samples available I will try to make a similar procedure as the stated in the OpenSPIM wiki.
If that method doesn't work I will have to buy the Ronchi Ruler Slide anyway.

The think is that since the material is also glass, would be the same to cut it or to cut the Ronchi Ruler Slide. That's
why I was asking how the Ruler Slide was cut. I will try to find a way to cut the glass, the problem
is that only a thin piece, let's say from 1 to 2 mm wide is needed. I know that would be so difficul to perform a cut
of this magnitude using a conventional glass cutter, but I'll give a try, if I am successful I will write a protocol to the OpenSPIM wiki.

I think until I had all the necessary material ready and mounted and if I have troubles with the alignment of the lightsheet,
that would be nice to have the Skype session.

Thanks for your kind answer and for offering me your support. Enjoy your holidays in the U.S.

Kind regards,
Edgar
 _________________________________________________________________________

Fraunhofer-Institut fr Produktionstechnologie IPT 
Edgar Escobar Nieto  
 

 
 
Steinbachstrae 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de 
http://www.ipt.fraunhofer.de
_________________________________________________________________________
 
 
 


-----"Peter Gabriel Pitrone" <pitrone at mpi-cbg.de> schrieb: -----
An: edgar.escobar.nieto at ipt.fraunhofer.de
Von: "Peter Gabriel Pitrone" <pitrone at mpi-cbg.de>
Datum: 06.08.2013 18:22
Kopie: openspim at openspim.org
Betreff: Re: [OpenSPIM] About the calibration of the lightsheet

Hello Edgar,

I'm sorry I didn't answer you till now, but I'm sometimes away from my
email as I am on vacation in the states.

To answer your questions, we broke the slide and found the best shard. It
is definitely not the most elegant way, but it works for us. If you want
to go about making it with more consistency with a glass or window
cutter, please write a protocol on our wiki on how to do it so others can
reproduce it.

If you need help with aligning the light sheet, we can make a skype
session tomorrow... just remember I have an 8 hour difference in
timezones. My Skype name is "petepitrone". send me an email as to the
time you would like to talk.

Pete

-- 
Peter Gabriel Pitrone - TechRMS
Microscopy/Imaging Specialist
Prof. Dr. Pavel Tomancak group
Max Planck Institute for
Molecular Biology and Genetics
Pfotenhauerstr. 108
01307 Dresden

"If a straight line fit is required, obtain only two data points." - Anon.


On Tue, August 6, 2013 5:08 pm, edgar.escobar.nieto at ipt.fraunhofer.de wrote:
<|> Hi dear All,
<|>
<|> I would like to know how you could cut the ruling slides. I don't
know if
<|> it is possible tu cut a thin part of glass using
<|> a conventional glass cutter.
<|>
<|> Kind regards,
<|> Edgar Escobar Nieto
<|> 
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut fr Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstrae 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> -----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 06.08.2013
<|> 17:01 -----
<|> An: openspim at openspim.org
<|> Von: Edgar Escobar Nieto/Fraunhofer IPT
<|> Datum: 05.08.2013 11:55
<|> Betreff: About the calibration of the lightsheet
<|>
<|>
<|> 
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut fr Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstrae 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> -----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 05.08.2013
<|> 11:54 -----
<|> An: openspim at openspim.org
<|> Von: Edgar Escobar Nieto/Fraunhofer IPT
<|> Datum: 31.07.2013 15:02
<|> Betreff: (Unbenannt)
<|>
<|> Hi dear all,
<|>
<|> I reached to the point where I need to calibrate the light-sheet. And I
<|> was wondering
<|> what is the optical density of the ND filter recommended to perform the
<|> calibration.
<|>
<|> I would like to know also what is the Frequency (lp/mm) recommended for
<|> the Opal Glass Ronchi Ruling Slides.
<|> I think that high a frequency would be better, but there must be a
limit.
<|>
<|> Only to be sure, what I understood is that these Ruling Slides have
<|> already a reflective surface, so there is no
<|> need to glue a mirror to the ruling slides.
<|>
<|> Thank you in advance for your comments.
<|>
<|> Kind regards,
<|> Edgar Escobar Nieto
<|> 
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut fr Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstrae 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> _______________________________________________
<|> OpenSPIM mailing list
<|> OpenSPIM at openspim.org
<|> http://openspim.org/mailman/listinfo/openspim
<|>


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130807/c1de6f06/attachment-0002.html>

From edgar.escobar.nieto at ipt.fraunhofer.de  Wed Aug  7 05:48:03 2013
From: edgar.escobar.nieto at ipt.fraunhofer.de (edgar.escobar.nieto at ipt.fraunhofer.de)
Date: Wed, 7 Aug 2013 12:48:03 +0200
Subject: [OpenSPIM] About the calibration of the lightsheet
In-Reply-To: <56A4F19F-C885-4041-BF78-BECDA36614E9@mpi-cbg.de>
References: <56A4F19F-C885-4041-BF78-BECDA36614E9@mpi-cbg.de>,
	<OF43BC6A67.1EC5993C-ONC1257BBE.00367CAB-C1257BBE.00367CAD@ipt.rwth-aachen.de>
Message-ID: <OF6662349C.05F700AE-ONC1257BC0.003B54EB-C1257BC0.003B54F3@ipt.rwth-aachen.de>

 Dear Jan,

Thanks for your comments. I have already available some ND filters, so I will set the laser
power to the minimum and make some tests. I will determine my FOV and then to buy
the Ruler Slides accordingly if necessary. I think I will try first the method that Jan Krieger
and Alexis Maizel used with a scratched little mirror, (seems to be cheaper and very good too).

Kind regards,
Edgar
 _________________________________________________________________________

Fraunhofer-Institut fr Produktionstechnologie IPT 
Edgar Escobar Nieto  
 

 
 
Steinbachstrae 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de 
http://www.ipt.fraunhofer.de
_________________________________________________________________________
 
 
 


-----Jan Huisken <huisken at mpi-cbg.de> schrieb: -----
An: edgar.escobar.nieto at ipt.fraunhofer.de
Von: Jan Huisken <huisken at mpi-cbg.de>
Datum: 06.08.2013 23:21
Kopie: openspim at openspim.org
Betreff: Re: [OpenSPIM] About the calibration of the lightsheet

Hi Edgar

-----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 05.08.2013 11:54 ----- 
 An: openspim at openspim.org
Von: Edgar Escobar Nieto/Fraunhofer IPT
 Datum: 31.07.2013 15:02
Betreff: (Unbenannt)

  Hi dear all,

I reached to the point where I need to calibrate the light-sheet. And I was wondering
 what is the optical density of the ND filter recommended to perform the calibration.

Since you are reflecting the laser right onto the camera you need to be sure that you can turn down the laser intensity far enough that you do not damage your camera. Therefore the ND depends on the lowest power you can set your laser to and the sensitivity of your camera. The ND filter can be introduced in the illumination or detection path. Some fluorescence emission filter may also do the job.
 
I would like to know also what is the Frequency (lp/mm) recommended for the Opal Glass Ronchi Ruling Slides.
 I think that high a frequency would be better, but there must be a limit.

The grid is used to (1) have some features on which you can focus and (2) to have some reflective parts the give you a profile of the beam. The distance does not matter very much. It is usually good if you have ca. 10 reflective lines across the entire FOV, which depends on your magnification, camera, etc.
In addition it is nice if you also have an adjacent region on this piece of glass that is fully reflective where you can inspect the full beam profile (after focusing on it with the grid part). Here you can align for sheet thickness and uniformity.
 
Only to be sure, what I understood is that these Ruling Slides have already a reflective surface, so there is no
 need to glue a mirror to the ruling slides.

No. The whole piece needs to be fairly thin to fit between the lenses. So you would have to cut out a small piece anyway. Cutting is not so easy but the cut does not need to be precise. Just smash it into pieces and pick the best piece ;-)

Thank you in advance for your comments.

Sure.

Best
Jan

Dr. Jan Huisken
MPI of Molecular CellBiology and Genetics
Pfotenhauerstr. 108, 01307 Dresden, Germany
 
Kind regards,
Edgar Escobar Nieto
  _________________________________________________________________________
 
Fraunhofer-Institut fr Produktionstechnologie IPT 
Edgar Escobar Nieto  
  

 
 
Steinbachstrae 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de  
 http://www.ipt.fraunhofer.de 
_________________________________________________________________________ 
 
  
 
 
_______________________________________________
OpenSPIM mailing list
OpenSPIM at openspim.org
http://openspim.org/mailman/listinfo/openspim

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130807/c039b851/attachment-0002.html>

From edgar.escobar.nieto at ipt.fraunhofer.de  Wed Aug  7 07:00:14 2013
From: edgar.escobar.nieto at ipt.fraunhofer.de (edgar.escobar.nieto at ipt.fraunhofer.de)
Date: Wed, 7 Aug 2013 14:00:14 +0200
Subject: [OpenSPIM] About the calibration of the lightsheet
In-Reply-To: <4F91222C659D5B4E8C89CE4D710BB08EF1440C5FDB@DKFZEX01.ad.dkfz-heidelberg.de>
References: <4F91222C659D5B4E8C89CE4D710BB08EF1440C5FDB@DKFZEX01.ad.dkfz-heidelberg.de>,
	<60102.174.24.0.131.1375806133.squirrel@webmail.mpi-cbg.de>,
	<OF15A84A94.5A9526D4-ONC1257BBF.005330CF-C1257BBF.005330D2@ipt.rwth-aachen.de>,
	<OF93E58D03.96F977B9-ONC1257BC0.00382E93-C1257BC0.00382E9A@ipt.rwth-aachen.de>
Message-ID: <OF80980D01.3665800B-ONC1257BC0.0041F098-C1257BC0.0041F09F@ipt.rwth-aachen.de>

 Hi Jan,

Thanks a lot for your comments. 
I will search in the lab if there is a little mirror, otherwise I should buy it.
The method that you used is also very clever. That will be the one that I will
use since looks like it is cheaper to buy a little simple mirror.

Kind regards,
Edgar

 _________________________________________________________________________

Fraunhofer-Institut fr Produktionstechnologie IPT 
Edgar Escobar Nieto  
 

 
 
Steinbachstrae 17 
52074 Aachen 

edgar.escobar.nieto at ipt.fraunhofer.de 
http://www.ipt.fraunhofer.de
_________________________________________________________________________
 
 
 


-----"Krieger, Jan" <j.krieger at Dkfz-Heidelberg.de> schrieb: -----
An: "edgar.escobar.nieto at ipt.fraunhofer.de" <edgar.escobar.nieto at ipt.fraunhofer.de>
Von: "Krieger, Jan" <j.krieger at Dkfz-Heidelberg.de>
Datum: 07.08.2013 12:21
Betreff: AW: [OpenSPIM] About the calibration of the lightsheet

Hi!

we are using a simple but small silver mirror (5x5mm, 1mm thick, bought from Melles-Griot) on a special mount, but you can glue it to anything that can be mounted inside the SPIM. First we focus the mirror, by illuminating it with a white LED or sinple lamp through the projection objective. The mirror is small enough to also fit into an openSPIM (I did that together with Alexis Maizel two weeks ago). Basically you can focus onto some dirk on the mirror surface, or a skratch that we make into it. Then you can switch to the laser and first bring it to the sharp line on the mirror and finally align it with the lenses of the illumination beam path. I think this is kind of the "classical" method to do that. 

The samples seem to be embedded in glass, right? Then you can focus them, but you won't have a focussed reflective surface then, that you can use to inspect the lightsheet.

Best,
JAN


Dipl.-Phys. Jan Krieger
German Cancer Research Center (dkfz)
Department B040 - Biophysics of Macromolecules (Prof. J. Langowski)
Im Neuenheimer Feld 580
69120 Heidelberg

fon: +49 / 6221 / 42-3395
fax: +49 / 6221 / 42-3391
e-mail: j.krieger at dkfz.de
www: http://www.dkfz.de/Macromol/
________________________________________
Von: openspim-bounces at openspim.org [openspim-bounces at openspim.org] im Auftrag von edgar.escobar.nieto at ipt.fraunhofer.de [edgar.escobar.nieto at ipt.fraunhofer.de]
Gesendet: Mittwoch, 7. August 2013 12:13
An: Peter Gabriel Pitrone
Cc: openspim at openspim.org
Betreff: Re: [OpenSPIM] About the calibration of the lightsheet

Hi dear Peter,

I have not yet bought the Ronchi Ruler Slide, but I was thinking of use a different
material in order to perform the alignment of the lighstsheet. I was thinking of use
some fluorescence samples that are mounted in a glass http://www.bw-optik.de/laufband/fluoreszenz.php
and since I have already some of this mounted glass samples available I will try to make a similar procedure as the stated in the OpenSPIM wiki.
If that method doesn't work I will have to buy the Ronchi Ruler Slide anyway.

The think is that since the material is also glass, would be the same to cut it or to cut the Ronchi Ruler Slide. That's
why I was asking how the Ruler Slide was cut. I will try to find a way to cut the glass, the problem
is that only a thin piece, let's say from 1 to 2 mm wide is needed. I know that would be so difficul to perform a cut
of this magnitude using a conventional glass cutter, but I'll give a try, if I am successful I will write a protocol to the OpenSPIM wiki.

I think until I had all the necessary material ready and mounted and if I have troubles with the alignment of the lightsheet,
that would be nice to have the Skype session.

Thanks for your kind answer and for offering me your support. Enjoy your holidays in the U.S.

Kind regards,
Edgar
_________________________________________________________________________

Fraunhofer-Institut fr Produktionstechnologie IPT
Edgar Escobar Nieto




Steinbachstrae 17
52074 Aachen

edgar.escobar.nieto at ipt.fraunhofer.de
http://www.ipt.fraunhofer.de<http://www.ipt.fraunhofer.de/>
_________________________________________________________________________







-----"Peter Gabriel Pitrone" <pitrone at mpi-cbg.de> schrieb: -----
An: edgar.escobar.nieto at ipt.fraunhofer.de
Von: "Peter Gabriel Pitrone" <pitrone at mpi-cbg.de>
Datum: 06.08.2013 18:22
Kopie: openspim at openspim.org
Betreff: Re: [OpenSPIM] About the calibration of the lightsheet

Hello Edgar,

I'm sorry I didn't answer you till now, but I'm sometimes away from my
email as I am on vacation in the states.

To answer your questions, we broke the slide and found the best shard. It
is definitely not the most elegant way, but it works for us. If you want
to go about making it with more consistency with a glass or window
cutter, please write a protocol on our wiki on how to do it so others can
reproduce it.

If you need help with aligning the light sheet, we can make a skype
session tomorrow... just remember I have an 8 hour difference in
timezones. My Skype name is "petepitrone". send me an email as to the
time you would like to talk.

Pete

--
Peter Gabriel Pitrone - TechRMS
Microscopy/Imaging Specialist
Prof. Dr. Pavel Tomancak group
Max Planck Institute for
Molecular Biology and Genetics
Pfotenhauerstr. 108
01307 Dresden

"If a straight line fit is required, obtain only two data points." - Anon.


On Tue, August 6, 2013 5:08 pm, edgar.escobar.nieto at ipt.fraunhofer.de wrote:
<|> Hi dear All,
<|>
<|> I would like to know how you could cut the ruling slides. I don't
know if
<|> it is possible tu cut a thin part of glass using
<|> a conventional glass cutter.
<|>
<|> Kind regards,
<|> Edgar Escobar Nieto
<|>
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut fr Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstrae 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> -----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 06.08.2013
<|> 17:01 -----
<|> An: openspim at openspim.org
<|> Von: Edgar Escobar Nieto/Fraunhofer IPT
<|> Datum: 05.08.2013 11:55
<|> Betreff: About the calibration of the lightsheet
<|>
<|>
<|>
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut fr Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstrae 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> -----Weitergeleitet von Edgar Escobar Nieto/Fraunhofer IPT am 05.08.2013
<|> 11:54 -----
<|> An: openspim at openspim.org
<|> Von: Edgar Escobar Nieto/Fraunhofer IPT
<|> Datum: 31.07.2013 15:02
<|> Betreff: (Unbenannt)
<|>
<|> Hi dear all,
<|>
<|> I reached to the point where I need to calibrate the light-sheet. And I
<|> was wondering
<|> what is the optical density of the ND filter recommended to perform the
<|> calibration.
<|>
<|> I would like to know also what is the Frequency (lp/mm) recommended for
<|> the Opal Glass Ronchi Ruling Slides.
<|> I think that high a frequency would be better, but there must be a
limit.
<|>
<|> Only to be sure, what I understood is that these Ruling Slides have
<|> already a reflective surface, so there is no
<|> need to glue a mirror to the ruling slides.
<|>
<|> Thank you in advance for your comments.
<|>
<|> Kind regards,
<|> Edgar Escobar Nieto
<|>
_________________________________________________________________________
<|>
<|> Fraunhofer-Institut fr Produktionstechnologie IPT
<|> Edgar Escobar Nieto
<|>
<|>
<|>
<|>
<|> Steinbachstrae 17
<|> 52074 Aachen
<|>
<|> edgar.escobar.nieto at ipt.fraunhofer.de
<|> http://www.ipt.fraunhofer.de
<|>
_________________________________________________________________________
<|>
<|>
<|>
<|>
<|>
<|> _______________________________________________
<|> OpenSPIM mailing list
<|> OpenSPIM at openspim.org
<|> http://openspim.org/mailman/listinfo/openspim
<|>


-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130807/5a377aa4/attachment-0002.html>

From Alexis.Maizel at cos.uni-heidelberg.de  Tue Aug 13 13:02:55 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Tue, 13 Aug 2013 20:02:55 +0200
Subject: [OpenSPIM] Software hangs during stack capture
In-Reply-To: <24140450-E3E7-4F2F-8163-7ACFCDED68D2@cos.uni-heidelberg.de>
References: <3FEEC96F-6E46-4B4F-8A50-141F6E9E169E@cos.uni-heidelberg.de>
	<alpine.DEB.1.00.1307301757130.24252@s15462909.onlinehome-server.info>
	<24140450-E3E7-4F2F-8163-7ACFCDED68D2@cos.uni-heidelberg.de>
Message-ID: <942DA4AF-230A-43B7-8196-D8B269D08B06@cos.uni-heidelberg.de>

Answering my own post
well after a few more days and more recordings, it seems that the problem did not appear again. No more hangs during acquisition.
I keep an eye open.

Alexis
 
On 31 Jul 2013, at 09:38, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:

> Hi Johannes & Luke,
>>> I noticed that upon acquisition of a stack  written to disk, OpenSPIM
>>> occasionally pauses in the middle of the acquisition for  some seconds
>>> to several minutes. I have only observed this when the stack is written
>>> to disk, not when the same stack is left in RAM and opened immediately.
>>> I tried to turn ON/OFF the asynchronous writing option but that did not
>>> change anything. 
>> 
>> Are you running an anti-virus software that could be at fault?
> 
> Not that I am aware of, but there might be a lot of crap (updates, power manager, blabla) that could interfere. I will see at cleaning this mess a bit.
> However, I did not notice anything happening @ the GUI level and correlating with these hangs.
> 
>> I've been getting some weird hangings as each slice is written out (affecting the entire operating system, not just the SPIM software), but typically they were lasting no more than a half-second, and never on the order of minutes. They're recent, and I've been doing some timing tests to try and determine the cause to greater accuracy than 'when writing'.
>> 
>> 
>> I take it the disk is local to the computer?
> 
> Yep, internal HD. 
> 
>>> Also I have noticed that I can not increase the memory allocated to Fiji
>>> beyond 1024M as it causes the HamamatsuHam driver to not load properly. 
>> 
>> Yes, that is unfortunately not something we can fix until we switch to a
>> 64-bit version (which should be doable soon, given the hardware).
> 
> Let's wait then.
> 
> With my best regards,
> 
> Alexis
> 
>> 
>> Ciao,
>> Johannes
> 
> 
> _______________________________________________
> OpenSPIM mailing list
> OpenSPIM at openspim.org
> http://openspim.org/mailman/listinfo/openspim

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130813/dd5a56b7/attachment.sig>

From Alexis.Maizel at cos.uni-heidelberg.de  Tue Aug 13 13:14:47 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Tue, 13 Aug 2013 20:14:47 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
Message-ID: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>

Hi,

I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.

 I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
 
You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip

With my best regards,

Alexis

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130813/a6fdd0c2/attachment.sig>

From tomancak at mpi-cbg.de  Tue Aug 13 17:30:24 2013
From: tomancak at mpi-cbg.de (Pavel Tomancak)
Date: Wed, 14 Aug 2013 00:30:24 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
Message-ID: <B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>

Hi Alexis,

That is very strange. I have never seen that. We made some acquisitions today and nothing like that was going on. Its obviously a serious issue. Does it happen only when you have the asynchronous writing enabled? I assume you are running on Windows 7.

All the best

PAvel

-----------------------------------------------------------------------------------
Pavel Tomancak, Ph.D.

Group Leader
Max Planck Institute of Molecular Cell Biology and Genetics
Pfotenhauerstr. 108
D-01307 Dresden				Tel.: +49 351 210 2670
Germany						Fax: +49 351 210 2020
tomancak at mpi-cbg.de
http://www.mpi-cbg.de
-----------------------------------------------------------------------------------



On Aug 13, 2013, at 8:14 PM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:

> Hi,
> 
> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
> 
> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
> 
> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
> 
> With my best regards,
> 
> Alexis
> 
> _______________________________________________
> OpenSPIM mailing list
> OpenSPIM at openspim.org
> http://openspim.org/mailman/listinfo/openspim

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130814/5ec66939/attachment-0002.html>

From huisken at mpi-cbg.de  Wed Aug 14 03:11:31 2013
From: huisken at mpi-cbg.de (Jan Huisken)
Date: Wed, 14 Aug 2013 10:11:31 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
Message-ID: <8C2EDD65-84C3-4E2A-9996-E1265BFC498F@mpi-cbg.de>

Hi Alexis,

this is clearly a camera issue. Sometimes this goes along with missing or partial frames. We have seen it happening when something goes wrong with the spooling, e.g. when the hard drive is too slow or too full or too many files in one folder.

BTW the images are not in focus and you seem to collect quite a bit of laser speckles. Clearly a sign for a bad emission filter or that your laser needs a clean-up filter.

Best
Jan

Dr. Jan Huisken
MPI of Molecular Cell Biology and Genetics
Pfotenhauerstr. 108, 01307 Dresden, Germany

On Aug 13, 2013, at 8:14 PM, Alexis Maizel wrote:

> Hi,
> 
> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
> 
> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
> 
> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
> 
> With my best regards,
> 
> Alexis
> 
> _______________________________________________
> OpenSPIM mailing list
> OpenSPIM at openspim.org
> http://openspim.org/mailman/listinfo/openspim

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130814/2c94cc6e/attachment-0002.html>

From Alexis.Maizel at cos.uni-heidelberg.de  Wed Aug 14 03:11:01 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Wed, 14 Aug 2013 10:11:01 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
Message-ID: <698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>

Hi Pavel,

I did some more tests this morning. 
The asynchronous writing is indeed the culprit. You can get two stacks here: http://dl.dropbox.com/u/484859/async_ON_vs_OFF.zip

With async OFF: the images are written to the disk as soon as they are acquired; it is slow but the planes are in the right order.

With async ON: the stack is acquired and written 'in bursts' to the disk. Sometimes there is a gap of several seconds between two bursts of disk writing and I have the feeling this corresponds to points when the planes are written in the wrong order. I have also noticed that irrespective of the time delay between two time lapse acquisition, the last image(s) of time point N are written to the disk instant before the time point N+1 starts to be acquired. Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk,  then there is a warning windows displayed (which is empty) after some seconds, the window disappear and the recording has been aborted. 
So I do not know, what's wrong but something does not seem right in this 'asynchronous writing' option, at least on our config:
Windows 7 pro
1024Mb of RAM allocated to Fiji (we can not allocate more, otherwise the Orca won't operate)
JRE 1.6.0
everything 32bits version

Would be great to get that fixed :-)

With my best regards,

Alexis


On 14 Aug 2013, at 00:30, Pavel Tomancak <tomancak at mpi-cbg.de> wrote:

> Hi Alexis,
> 
> That is very strange. I have never seen that. We made some acquisitions today and nothing like that was going on. Its obviously a serious issue. Does it happen only when you have the asynchronous writing enabled? I assume you are running on Windows 7.
> 
> All the best
> 
> PAvel
> 
> -----------------------------------------------------------------------------------
> Pavel Tomancak, Ph.D.
> 
> Group Leader
> Max Planck Institute of Molecular Cell Biology and Genetics
> Pfotenhauerstr. 108
> D-01307 Dresden				Tel.: +49 351 210 2670
> Germany						Fax: +49 351 210 2020
> tomancak at mpi-cbg.de
> http://www.mpi-cbg.de
> -----------------------------------------------------------------------------------
> 
> 
> 
> On Aug 13, 2013, at 8:14 PM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:
> 
>> Hi,
>> 
>> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
>> 
>> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
>> 
>> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
>> 
>> With my best regards,
>> 
>> Alexis
>> 
>> _______________________________________________
>> OpenSPIM mailing list
>> OpenSPIM at openspim.org
>> http://openspim.org/mailman/listinfo/openspim
> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130814/b83aa6d8/attachment.sig>

From Alexis.Maizel at cos.uni-heidelberg.de  Wed Aug 14 03:20:58 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Wed, 14 Aug 2013 10:20:58 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <8C2EDD65-84C3-4E2A-9996-E1265BFC498F@mpi-cbg.de>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<8C2EDD65-84C3-4E2A-9996-E1265BFC498F@mpi-cbg.de>
Message-ID: <12EEF5BD-10F2-4328-9929-7C8CDAAFEF9E@cos.uni-heidelberg.de>

Hi Jan,

> BTW the images are not in focus and you seem to collect quite a bit of laser speckles. Clearly a sign for a bad emission filter or that your laser needs a clean-up filter.

Yes, the laser does need a clean up filter quite a bit of green light in it, especially at low power; it is ordered. The emission filter (http://www.semrock.com/FilterDetails.aspx?id=FF03-525/50-25) seems right, no? 

As for focus, this is the best we could get so far :-/
Our LS is ~10m thick and nicely shaped. The excitation lens is a Nikon CFI FLUOR 10x/0.30.
Do you have suggestions to improve the focus?

With my best regards,

Alexis


On 14 Aug 2013, at 10:11, Jan Huisken <huisken at mpi-cbg.de> wrote:

> Hi Alexis,
> 
> this is clearly a camera issue. Sometimes this goes along with missing or partial frames. We have seen it happening when something goes wrong with the spooling, e.g. when the hard drive is too slow or too full or too many files in one folder.
> 
> 
> Best
> Jan
> 
> Dr. Jan Huisken
> MPI of Molecular Cell Biology and Genetics
> Pfotenhauerstr. 108, 01307 Dresden, Germany
> 
> On Aug 13, 2013, at 8:14 PM, Alexis Maizel wrote:
> 
>> Hi,
>> 
>> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
>> 
>> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
>> 
>> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
>> 
>> With my best regards,
>> 
>> Alexis
>> 
>> _______________________________________________
>> OpenSPIM mailing list
>> OpenSPIM at openspim.org
>> http://openspim.org/mailman/listinfo/openspim
> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130814/02b2c788/attachment.sig>

From weber at mpi-cbg.de  Wed Aug 14 07:53:09 2013
From: weber at mpi-cbg.de (Michael Weber)
Date: Wed, 14 Aug 2013 14:53:09 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <12EEF5BD-10F2-4328-9929-7C8CDAAFEF9E@cos.uni-heidelberg.de>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<8C2EDD65-84C3-4E2A-9996-E1265BFC498F@mpi-cbg.de>
	<12EEF5BD-10F2-4328-9929-7C8CDAAFEF9E@cos.uni-heidelberg.de>
Message-ID: <5C2B0E2F-DEEA-4B52-AF34-93AAF49B00E5@mpi-cbg.de>

Hi Alexis,

the light sheet might simply be too thick, I think that's still an issue with OpenSPIM v1. If you have laser power left you could try and extend the beam further in the first place.

Regarding the stack issue - you mentioned that it takes about 3 min to write 400 MB to disk, that might be an issue on the computer hardware side. A simple hard drive should give you around 100 MB/s, with smaller data packages it might be less but 3 min sounds way to slow. Can you reduce the data rate and see if that helps? For example by reducing the frame rate or using binning while keeping the frame rate constant. Or if you can, try an SSD drive or a RAID system for data acquisition. Streaming data can also be very sensitive to background activities, such as virus scanners, cloud syncing, update scanners.

Best regards,
Michael


On Aug 14, 2013, at 10:20 AM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:

> Hi Jan,
> 
>> BTW the images are not in focus and you seem to collect quite a bit of laser speckles. Clearly a sign for a bad emission filter or that your laser needs a clean-up filter.
> 
> Yes, the laser does need a clean up filter quite a bit of green light in it, especially at low power; it is ordered. The emission filter (http://www.semrock.com/FilterDetails.aspx?id=FF03-525/50-25) seems right, no? 
> 
> As for focus, this is the best we could get so far :-/
> Our LS is ~10m thick and nicely shaped. The excitation lens is a Nikon CFI FLUOR 10x/0.30.
> Do you have suggestions to improve the focus?
> 
> With my best regards,
> 
> Alexis
> 
> 
> On 14 Aug 2013, at 10:11, Jan Huisken <huisken at mpi-cbg.de> wrote:
> 
>> Hi Alexis,
>> 
>> this is clearly a camera issue. Sometimes this goes along with missing or partial frames. We have seen it happening when something goes wrong with the spooling, e.g. when the hard drive is too slow or too full or too many files in one folder.
>> 
>> 
>> Best
>> Jan
>> 
>> Dr. Jan Huisken
>> MPI of Molecular Cell Biology and Genetics
>> Pfotenhauerstr. 108, 01307 Dresden, Germany
>> 
>> On Aug 13, 2013, at 8:14 PM, Alexis Maizel wrote:
>> 
>>> Hi,
>>> 
>>> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
>>> 
>>> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
>>> 
>>> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
>>> 
>>> With my best regards,
>>> 
>>> Alexis
>>> 
>>> _______________________________________________
>>> OpenSPIM mailing list
>>> OpenSPIM at openspim.org
>>> http://openspim.org/mailman/listinfo/openspim
>> 
> 
> _______________________________________________
> OpenSPIM mailing list
> OpenSPIM at openspim.org
> http://openspim.org/mailman/listinfo/openspim

_____________

Michael Weber
PhD Student, Huisken lab
Max Planck Institute of Molecular Cell Biology and Genetics
Pfotenhauerstrasse 108, 01307 Dresden
Tel. 0049 351/2102837

http://www.mpi-cbg.de/huisken

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130814/3106b327/attachment-0002.html>

From Alexis.Maizel at cos.uni-heidelberg.de  Wed Aug 14 09:54:59 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Wed, 14 Aug 2013 16:54:59 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <5C2B0E2F-DEEA-4B52-AF34-93AAF49B00E5@mpi-cbg.de>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<8C2EDD65-84C3-4E2A-9996-E1265BFC498F@mpi-cbg.de>
	<12EEF5BD-10F2-4328-9929-7C8CDAAFEF9E@cos.uni-heidelberg.de>
	<5C2B0E2F-DEEA-4B52-AF34-93AAF49B00E5@mpi-cbg.de>
Message-ID: <F58F755A-F260-4B0E-A467-B7D4E241A459@cos.uni-heidelberg.de>

hi,
> Regarding the stack issue - you mentioned that it takes about 3 min to write 400 MB to disk, that might be an issue on the computer hardware side. A simple hard drive should give you around 100 MB/s, with smaller data packages it might be less but 3 min sounds way to slow. Can you reduce the data rate and see if that helps?For example by reducing the frame rate or using binning while keeping the frame rate constant. Or if you can, try an SSD drive or a RAID system for data acquisition. Streaming data can also be very sensitive to background activities, such as virus scanners, cloud syncing, update scanners.

I do not think that the problem comes from the hard drive:
1) reducing the file size by a factor of 2 or 4 (binning) did not make the writing to disk faster. With the asynchronous writing ON, the planes are always written to disk in a peppered manner between two consecutive time lapse, with the last image(s)  written right before the next time point.

2) When  asynchronous writing os OFF (i-e writing to disk is occurring immediately, which should be a good readout of the hard drive speed) the stacks are written in about 20sec (for ~200Mb)

With my best regards,

Alexis


> Best regards,
> Michael
> 
> 
> On Aug 14, 2013, at 10:20 AM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:
> 
>> Hi Jan,
>> 
>>> BTW the images are not in focus and you seem to collect quite a bit of laser speckles. Clearly a sign for a bad emission filter or that your laser needs a clean-up filter.
>> 
>> Yes, the laser does need a clean up filter quite a bit of green light in it, especially at low power; it is ordered. The emission filter (http://www.semrock.com/FilterDetails.aspx?id=FF03-525/50-25) seems right, no? 
>> 
>> As for focus, this is the best we could get so far :-/
>> Our LS is ~10m thick and nicely shaped. The excitation lens is a Nikon CFI FLUOR 10x/0.30.
>> Do you have suggestions to improve the focus?
>> 
>> With my best regards,
>> 
>> Alexis
>> 
>> 
>> On 14 Aug 2013, at 10:11, Jan Huisken <huisken at mpi-cbg.de> wrote:
>> 
>>> Hi Alexis,
>>> 
>>> this is clearly a camera issue. Sometimes this goes along with missing or partial frames. We have seen it happening when something goes wrong with the spooling, e.g. when the hard drive is too slow or too full or too many files in one folder.
>>> 
>>> 
>>> Best
>>> Jan
>>> 
>>> Dr. Jan Huisken
>>> MPI of Molecular Cell Biology and Genetics
>>> Pfotenhauerstr. 108, 01307 Dresden, Germany
>>> 
>>> On Aug 13, 2013, at 8:14 PM, Alexis Maizel wrote:
>>> 
>>>> Hi,
>>>> 
>>>> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
>>>> 
>>>> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
>>>> 
>>>> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
>>>> 
>>>> With my best regards,
>>>> 
>>>> Alexis
>>>> 
>>>> _______________________________________________
>>>> OpenSPIM mailing list
>>>> OpenSPIM at openspim.org
>>>> http://openspim.org/mailman/listinfo/openspim
>>> 
>> 
>> _______________________________________________
>> OpenSPIM mailing list
>> OpenSPIM at openspim.org
>> http://openspim.org/mailman/listinfo/openspim
> 
> _____________
> 
> Michael Weber
> PhD Student, Huisken lab
> Max Planck Institute of Molecular Cell Biology and Genetics
> Pfotenhauerstrasse 108, 01307 Dresden
> Tel. 0049 351/2102837
> 
> http://www.mpi-cbg.de/huisken
> 
> _______________________________________________
> OpenSPIM mailing list
> OpenSPIM at openspim.org
> http://openspim.org/mailman/listinfo/openspim

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130814/614a38d3/attachment.sig>

From tomancak at mpi-cbg.de  Sun Aug 18 14:42:07 2013
From: tomancak at mpi-cbg.de (Pavel Tomancak)
Date: Sun, 18 Aug 2013 21:42:07 +0200
Subject: [OpenSPIM] OpenSPIM
In-Reply-To: <2CF004C2D6E4BD4A871084E05627BCAD2916A85C@serv-mailbox3.igbmc.u-strasbg.fr>
References: <2CF004C2D6E4BD4A871084E05627BCAD2916A85C@serv-mailbox3.igbmc.u-strasbg.fr>
Message-ID: <D1F5E274-CFC8-469D-884E-2942CAED8C2B@mpi-cbg.de>

Dear Yusuke,

We are working on organising a course about light sheet microscopy. Its an EMBO practical course application, if it works out, you will hear about it. That would be the easiest way to start.

Otherwise I recommend to search for resources on the internet. Open hardware is an increasingly active movement. Lots of things are discussed on twitter where you can easily connect with people with the same interests. The keywords to search for are things like 3d printing, arduino, raspberry pi etc. There is a lot out there.

There is also a primordial idea to organise a course about open hardware in general but its too soon to talk about it.

All the best

PAvel

-----------------------------------------------------------------------------------
Pavel Tomancak, Ph.D.

Group Leader
Max Planck Institute of Molecular Cell Biology and Genetics
Pfotenhauerstr. 108
D-01307 Dresden				Tel.: +49 351 210 2670
Germany						Fax: +49 351 210 2020
tomancak at mpi-cbg.de
http://www.mpi-cbg.de
-----------------------------------------------------------------------------------



On Aug 14, 2013, at 4:52 PM, Yusuke MIYANARI <miyanari at igbmc.fr> wrote:

> Dear Pavel and people in OpenSPIM project
> 
> I'm Yusuke, a posdoc in IGBMC, Strasbourg. First of all, I appreciate you and people involved in OpenSpim project, since it makes me so motivated to build own SPIM, whereas I am just a user of microscopy in the meantime. I really enjoyed the paper published in Nature method and the website. This is really helpful especially for biologist like me to learn how to assemble microscopy. I would like to learn how to design microscopy, including assemble of optical path, choice of each components (laser, beam expander, relay lens, etc), and practical skill or difficulties. I'm wondering whether you could tell me some text books, manuscripts, websites, or workshops, which are useful to understand how to design and assemble microscopy. 
> 
> Thank you for your help,
> 
> Yusuke
> 
> 
> Yusuke Miyanari
> INSTITUT DE GENETIQUE ET DE BIOLOGIE MOLECULAIRE ET CELLULAIRE (IGBMC)
> Parc d'Innovation
> 1, rue Laurent Fries,
> 67404 ILLKIRCH Cedex,
> C.U. de STRASBOURG, FRANCE
> Tel: 00 33 (0)3 88 65 33 58 (lab) 
> Fax: 00 33 (0)3 88 65 32 01
> Maria-Elena Torres-Padilla lab
> 
> 

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130818/f9da2e99/attachment-0002.html>

From miyanari at igbmc.fr  Mon Aug 19 09:25:11 2013
From: miyanari at igbmc.fr (Yusuke MIYANARI)
Date: Mon, 19 Aug 2013 14:25:11 +0000
Subject: [OpenSPIM] OpenSPIM
In-Reply-To: <D1F5E274-CFC8-469D-884E-2942CAED8C2B@mpi-cbg.de>
References: <2CF004C2D6E4BD4A871084E05627BCAD2916A85C@serv-mailbox3.igbmc.u-strasbg.fr>,
	<D1F5E274-CFC8-469D-884E-2942CAED8C2B@mpi-cbg.de>
Message-ID: <2CF004C2D6E4BD4A871084E05627BCAD29185725@serv-mailbox3.igbmc.u-strasbg.fr>

Hello Pavel,

Thank you for your reply. If you could organize the EMBO practical course, it sounds great.

All the best,

Yusuke




Yusuke Miyanari
INSTITUT DE GENETIQUE ET DE BIOLOGIE MOLECULAIRE ET CELLULAIRE (IGBMC)
Parc d'Innovation
1, rue Laurent Fries,
67404 ILLKIRCH Cedex,
C.U. de STRASBOURG, FRANCE
Tel: 00 33 (0)3 88 65 33 58 (lab)
Fax: 00 33 (0)3 88 65 32 01
Maria-Elena Torres-Padilla lab

________________________________
From: Pavel Tomancak [tomancak at mpi-cbg.de]
Sent: 18 August 2013 21:42
To: Yusuke MIYANARI
Cc: OpenSPIM at openspim.org
Subject: Re: OpenSPIM

Dear Yusuke,

We are working on organising a course about light sheet microscopy. Its an EMBO practical course application, if it works out, you will hear about it. That would be the easiest way to start.

Otherwise I recommend to search for resources on the internet. Open hardware is an increasingly active movement. Lots of things are discussed on twitter where you can easily connect with people with the same interests. The keywords to search for are things like 3d printing, arduino, raspberry pi etc. There is a lot out there.

There is also a primordial idea to organise a course about open hardware in general but its too soon to talk about it.

All the best

PAvel

-----------------------------------------------------------------------------------
Pavel Tomancak, Ph.D.

Group Leader
Max Planck Institute of Molecular Cell Biology and Genetics
Pfotenhauerstr. 108
D-01307 Dresden Tel.: +49 351 210 2670
Germany Fax: +49 351 210 2020
tomancak at mpi-cbg.de<mailto:tomancak at mpi-cbg.de>
http://www.mpi-cbg.de<http://www.mpi-cbg.de/>
-----------------------------------------------------------------------------------



On Aug 14, 2013, at 4:52 PM, Yusuke MIYANARI <miyanari at igbmc.fr<mailto:miyanari at igbmc.fr>> wrote:

Dear Pavel and people in OpenSPIM project

I'm Yusuke, a posdoc in IGBMC, Strasbourg. First of all, I appreciate you and people involved in OpenSpim project, since it makes me so motivated to build own SPIM, whereas I am just a user of microscopy in the meantime. I really enjoyed the paper published in Nature method and the website. This is really helpful especially for biologist like me to learn how to assemble microscopy. I would like to learn how to design microscopy, including assemble of optical path, choice of each components (laser, beam expander, relay lens, etc), and practical skill or difficulties. I'm wondering whether you could tell me some text books, manuscripts, websites, or workshops, which are useful to understand how to design and assemble microscopy.

Thank you for your help,

Yusuke


Yusuke Miyanari
INSTITUT DE GENETIQUE ET DE BIOLOGIE MOLECULAIRE ET CELLULAIRE (IGBMC)
Parc d'Innovation
1, rue Laurent Fries,
67404 ILLKIRCH Cedex,
C.U. de STRASBOURG, FRANCE
Tel: 00 33 (0)3 88 65 33 58 (lab)
Fax: 00 33 (0)3 88 65 32 01
Maria-Elena Torres-Padilla lab



-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130819/13e2eefd/attachment-0002.html>

From stuyvenberg at wisc.edu  Mon Aug 19 12:22:08 2013
From: stuyvenberg at wisc.edu (Luke Stuyvenberg)
Date: Mon, 19 Aug 2013 12:22:08 -0500
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <75f08c45483b1.52125421@wiscmail.wisc.edu>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<76209bad4c9b2.52122355@wiscmail.wisc.edu>
	<7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
Message-ID: <7530b15b4bd9a.52120df0@wiscmail.wisc.edu>

Hi Alexis,

Sorry for the delay in my reply; I was moving into a new apartment last week.

On 08/14/13, Alexis Maizel wrote:
> Hi Pavel,
>
> I did some more tests this morning.
> The asynchronous writing is indeed the culprit. You can get two stacks here: http://dl.dropbox.com/u/484859/async_ON_vs_OFF.zip
Did you re-save this data, or are these stacks fresh from the OpenSPIM plugin? I ask because the OME-TIFF metadata have been clobbered in both stacks (you can check using the Bio-Formats importer); if this is fresh output, the plugin may have a serious error in its metadata generation. Alternatively, Bio-Formats might be out-of-date or acting up. At any rate, the image slices obviously shouldn't be written out of order, but even if they were to be written in the wrong order, were the OME metadata intact, Bio-Formats should be able to display the slices in the correct order (because now it has a table relating the slice's index in the file to its physical Z position).


On 08/14/13, Alexis Maizel wrote:
> With async ON: the stack is acquired and written 'in bursts' to the disk. Sometimes there is a gap of several seconds between two bursts of disk writing and I have the feeling this corresponds to points when the planes are written in the wrong order. I have also noticed that irrespective of the time delay between two time lapse acquisition, the last image(s) of time point N are written to the disk instant before the time point N+1 starts to be acquired.
The data isn't truly meant to be written in bursts -- ideally it would just chug along in the background... I'll try tweaking the async a little more; probably, the code is doing something incorrectly leading to this. As for the timepoint images, this too is very unusual -- I specifically have the writer finish writing any pending images before finalizing a stack. I'll look into this as well.


On 08/14/13, Alexis Maizel wrote:
>Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty) after some seconds, the window disappear and the recording has been aborted.
I'm aware of this; I've never been able to determine what that message box is trying to display. No warnings should be appearing during an abort, but this one mysteriously shows up. It's a low priority for me, however; it doesn't seem to affect the abort or change the data that was written (well, any more than aborting mid-acquisition already does).


On 08/14/13, Alexis Maizel wrote:
> So I do not know, what's wrong but something does not seem right in this 'asynchronous writing' option, at least on our config:
> Windows 7 pro
> 1024Mb of RAM allocated to Fiji (we can not allocate more, otherwise the Orca won't operate)
> JRE 1.6.0
> everything 32bits version
Despite the many imaging sessions Julie Last and I have been performing over here, I haven't been able to reproduce this bug. I also tried imaging while stress-testing the CPU and RAM with Prime95; although this made the queue build up a few more images than normal use, I didn't notice any actual problems while writing, and the OME metadata came out fine. I've also been doing some CPU time sampling using jvisualvm, though all I discovered is that snapImage is *very* slow on our machine for some reason. I'll see what more I can find out soon.

Is everything completely up-to-date, including Bio-Formats? (Note that I haven't yet tried the Bio-Formats daily builds -- whether they will fix or break the plugin has yet to be seen.) If so, the next time you're running an acquisition, could you try monitoring Fiji's memory usage (Plugins > Utilities > Memory Monitor) as the sequence progresses? I suspect that the bursts you are observing happen because the queue is filling up, forcing the output handler to synchronously write out one or more slices; you should see the memory use climb in a stair-step pattern until one of these bursts, when it should plummet back to nearly nothing.


Thanks,
Luke

On 08/14/13, Alexis Maizel wrote:
> Hi Pavel,
> 
> I did some more tests this morning. 
> The asynchronous writing is indeed the culprit. You can get two stacks here: http://dl.dropbox.com/u/484859/async_ON_vs_OFF.zip
> 
> With async OFF: the images are written to the disk as soon as they are acquired; it is slow but the planes are in the right order.
> 
> With async ON: the stack is acquired and written 'in bursts' to the disk. Sometimes there is a gap of several seconds between two bursts of disk writing and I have the feeling this corresponds to points when the planes are written in the wrong order. I have also noticed that irrespective of the time delay between two time lapse acquisition, the last image(s) of time point N are written to the disk instant before the time point N+1 starts to be acquired. Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty) after some seconds, the window disappear and the recording has been aborted. 
> So I do not know, what's wrong but something does not seem right in this 'asynchronous writing' option, at least on our config:
> Windows 7 pro
> 1024Mb of RAM allocated to Fiji (we can not allocate more, otherwise the Orca won't operate)
> JRE 1.6.0
> everything 32bits version
> 
> Would be great to get that fixed :-)
> 
> With my best regards,
> 
> Alexis
> 
> 
> On 14 Aug 2013, at 00:30, Pavel Tomancak <tomancak at mpi-cbg.de> wrote:
> 
> > Hi Alexis,
> > 
> > That is very strange. I have never seen that. We made some acquisitions today and nothing like that was going on. Its obviously a serious issue. Does it happen only when you have the asynchronous writing enabled? I assume you are running on Windows 7.
> > 
> > All the best
> > 
> > PAvel
> > 
> > -----------------------------------------------------------------------------------
> > Pavel Tomancak, Ph.D.
> > 
> > Group Leader
> > Max Planck Institute of Molecular Cell Biology and Genetics
> > Pfotenhauerstr. 108
> > D-01307 Dresden Tel.: +49 351 210 2670
> > Germany Fax: +49 351 210 2020
> > tomancak at mpi-cbg.de
> > http://www.mpi-cbg.de
> > -----------------------------------------------------------------------------------
> > 
> > 
> > 
> > On Aug 13, 2013, at 8:14 PM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:
> > 
> >> Hi,
> >> 
> >> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
> >> 
> >> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
> >> 
> >> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
> >> 
> >> With my best regards,
> >> 
> >> Alexis
> >> 
> >> _______________________________________________
> >> OpenSPIM mailing list
> >> OpenSPIM at openspim.org
> >> http://openspim.org/mailman/listinfo/openspim
> >



From Alexis.Maizel at cos.uni-heidelberg.de  Tue Aug 20 08:54:51 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Tue, 20 Aug 2013 15:54:51 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<76209bad4c9b2.52122355@wiscmai!
	l.wisc.edu> <7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
Message-ID: <F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>

Hi Luke,

> Did you re-save this data, or are these stacks fresh from the OpenSPIM plugin?

I scaled them down to 8bits and saved them on my Mac. You can get an original "shuffled" stack here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiff

> I ask because the OME-TIFF metadata have been clobbered in both stacks (you can check using the Bio-Formats importer); if this is fresh output, the plugin may have a serious error in its metadata generation. Alternatively, Bio-Formats might be out-of-date or acting up. At any rate, the image slices obviously shouldn't be written out of order, but even if they were to be written in the wrong order, were the OME metadata intact, Bio-Formats should be able to display the slices in the correct order (because now it has a table relating the slice's index in the file to its physical Z position).

I forced Fiji to open the fresh-from-openSPIM stacks on my mac using the Bio-format importer (updated) and the slices are still in the wrong order.

> The data isn't truly meant to be written in bursts -- ideally it would just chug along in the background... I'll try tweaking the async a little more; probably, the code is doing something incorrectly leading to this. As for the timepoint images, this too is very unusual -- I specifically have the writer finish writing any pending images before finalizing a stack. I'll look into this as well.

Definitively the writing to disk finishes way after the stack has finished to be acquired. 

>>  Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty) after some seconds, the window disappear and the recording has been aborted.
> I'm aware of this; I've never been able to determine what that message box is trying to display. No warnings should be appearing during an abort, but this one mysteriously shows up. It's a low priority for me, however; it doesn't seem to affect the abort or change the data that was written (well, any more than aborting mid-acquisition already does).

I concur that it is not critical, I just wanted to point it out. Glad to read that at least some of what I am experiencing can be reproduced :-)

> Is everything completely up-to-date, including Bio-Formats? (Note that I haven't yet tried the Bio-Formats daily builds -- whether they will fix or break the plugin has yet to be seen.)

I will update everything and perform more tests.

> If so, the next time you're running an acquisition, could you try monitoring Fiji's memory usage (Plugins > Utilities > Memory Monitor) as the sequence progresses? I suspect that the bursts you are observing happen because the queue is filling up, forcing the output handler to synchronously write out one or more slices; you should see the memory use climb in a stair-step pattern until one of these bursts, when it should plummet back to nearly nothing.

I'll do that and get back to you.

Thanks for your help,

Alexis



> 
> 
> Thanks,
> Luke
> 
> On 08/14/13, Alexis Maizel wrote:
>> Hi Pavel,
>> 
>> I did some more tests this morning. 
>> The asynchronous writing is indeed the culprit. You can get two stacks here: http://dl.dropbox.com/u/484859/async_ON_vs_OFF.zip
>> 
>> With async OFF: the images are written to the disk as soon as they are acquired; it is slow but the planes are in the right order.
>> 
>> With async ON: the stack is acquired and written 'in bursts' to the disk. Sometimes there is a gap of several seconds between two bursts of disk writing and I have the feeling this corresponds to points when the planes are written in the wrong order. I have also noticed that irrespective of the time delay between two time lapse acquisition, the last image(s) of time point N are written to the disk instant before the time point N+1 starts to be acquired. Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty) after some seconds, the window disappear and the recording has been aborted. 
>> So I do not know, what's wrong but something does not seem right in this 'asynchronous writing' option, at least on our config:
>> Windows 7 pro
>> 1024Mb of RAM allocated to Fiji (we can not allocate more, otherwise the Orca won't operate)
>> JRE 1.6.0
>> everything 32bits version
>> 
>> Would be great to get that fixed :-)
>> 
>> With my best regards,
>> 
>> Alexis
>> 
>> 
>> On 14 Aug 2013, at 00:30, Pavel Tomancak <tomancak at mpi-cbg.de> wrote:
>> 
>>> Hi Alexis,
>>> 
>>> That is very strange. I have never seen that. We made some acquisitions today and nothing like that was going on. Its obviously a serious issue. Does it happen only when you have the asynchronous writing enabled? I assume you are running on Windows 7.
>>> 
>>> All the best
>>> 
>>> PAvel
>>> 
>>> -----------------------------------------------------------------------------------
>>> Pavel Tomancak, Ph.D.
>>> 
>>> Group Leader
>>> Max Planck Institute of Molecular Cell Biology and Genetics
>>> Pfotenhauerstr. 108
>>> D-01307 Dresden Tel.: +49 351 210 2670
>>> Germany Fax: +49 351 210 2020
>>> tomancak at mpi-cbg.de
>>> http://www.mpi-cbg.de
>>> -----------------------------------------------------------------------------------
>>> 
>>> 
>>> 
>>> On Aug 13, 2013, at 8:14 PM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:
>>> 
>>>> Hi,
>>>> 
>>>> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
>>>> 
>>>> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
>>>> 
>>>> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
>>>> 
>>>> With my best regards,
>>>> 
>>>> Alexis
>>>> 
>>>> _______________________________________________
>>>> OpenSPIM mailing list
>>>> OpenSPIM at openspim.org
>>>> http://openspim.org/mailman/listinfo/openspim
>>> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130820/4db14a08/attachment.sig>

From stuyvenberg at wisc.edu  Tue Aug 20 13:24:23 2013
From: stuyvenberg at wisc.edu (Luke Stuyvenberg)
Date: Tue, 20 Aug 2013 13:24:23 -0500
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <75f0fb794fc8f.5213b44f@wiscmail.wisc.edu>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<76209bad4c9b2.52122355@wiscmai!> <l.wisc.edu@wiscmail.wisc.edu>
	<7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
	<F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>
	<77509ad34a859.5213a0a0@wiscmail.wisc.edu>
	<747093234c4f4.5213a0dc@wiscmail.wisc.edu>
	<76f0c89648b4c.5213a119@wiscmail.wisc.edu>
	<77809ad24bb9a.5213a155@wiscmail.wisc.edu>
	<7790f9e84a27f.5213a192@wiscmail.wisc.edu>
	<7780ef964d852.5213a1ce@wiscmail.wisc.edu>
	<740096c04dc83.5213a20b@wiscmail.wisc.edu>
	<75e0ec504df4a.5213a247@wiscmail.wisc.edu>
	<75b0a0ad4a219.5213a284@wiscmail.wisc.edu>
	<7530ccce48e90.5213a2c0@wiscmail.wisc.edu>
	<75b0891f4e792.5213a2fd@wiscmail.wisc.edu>
	<7790f2f44e93e.5213a33a@wiscmail.wisc.edu>
	<7780c8764e553.5213a3b5@wiscmail.wisc.edu>
	<7530f9bc4b916.5213a651@wiscmail.wisc.edu>
	<7470ae67490d8.5213a68d@wiscmail.wisc.edu>
	<7780e5eb4ee99.5213a6ca@wiscmail.wisc.edu>
	<77809dea4f72c.5213a706@wiscmail.wisc.edu>
	<7780b8994cb3a.5213a743@wiscmail.wisc.edu>
	<7630b03b495d0.5213a77f@wiscmail.wisc.edu>
	<7470aae6496f4.5213a7bb@wiscmail.wisc.edu>
	<7790f4014969d.5213a7f8@wiscmail.wisc.edu>
	<753095434e34e.5213a834@wiscmail.wisc.edu>
	<74008841498a1.5213a8ad@wiscmail.wisc.edu>
	<75f0b9f14da0c.5213a8e9@wiscmail.wisc.edu>
	<7630b9b54a789.5213a926@wiscmail.wisc.edu>
	<7470f2314fddf.5213a962@wiscmail.wisc.edu>
	<7530c2b9492c6.5213a99f@wiscmail.wisc.edu>
	<7470dd924c0cd.5213b0fd@wiscmail.wisc.edu>
	<7470f76d4f5d8.5213b139@wiscmail.wisc.edu>
	<75b098c04e484.5213b176@wiscmail.wisc.edu>
	<7530cbb0480cf.5213b1b2@wiscmail.wisc.edu>
	<74709e354a777.5213b1ef@wiscmail.wisc.edu>
	<75f0ca114f6fc.5213b22e@wiscmail.wisc.edu>
	<7400fe1c492c0.5213b26c@wiscmail.wisc.edu>
	<7630cd1b4ae42.5213b2a9@wiscmail.wisc.edu>
	<76f0f0df4f0fa.5213b2e5@wiscmail.wisc.edu>
	<7790bef14ad82.5213b39a@wiscmail.wisc.edu>
	<778084b74be17.5213b3d6@wiscmail.wisc.edu>
	<75f0bc25493ed.5213b413@wiscmail.wisc.edu>
	<75f0fb794fc8f.5213b44f@wiscmail.wisc.edu>
Message-ID: <75f0fb424cace.52136e07@wiscmail.wisc.edu>

Hi Alexis,

> I scaled them down to 8bits and saved them on my Mac. You can get an original "shuffled" stack here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiffUnfortunately this stack, too, seems to have no metadata attached to it. If the plugin is outputting this kind of data, I may have a serious problem. :-)


Are you by any chance clicking the Abort! button at the end of an acquisition? Although acquiring the data may finish, the text will keep reading Abort! until the writing is finished; with large data sets, it may seem the process is done and the button has just gotten stuck, when in fact the OME-TIFF writer is updating the metadata in each of its images (I explain this in more detail below.). Clicking Abort! at such a time would stop the metadata from being written.


> Definitively the writing to disk finishes way after the stack has finished to be acquired. 
Upon review, this makes more sense than it did. The basic OME-TIFF specification requires that each file in the dataset have the complete metadata. Unfortunately, there's no way to know what the complete metadata is until all the files have been written. So as the output handler writes each file, it builds up the metadata, until the end of the acquisition. At this point, it reopens each file in the dataset and rewrites the comment to have the correct metadata (this is the long delay I mentioned above -- the more files you write, the longer it takes to put the finished metadata in place). The question, then, is whether or not the last *slice* of each is actually being held back -- I suspect not; the part of the code that appends a slice to the file is unambiguous in its function, while the metadata can easily reach such a size that writing it seems much like writing a new slice. In essence, there's not much we can presently do about this particular quirk, though now that Bio-Formats is outputting daily builds, there may be other options soon.


> > If so, the next time you're running an acquisition, could you try monitoring Fiji's memory usage (Plugins > Utilities > Memory Monitor) as the sequence progresses? I suspect that the bursts you are observing happen because the queue is filling up, forcing the output handler to synchronously write out one or more slices; you should see the memory use climb in a stair-step pattern until one of these bursts, when it should plummet back to nearly nothing.

Actually, thinking on it, the plummeting most likely will not coincide with the writes -- more likely, the images will stick around in memory until the garbage collector decides to run. So the aforementioned pattern should appear in some form or other, though the timing may not tell us very much.


A few other thoughts:
- Do any other programs make regular use of your hard drive? Perhaps Windows 7's automatic (background) defragmentation, or a filesystem-filter antivirus (like avast!, which scans files as they are read and written)? The writing thread is given a low priority so the acquisition can run freely, but this might cause it to be blocked outright by low-level processes making regular use of the hard drive.
- Is your RAM mostly free when you start the acquisition? The maximum length of the image queue is decided based on the available memory at the beginning of an acquisition; I don't yet have a display for it, but it might be being set too low...


I'm working on a slight rework of the async handler which might help to fix these issues; I'll upload it to our update server once it's done. Of course, it's hard to gauge its efficacy until I can reproduce this behavior, so I'll see what I can do on that front as well.


Thanks again,
Luke


On 08/20/13, Alexis Maizelwrote:
> Hi Luke,
> 
> > Did you re-save this data, or are these stacks fresh from the OpenSPIM plugin?
> 
> I scaled them down to 8bits and saved them on my Mac. You can get an original "shuffled" stack here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiff
> 
> > I ask because the OME-TIFF metadata have been clobbered in both stacks (you can check using the Bio-Formats importer); if this is fresh output, the plugin may have a serious error in its metadata generation. Alternatively, Bio-Formats might be out-of-date or acting up. At any rate, the image slices obviously shouldn't be written out of order, but even if they were to be written in the wrong order, were the OME metadata intact, Bio-Formats should be able to display the slices in the correct order (because now it has a table relating the slice's index in the file to its physical Z position).
> 
> I forced Fiji to open the fresh-from-openSPIM stacks on my mac using the Bio-format importer (updated) and the slices are still in the wrong order.
> 
> > The data isn't truly meant to be written in bursts -- ideally it would just chug along in the background... I'll try tweaking the async a little more; probably, the code is doing something incorrectly leading to this. As for the timepoint images, this too is very unusual -- I specifically have the writer finish writing any pending images before finalizing a stack. I'll look into this as well.
> 
> Definitively the writing to disk finishes way after the stack has finished to be acquired. 
> 
> >> Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty) after some seconds, the window disappear and the recording has been aborted.
> > I'm aware of this; I've never been able to determine what that message box is trying to display. No warnings should be appearing during an abort, but this one mysteriously shows up. It's a low priority for me, however; it doesn't seem to affect the abort or change the data that was written (well, any more than aborting mid-acquisition already does).
> 
> I concur that it is not critical, I just wanted to point it out. Glad to read that at least some of what I am experiencing can be reproduced :-)
> 
> > Is everything completely up-to-date, including Bio-Formats? (Note that I haven't yet tried the Bio-Formats daily builds -- whether they will fix or break the plugin has yet to be seen.)
> 
> I will update everything and perform more tests.
> 
> > If so, the next time you're running an acquisition, could you try monitoring Fiji's memory usage (Plugins > Utilities > Memory Monitor) as the sequence progresses? I suspect that the bursts you are observing happen because the queue is filling up, forcing the output handler to synchronously write out one or more slices; you should see the memory use climb in a stair-step pattern until one of these bursts, when it should plummet back to nearly nothing.
> 
> I'll do that and get back to you.
> 
> Thanks for your help,
> 
> Alexis
> 
> 
> 
> > 
> > 
> > Thanks,
> > Luke
> > 
> > On 08/14/13, Alexis Maizel wrote:
> >> Hi Pavel,
> >> 
> >> I did some more tests this morning. 
> >> The asynchronous writing is indeed the culprit. You can get two stacks here: http://dl.dropbox.com/u/484859/async_ON_vs_OFF.zip
> >> 
> >> With async OFF: the images are written to the disk as soon as they are acquired; it is slow but the planes are in the right order.
> >> 
> >> With async ON: the stack is acquired and written 'in bursts' to the disk. Sometimes there is a gap of several seconds between two bursts of disk writing and I have the feeling this corresponds to points when the planes are written in the wrong order. I have also noticed that irrespective of the time delay between two time lapse acquisition, the last image(s) of time point N are written to the disk instant before the time point N+1 starts to be acquired. Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty) after some seconds, the window disappear and the recording has been aborted. 
> >> So I do not know, what's wrong but something does not seem right in this 'asynchronous writing' option, at least on our config:
> >> Windows 7 pro
> >> 1024Mb of RAM allocated to Fiji (we can not allocate more, otherwise the Orca won't operate)
> >> JRE 1.6.0
> >> everything 32bits version
> >> 
> >> Would be great to get that fixed :-)
> >> 
> >> With my best regards,
> >> 
> >> Alexis
> >> 
> >> 
> >> On 14 Aug 2013, at 00:30, Pavel Tomancak <tomancak at mpi-cbg.de> wrote:
> >> 
> >>> Hi Alexis,
> >>> 
> >>> That is very strange. I have never seen that. We made some acquisitions today and nothing like that was going on. Its obviously a serious issue. Does it happen only when you have the asynchronous writing enabled? I assume you are running on Windows 7.
> >>> 
> >>> All the best
> >>> 
> >>> PAvel
> >>> 
> >>> -----------------------------------------------------------------------------------
> >>> Pavel Tomancak, Ph.D.
> >>> 
> >>> Group Leader
> >>> Max Planck Institute of Molecular Cell Biology and Genetics
> >>> Pfotenhauerstr. 108
> >>> D-01307 Dresden Tel.: +49 351 210 2670
> >>> Germany Fax: +49 351 210 2020
> >>> tomancak at mpi-cbg.de
> >>> http://www.mpi-cbg.de
> >>> -----------------------------------------------------------------------------------
> >>> 
> >>> 
> >>> 
> >>> On Aug 13, 2013, at 8:14 PM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:
> >>> 
> >>>> Hi,
> >>>> 
> >>>> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
> >>>> 
> >>>> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
> >>>> 
> >>>> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
> >>>> 
> >>>> With my best regards,
> >>>> 
> >>>> Alexis
> >>>> 
> >>>> _______________________________________________
> >>>> OpenSPIM mailing list
> >>>> OpenSPIM at openspim.org
> >>>> http://openspim.org/mailman/listinfo/openspim
> >>>



From Alexis.Maizel at cos.uni-heidelberg.de  Wed Aug 21 05:18:49 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Wed, 21 Aug 2013 12:18:49 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <75f0fb424cace.52136e07@wiscmail.wisc.edu>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<76209bad4c9b2.52122355@wiscmai! !> <l.wisc.edu@wiscmail.wisc.edu>
	<7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
	<F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>
	<77509ad34a859.5213a0a0@wiscmail.wisc.edu>
	<747093234c4f4.5213a0dc@wiscmail.wisc.edu>
	<76f0c89648b4c.5213a119@wiscmail.wisc.edu>
	<77809ad24bb9a.5213a155@wiscmail.!
	wisc.edu> <7790f9e84a27f.5213a192@wiscmail.wisc.edu> <7780ef96!
	4d852.5213a1ce@wiscmail.wisc.edu>
	<740096c04dc83.5213a20b@wiscmail.wisc.edu>
	<75e0ec504df4a.5213a247@wiscmail.wisc.edu>
	<75b0a0ad4a219.5213a284@wiscmail.wisc.edu>
	<7530ccce48e90.5213a2c0@wiscmail.wisc.edu>
	<75b0891f4e792.5213a2fd@wiscmail.wisc.edu> <7790f2f44e93e.
Message-ID: <599178A9-1BFA-4F1D-895B-76A66D43DAB3@cos.uni-heidelberg.de>

Hi Luke,

some more tests and hopefully some progresses. I scanned a 82Mb stack (1024*1024*41 @ 16bits)

- RAM usage by Fiji and stack writing to disk: 
	~20Mb used at start (out of  1Gb allocated) 
	as the scan progresses in ramps up to ~110Mb and then slowly decreases as the size of the file on disk increases; finally the garbage collector resets to 20Mb
	it takes up to 2 minutes  to write the 84Mb and at least 1 minute more to write the remaining 2Mb (the metadata, I presume...)
- Metadata: 
	I have updated Fiji
	I have tried to update LOCI to the trunk/daily/stable versions but after downloading some files I get an error message ("there was an error updating LOCI")
	It takes a very long time to write the metadata, much longer than the images. You can retrieve one stack with metadata here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiff
	If you open that stack & metadata, you will see that the planes are not written in the right order. Look at the sequence (toward the end of the file above):

<Plane DeltaT="18.37296911600015" PositionX="3205.0" PositionY="2628.0" PositionZ="3085.0" TheT="0" TheZ="36">
<Plane DeltaT="12.293132814000273" PositionX="3205.0" PositionY="2628.0" PositionZ="3025.0" TheT="0" TheZ="37">
<Plane DeltaT="18.884906363000027" PositionX="3205.0" PositionY="2628.0" PositionZ="3090.0" TheT="0" TheZ="38">
 The plane with the index TheZ=37 (physical position in Z 3025 and the timestamp 12.29)  is intercalated between two obviously consecutive slices (TheZ=36 and TheZ=38). This is obviously wrong

I have the feeling that my problems are somehow linked to Bio-Format	not behaving properly. 

With my best regards,

Alexis
 

On 20 Aug 2013, at 20:24, Luke Stuyvenberg <stuyvenberg at wisc.edu> wrote:

> Hi Alexis,
> 
>> I scaled them down to 8bits and saved them on my Mac. You can get an original "shuffled" stack here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiffUnfortunately this stack, too, seems to have no metadata attached to it. If the plugin is outputting this kind of data, I may have a serious problem. :-)
> 
> 
> Are you by any chance clicking the Abort! button at the end of an acquisition? Although acquiring the data may finish, the text will keep reading Abort! until the writing is finished; with large data sets, it may seem the process is done and the button has just gotten stuck, when in fact the OME-TIFF writer is updating the metadata in each of its images (I explain this in more detail below.). Clicking Abort! at such a time would stop the metadata from being written.
> 
> 
>> Definitively the writing to disk finishes way after the stack has finished to be acquired. 
> Upon review, this makes more sense than it did. The basic OME-TIFF specification requires that each file in the dataset have the complete metadata. Unfortunately, there's no way to know what the complete metadata is until all the files have been written. So as the output handler writes each file, it builds up the metadata, until the end of the acquisition. At this point, it reopens each file in the dataset and rewrites the comment to have the correct metadata (this is the long delay I mentioned above -- the more files you write, the longer it takes to put the finished metadata in place). The question, then, is whether or not the last *slice* of each is actually being held back -- I suspect not; the part of the code that appends a slice to the file is unambiguous in its function, while the metadata can easily reach such a size that writing it seems much like writing a new slice. In essence, there's not much we can presently do about this particular quirk, though now that Bio-Formats is outputting daily builds, there may be other options soon.
> 
> 
>>> If so, the next time you're running an acquisition, could you try monitoring Fiji's memory usage (Plugins > Utilities > Memory Monitor) as the sequence progresses? I suspect that the bursts you are observing happen because the queue is filling up, forcing the output handler to synchronously write out one or more slices; you should see the memory use climb in a stair-step pattern until one of these bursts, when it should plummet back to nearly nothing.
> 
> Actually, thinking on it, the plummeting most likely will not coincide with the writes -- more likely, the images will stick around in memory until the garbage collector decides to run. So the aforementioned pattern should appear in some form or other, though the timing may not tell us very much.
> 
> 
> A few other thoughts:
> - Do any other programs make regular use of your hard drive? Perhaps Windows 7's automatic (background) defragmentation, or a filesystem-filter antivirus (like avast!, which scans files as they are read and written)? The writing thread is given a low priority so the acquisition can run freely, but this might cause it to be blocked outright by low-level processes making regular use of the hard drive.
> - Is your RAM mostly free when you start the acquisition? The maximum length of the image queue is decided based on the available memory at the beginning of an acquisition; I don't yet have a display for it, but it might be being set too low...
> 
> 
> I'm working on a slight rework of the async handler which might help to fix these issues; I'll upload it to our update server once it's done. Of course, it's hard to gauge its efficacy until I can reproduce this behavior, so I'll see what I can do on that front as well.
> 
> 
> Thanks again,
> Luke
> 
> 
> On 08/20/13, Alexis Maizel wrote:
>> Hi Luke,
>> 
>>> Did you re-save this data, or are these stacks fresh from the OpenSPIM plugin?
>> 
>> I scaled them down to 8bits and saved them on my Mac. You can get an original "shuffled" stack here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiff
>> 
>>> I ask because the OME-TIFF metadata have been clobbered in both stacks (you can check using the Bio-Formats importer); if this is fresh output, the plugin may have a serious error in its metadata generation. Alternatively, Bio-Formats might be out-of-date or acting up. At any rate, the image slices obviously shouldn't be written out of order, but even if they were to be written in the wrong order, were the OME metadata intact, Bio-Formats should be able to display the slices in the correct order (because now it has a table relating the slice's index in the file to its physical Z position).
>> 
>> I forced Fiji to open the fresh-from-openSPIM stacks on my mac using the Bio-format importer (updated) and the slices are still in the wrong order.
>> 
>>> The data isn't truly meant to be written in bursts -- ideally it would just chug along in the background... I'll try tweaking the async a little more; probably, the code is doing something incorrectly leading to this. As for the timepoint images, this too is very unusual -- I specifically have the writer finish writing any pending images before finalizing a stack. I'll look into this as well.
>> 
>> Definitively the writing to disk finishes way after the stack has finished to be acquired. 
>> 
>>>> Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty) after some seconds, the window disappear and the recording has been aborted.
>>> I'm aware of this; I've never been able to determine what that message box is trying to display. No warnings should be appearing during an abort, but this one mysteriously shows up. It's a low priority for me, however; it doesn't seem to affect the abort or change the data that was written (well, any more than aborting mid-acquisition already does).
>> 
>> I concur that it is not critical, I just wanted to point it out. Glad to read that at least some of what I am experiencing can be reproduced :-)
>> 
>>> Is everything completely up-to-date, including Bio-Formats? (Note that I haven't yet tried the Bio-Formats daily builds -- whether they will fix or break the plugin has yet to be seen.)
>> 
>> I will update everything and perform more tests.
>> 
>>> If so, the next time you're running an acquisition, could you try monitoring Fiji's memory usage (Plugins > Utilities > Memory Monitor) as the sequence progresses? I suspect that the bursts you are observing happen because the queue is filling up, forcing the output handler to synchronously write out one or more slices; you should see the memory use climb in a stair-step pattern until one of these bursts, when it should plummet back to nearly nothing.
>> 
>> I'll do that and get back to you.
>> 
>> Thanks for your help,
>> 
>> Alexis
>> 
>> 
>> 
>>> 
>>> 
>>> Thanks,
>>> Luke
>>> 
>>> On 08/14/13, Alexis Maizel wrote:
>>>> Hi Pavel,
>>>> 
>>>> I did some more tests this morning. 
>>>> The asynchronous writing is indeed the culprit. You can get two stacks here: http://dl.dropbox.com/u/484859/async_ON_vs_OFF.zip
>>>> 
>>>> With async OFF: the images are written to the disk as soon as they are acquired; it is slow but the planes are in the right order.
>>>> 
>>>> With async ON: the stack is acquired and written 'in bursts' to the disk. Sometimes there is a gap of several seconds between two bursts of disk writing and I have the feeling this corresponds to points when the planes are written in the wrong order. I have also noticed that irrespective of the time delay between two time lapse acquisition, the last image(s) of time point N are written to the disk instant before the time point N+1 starts to be acquired. Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty) after some seconds, the window disappear and the recording has been aborted. 
>>>> So I do not know, what's wrong but something does not seem right in this 'asynchronous writing' option, at least on our config:
>>>> Windows 7 pro
>>>> 1024Mb of RAM allocated to Fiji (we can not allocate more, otherwise the Orca won't operate)
>>>> JRE 1.6.0
>>>> everything 32bits version
>>>> 
>>>> Would be great to get that fixed :-)
>>>> 
>>>> With my best regards,
>>>> 
>>>> Alexis
>>>> 
>>>> 
>>>> On 14 Aug 2013, at 00:30, Pavel Tomancak <tomancak at mpi-cbg.de> wrote:
>>>> 
>>>>> Hi Alexis,
>>>>> 
>>>>> That is very strange. I have never seen that. We made some acquisitions today and nothing like that was going on. Its obviously a serious issue. Does it happen only when you have the asynchronous writing enabled? I assume you are running on Windows 7.
>>>>> 
>>>>> All the best
>>>>> 
>>>>> PAvel
>>>>> 
>>>>> -----------------------------------------------------------------------------------
>>>>> Pavel Tomancak, Ph.D.
>>>>> 
>>>>> Group Leader
>>>>> Max Planck Institute of Molecular Cell Biology and Genetics
>>>>> Pfotenhauerstr. 108
>>>>> D-01307 Dresden Tel.: +49 351 210 2670
>>>>> Germany Fax: +49 351 210 2020
>>>>> tomancak at mpi-cbg.de
>>>>> http://www.mpi-cbg.de
>>>>> -----------------------------------------------------------------------------------
>>>>> 
>>>>> 
>>>>> 
>>>>> On Aug 13, 2013, at 8:14 PM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:
>>>>> 
>>>>>> Hi,
>>>>>> 
>>>>>> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
>>>>>> 
>>>>>> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
>>>>>> 
>>>>>> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
>>>>>> 
>>>>>> With my best regards,
>>>>>> 
>>>>>> Alexis
>>>>>> 
>>>>>> _______________________________________________
>>>>>> OpenSPIM mailing list
>>>>>> OpenSPIM at openspim.org
>>>>>> http://openspim.org/mailman/listinfo/openspim
>>>>> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130821/9454bea2/attachment.sig>

From stuyvenberg at wisc.edu  Wed Aug 21 10:36:15 2013
From: stuyvenberg at wisc.edu (Luke Stuyvenberg)
Date: Wed, 21 Aug 2013 10:36:15 -0500
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <7630f58a55128.5214de5b@wiscmail.wisc.edu>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<l.wisc.edu@wiscmail.wisc.edu>
	<7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
	<F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>
	<77509ad34a859.5213a0a0@wiscmail.wisc.edu>
	<747093234c4f4.5213a0dc@wiscmail.wisc.edu>
	<76f0c89648b4c.5213a119@wiscmail.wisc.edu>
	<77809ad24bb9a.5213a155@wiscmail.!>
	<wisc.edu@wiscmail.wisc.edu> <7790f9e84a27f.5213a192@wiscmail.wisc.edu>
	<740096c04dc83.5213a20b@wiscmail.wisc.edu>
	<75e0ec504df4a.5213a247@wiscmail.wisc.edu>
	<75b0a0ad4a219.5213a284@wiscmail.wisc.edu>
	<7530ccce48e90.5213a2c0@wiscmail.wisc.edu>
	<75b0891f4e792.5213a2fd@wiscmail.wisc.edu>
	<599178A9-1BFA-4F1D-895B-76A66D43DAB3@cos.uni-heidelberg.de>
	<75f0ddef53ce9.5214d523@wiscmail.wisc.edu>
	<75e0816d56602.5214d55f@wiscmail.wisc.edu>
	<75f0a2a851b6f.5214d59c@wiscmail.wisc.edu>
	<75f0a40057606.5214d5d8@wiscmail.wisc.edu>
	<75f0e3af52dad.5214d615@wiscmail.wisc.edu>
	<75f0e06155195.5214d651@wiscmail.wisc.edu>
	<75b0e6bb51516.5214d68d@wiscmail.wisc.edu>
	<7470deb554615.5214d6ca@wiscmail.wisc.edu>
	<7630f202501e0.5214d706@wiscmail.wisc.edu>
	<775092f1523b2.5214d743@wiscmail.wisc.edu>
	<7790d02e50e94.5214d77f@wiscmail.wisc.edu>
	<7790c21c574d1.5214d7bc@wiscmail.wisc.edu>
	<7630a4b953c61.5214d7f8@wiscmail.wisc.edu>
	<778083d557c6a.5214d835@wiscmail.wisc.edu>
	<75b0d75851ca2.5214d871@wiscmail.wisc.edu>
	<76309e0255df7.5214d8ae@wiscmail.wisc.edu>
	<7400e434506ad.5214db4b@wiscmail.wisc.edu>
	<7780ee1c522d5.5214dbc3@wiscmail.wisc.edu>
	<75f0c12f51490.5214dbff@wiscmail.wisc.edu>
	<7470df7753be2.5214dc3c@wiscmail.wisc.edu>
	<75b09fac570df.5214dda5@wiscmail.wisc.edu>
	<75e0d65b50858.5214dde2@wiscmail.wisc.edu>
	<7630adff547c9.5214de1e@wiscmail.wisc.edu>
	<7630f58a55128.5214de5b@wiscmail.wisc.edu>
Message-ID: <75f0ad0e57b91.5214981f@wiscmail.wisc.edu>

Hi Alexis,

> some more tests and hopefully some progresses. I scanned a 82Mb stack (1024*1024*41 @ 16bits)
>
> - RAM usage by Fiji and stack writing to disk:
> ~20Mb used at start (out of 1Gb allocated)
> as the scan progresses in ramps up to ~110Mb and then slowly decreases as the size of the file on disk increases; finally the garbage collector resets to 20Mb
> it takes up to 2 minutes to write the 84Mb and at least 1 minute more to write the remaining 2Mb (the metadata, I presume...)
Okay; although the writing times are not at all ideal (my 400MB stacks were written synchronously and in their entirety in a little over a minute and a half), the RAM usage is more or less as expected.


> If you open that stack & metadata, you will see that the planes are not written in the right order. Look at the sequence (toward the end of the file above):
>
> <Plane DeltaT="18.37296911600015" PositionX="3205.0" PositionY="2628.0" PositionZ="3085.0" TheT="0" TheZ="36">
> <Plane DeltaT="12.293132814000273" PositionX="3205.0" PositionY="2628.0" PositionZ="3025.0" TheT="0" TheZ="37">
> <Plane DeltaT="18.884906363000027" PositionX="3205.0" PositionY="2628.0" PositionZ="3090.0" TheT="0" TheZ="38">
> The plane with the index TheZ=37 (physical position in Z 3025 and the timestamp 12.29) is intercalated between two obviously consecutive slices (TheZ=36 and TheZ=38). This is obviously wrong

I have some mixed feelings on this; this is the definitive proof I was looking for, but I'm genuinely at a loss as to how this may have come about. My best guess is that the synchronization is too loose, and somehow the acquisition ("main") thread is writing slices (because it feels it needs to) before the writing thread.


> I have the feeling that my problems are somehow linked to Bio-Format	not behaving properly.

I've tried the new Bio-Formats daily builds update site through the Fiji updater, and haven't noticed any problems from it; perhaps try enabling that update site and downloading the very latest versions?


Finally, I have most of the promised plugin updates. There are two main additions:
- First, it does some timing over the acquisition, outputting the results to the log; although it isn't very detailed, it gives a breakdown of how time is used while acquiring (for example, if the async isn't appropriately asynchronous, the acquisition time will be roughly the same as though it were off).
- Second, and more importantly for you, the async writer has been completely revamped. It now queues the begin/end stack events as well as new slices, so these operations shouldn't *ever* get out of order. It also puts up a monitor showing the state of its queue, which will let us know if it's filling up or not.
If you're willing to play guinea pig again, please try replacing mmplugins/SPIMAcquisition.jar with the version from: https://dl.dropboxusercontent.com/u/57890359/SPIMAcquisition.jar
Anyone is free to try this JAR out, but I haven't put it on our Fiji update site for a few reasons -- the new features are only conditionally useful, and can't (yet) be disabled. That said, if the new async fixes your output problems, I'll polish these changes and incorporate them.


Thanks,
Luke


On 08/21/13, Alexis Maizel  wrote:
> Hi Luke,
> 
> some more tests and hopefully some progresses. I scanned a 82Mb stack (1024*1024*41 @ 16bits)
> 
> - RAM usage by Fiji and stack writing to disk: 
> ~20Mb used at start (out of 1Gb allocated) 
> as the scan progresses in ramps up to ~110Mb and then slowly decreases as the size of the file on disk increases; finally the garbage collector resets to 20Mb
> it takes up to 2 minutes to write the 84Mb and at least 1 minute more to write the remaining 2Mb (the metadata, I presume...)
> - Metadata: 
> I have updated Fiji
> I have tried to update LOCI to the trunk/daily/stable versions but after downloading some files I get an error message ("there was an error updating LOCI")
> It takes a very long time to write the metadata, much longer than the images. You can retrieve one stack with metadata here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiff
> If you open that stack & metadata, you will see that the planes are not written in the right order. Look at the sequence (toward the end of the file above):
> 
> <Plane DeltaT="18.37296911600015" PositionX="3205.0" PositionY="2628.0" PositionZ="3085.0" TheT="0" TheZ="36">
> <Plane DeltaT="12.293132814000273" PositionX="3205.0" PositionY="2628.0" PositionZ="3025.0" TheT="0" TheZ="37">
> <Plane DeltaT="18.884906363000027" PositionX="3205.0" PositionY="2628.0" PositionZ="3090.0" TheT="0" TheZ="38">
> The plane with the index TheZ=37 (physical position in Z 3025 and the timestamp 12.29) is intercalated between two obviously consecutive slices (TheZ=36 and TheZ=38). This is obviously wrong
> 
> I have the feeling that my problems are somehow linked to Bio-Format	not behaving properly. 
> 
> With my best regards,
> 
> Alexis
> 
> 
> On 20 Aug 2013, at 20:24, Luke Stuyvenberg <stuyvenberg at wisc.edu> wrote:
> 
> > Hi Alexis,
> > 
> >> I scaled them down to 8bits and saved them on my Mac. You can get an original "shuffled" stack here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiffUnfortunately this stack, too, seems to have no metadata attached to it. If the plugin is outputting this kind of data, I may have a serious problem. :-)
> > 
> > 
> > Are you by any chance clicking the Abort! button at the end of an acquisition? Although acquiring the data may finish, the text will keep reading Abort! until the writing is finished; with large data sets, it may seem the process is done and the button has just gotten stuck, when in fact the OME-TIFF writer is updating the metadata in each of its images (I explain this in more detail below.). Clicking Abort! at such a time would stop the metadata from being written.
> > 
> > 
> >> Definitively the writing to disk finishes way after the stack has finished to be acquired. 
> > Upon review, this makes more sense than it did. The basic OME-TIFF specification requires that each file in the dataset have the complete metadata. Unfortunately, there's no way to know what the complete metadata is until all the files have been written. So as the output handler writes each file, it builds up the metadata, until the end of the acquisition. At this point, it reopens each file in the dataset and rewrites the comment to have the correct metadata (this is the long delay I mentioned above -- the more files you write, the longer it takes to put the finished metadata in place). The question, then, is whether or not the last *slice* of each is actually being held back -- I suspect not; the part of the code that appends a slice to the file is unambiguous in its function, while the metadata can easily reach such a size that writing it seems much like writing a new slice. In essence, there's not much we can presently do about this particular quirk, though now that Bio-Formats is outputting daily builds, there may be other options soon.
> > 
> > 
> >>> If so, the next time you're running an acquisition, could you try monitoring Fiji's memory usage (Plugins > Utilities > Memory Monitor) as the sequence progresses? I suspect that the bursts you are observing happen because the queue is filling up, forcing the output handler to synchronously write out one or more slices; you should see the memory use climb in a stair-step pattern until one of these bursts, when it should plummet back to nearly nothing.
> > 
> > Actually, thinking on it, the plummeting most likely will not coincide with the writes -- more likely, the images will stick around in memory until the garbage collector decides to run. So the aforementioned pattern should appear in some form or other, though the timing may not tell us very much.
> > 
> > 
> > A few other thoughts:
> > - Do any other programs make regular use of your hard drive? Perhaps Windows 7's automatic (background) defragmentation, or a filesystem-filter antivirus (like avast!, which scans files as they are read and written)? The writing thread is given a low priority so the acquisition can run freely, but this might cause it to be blocked outright by low-level processes making regular use of the hard drive.
> > - Is your RAM mostly free when you start the acquisition? The maximum length of the image queue is decided based on the available memory at the beginning of an acquisition; I don't yet have a display for it, but it might be being set too low...
> > 
> > 
> > I'm working on a slight rework of the async handler which might help to fix these issues; I'll upload it to our update server once it's done. Of course, it's hard to gauge its efficacy until I can reproduce this behavior, so I'll see what I can do on that front as well.
> > 
> > 
> > Thanks again,
> > Luke
> > 
> > 
> > On 08/20/13, Alexis Maizel wrote:
> >> Hi Luke,
> >> 
> >>> Did you re-save this data, or are these stacks fresh from the OpenSPIM plugin?
> >> 
> >> I scaled them down to 8bits and saved them on my Mac. You can get an original "shuffled" stack here: http://dl.dropbox.com/u/484859/spim_TL01_Angle0.ome.tiff
> >> 
> >>> I ask because the OME-TIFF metadata have been clobbered in both stacks (you can check using the Bio-Formats importer); if this is fresh output, the plugin may have a serious error in its metadata generation. Alternatively, Bio-Formats might be out-of-date or acting up. At any rate, the image slices obviously shouldn't be written out of order, but even if they were to be written in the wrong order, were the OME metadata intact, Bio-Formats should be able to display the slices in the correct order (because now it has a table relating the slice's index in the file to its physical Z position).
> >> 
> >> I forced Fiji to open the fresh-from-openSPIM stacks on my mac using the Bio-format importer (updated) and the slices are still in the wrong order.
> >> 
> >>> The data isn't truly meant to be written in bursts -- ideally it would just chug along in the background... I'll try tweaking the async a little more; probably, the code is doing something incorrectly leading to this. As for the timepoint images, this too is very unusual -- I specifically have the writer finish writing any pending images before finalizing a stack. I'll look into this as well.
> >> 
> >> Definitively the writing to disk finishes way after the stack has finished to be acquired. 
> >> 
> >>>> Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty) after some seconds, the window disappear and the recording has been aborted.
> >>> I'm aware of this; I've never been able to determine what that message box is trying to display. No warnings should be appearing during an abort, but this one mysteriously shows up. It's a low priority for me, however; it doesn't seem to affect the abort or change the data that was written (well, any more than aborting mid-acquisition already does).
> >> 
> >> I concur that it is not critical, I just wanted to point it out. Glad to read that at least some of what I am experiencing can be reproduced :-)
> >> 
> >>> Is everything completely up-to-date, including Bio-Formats? (Note that I haven't yet tried the Bio-Formats daily builds -- whether they will fix or break the plugin has yet to be seen.)
> >> 
> >> I will update everything and perform more tests.
> >> 
> >>> If so, the next time you're running an acquisition, could you try monitoring Fiji's memory usage (Plugins > Utilities > Memory Monitor) as the sequence progresses? I suspect that the bursts you are observing happen because the queue is filling up, forcing the output handler to synchronously write out one or more slices; you should see the memory use climb in a stair-step pattern until one of these bursts, when it should plummet back to nearly nothing.
> >> 
> >> I'll do that and get back to you.
> >> 
> >> Thanks for your help,
> >> 
> >> Alexis
> >> 
> >> 
> >> 
> >>> 
> >>> 
> >>> Thanks,
> >>> Luke
> >>> 
> >>> On 08/14/13, Alexis Maizel wrote:
> >>>> Hi Pavel,
> >>>> 
> >>>> I did some more tests this morning. 
> >>>> The asynchronous writing is indeed the culprit. You can get two stacks here: http://dl.dropbox.com/u/484859/async_ON_vs_OFF.zip
> >>>> 
> >>>> With async OFF: the images are written to the disk as soon as they are acquired; it is slow but the planes are in the right order.
> >>>> 
> >>>> With async ON: the stack is acquired and written 'in bursts' to the disk. Sometimes there is a gap of several seconds between two bursts of disk writing and I have the feeling this corresponds to points when the planes are written in the wrong order. I have also noticed that irrespective of the time delay between two time lapse acquisition, the last image(s) of time point N are written to the disk instant before the time point N+1 starts to be acquired. Also, if one abort a time lapse recording in between two time points, but when all planes have not yet been written to disk, then there is a warning windows displayed (which is empty) after some seconds, the window disappear and the recording has been aborted. 
> >>>> So I do not know, what's wrong but something does not seem right in this 'asynchronous writing' option, at least on our config:
> >>>> Windows 7 pro
> >>>> 1024Mb of RAM allocated to Fiji (we can not allocate more, otherwise the Orca won't operate)
> >>>> JRE 1.6.0
> >>>> everything 32bits version
> >>>> 
> >>>> Would be great to get that fixed :-)
> >>>> 
> >>>> With my best regards,
> >>>> 
> >>>> Alexis
> >>>> 
> >>>> 
> >>>> On 14 Aug 2013, at 00:30, Pavel Tomancak <tomancak at mpi-cbg.de> wrote:
> >>>> 
> >>>>> Hi Alexis,
> >>>>> 
> >>>>> That is very strange. I have never seen that. We made some acquisitions today and nothing like that was going on. Its obviously a serious issue. Does it happen only when you have the asynchronous writing enabled? I assume you are running on Windows 7.
> >>>>> 
> >>>>> All the best
> >>>>> 
> >>>>> PAvel
> >>>>> 
> >>>>> -----------------------------------------------------------------------------------
> >>>>> Pavel Tomancak, Ph.D.
> >>>>> 
> >>>>> Group Leader
> >>>>> Max Planck Institute of Molecular Cell Biology and Genetics
> >>>>> Pfotenhauerstr. 108
> >>>>> D-01307 Dresden Tel.: +49 351 210 2670
> >>>>> Germany Fax: +49 351 210 2020
> >>>>> tomancak at mpi-cbg.de
> >>>>> http://www.mpi-cbg.de
> >>>>> -----------------------------------------------------------------------------------
> >>>>> 
> >>>>> 
> >>>>> 
> >>>>> On Aug 13, 2013, at 8:14 PM, Alexis Maizel <Alexis.Maizel at cos.uni-heidelberg.de> wrote:
> >>>>> 
> >>>>>> Hi,
> >>>>>> 
> >>>>>> I have noticed that when acquiring stacks during a time lapse and writing them to disk, using the 'asynchronous writing' option, the order in which the individual images are laid into the stack is imprecise. What I mean is that an image obviously in the middle of the stack is shifted toward the end. I did not observed a fixed pattern, except that usually the first 15-20 planes are in the right order and the mess is a the end.
> >>>>>> 
> >>>>>> I have carefully observed and the problem does not come from the stage 'going back and forth' during acquisition. It is upon writing to the disk that the problem seems to occur. Also I have noticed that it takes quite a long time (up to 3 minutes) to write to disk a ~400Mb stack. 
> >>>>>> 
> >>>>>> You can see more precisely what I am talking about by looking at two representative stacks: http://dl.dropbox.com/u/484859/Stacks.zip
> >>>>>> 
> >>>>>> With my best regards,
> >>>>>> 
> >>>>>> Alexis
> >>>>>> 
> >>>>>> _______________________________________________
> >>>>>> OpenSPIM mailing list
> >>>>>> OpenSPIM at openspim.org
> >>>>>> http://openspim.org/mailman/listinfo/openspim
> >>>>>



From stuyvenberg at wisc.edu  Thu Aug 22 08:17:54 2013
From: stuyvenberg at wisc.edu (Luke Stuyvenberg)
Date: Thu, 22 Aug 2013 08:17:54 -0500
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <77108c785febc.52160f81@wiscmail.wisc.edu>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<l.wisc.edu@wiscmail.wisc.edu>
	<7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
	<F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>
	<77509ad34a859.5213a0a0@wiscmail.wisc.edu>
	<747093234c4f4.5213a0dc@wiscmail.wisc.edu>
	<76f0c89648b4c.5213a119@wiscmail.wisc.edu>
	<77809ad24bb9a.5213a155@wiscmail.!>
	<wisc.edu@wiscmail.wisc.edu> <7790f9e84a27f.5213a192@wiscmail.wisc.edu>
	<740096c04dc83.5213a20b@wiscmail.wisc.edu>
	<75e0ec504df4a.5213a247@wiscmail.wisc.edu>
	<75b0a0ad4a219.5213a284@wiscmail.wisc.edu>
	<7530ccce48e90.5213a2c0@wiscmail.wisc.edu>
	<75b0891f4e792.5213a2fd@wiscmail.wisc.edu>
	<599178A9-1BFA-4F1D-895B-76A66D43DAB3@cos.uni-heidelberg.de>
	<75f0ddef53ce9.5214d523@wiscmail.wisc.edu>
	<75e0816d56602.5214d55f@wiscmail.wisc.edu>
	<75f0a2a851b6f.5214d59c@wiscmail.wisc.edu>
	<75f0a40057606.5214d5d8@wiscmail.wisc.edu>
	<75f0e3af52dad.5214d615@wiscmail.wisc.edu>
	<75f0e06155195.5214d651@wiscmail.wisc.edu>
	<75b0e6bb51516.5214d68d@wiscmail.wisc.edu>
	<7470deb554615.5214d6ca@wiscmail.wisc.edu>
	<7630f202501e0.5214d706@wiscmail.wisc.edu>
	<775092f1523b2.5214d743@wiscmail.wisc.edu>
	<7790d02e50e94.5214d77f@wiscmail.wisc.edu>
	<7790c21c574d1.5214d7bc@wiscmail.wisc.edu>
	<7630a4b953c61.5214d7f8@wiscmail.wisc.edu>
	<778083d557c6a.5214d835@wiscmail.wisc.edu>
	<75b0d75851ca2.5214d871@wiscmail.wisc.edu>
	<76309e0255df7.5214d8ae@wiscmail.wisc.edu>
	<7400e434506ad.5214db4b@wiscmail.wisc.edu>
	<7780ee1c522d5.5214dbc3@wiscmail.wisc.edu>
	<75f0c12f51490.5214dbff@wiscmail.wisc.edu>
	<7470df7753be2.5214dc3c@wiscmail.wisc.edu>
	<75b09fac570df.5214dda5@wiscmail.wisc.edu>
	<75e0d65b50858.5214dde2@wiscmail.wisc.edu>
	<7630adff547c9.5214de1e@wiscmail.wisc.edu>
	<7630f58a55128.5214de5b@wiscmail.wisc.edu>
	<75f0ad0e57b91.5214981f@wiscmail.wisc.edu>
	<7790bc795f1c7.52160ecc@wiscmail.wisc.edu>
	<76d0b5005ff34.52160f08@wiscmail.wisc.edu>
	<7640976e5c55d.52160f44@wiscmail.wisc.edu>
	<77108c785febc.52160f81@wiscmail.wisc.edu>
Message-ID: <7690dfdf5d53d.5215c932@wiscmail.wisc.edu>

Hi Alexis,

Here's the email I sent, sans the rest of the conversation thread (which may have been the cause of the rejection).


Thanks,
Luke


P.S. The source code for the linked plugin is available, but split up into two branches on github -- async-fix and profiling -- in case anyone is looking for it. At the moment, neither merges cleanly into the main branch, since I've just merged the new device manager code yesterday.

On 08/21/13, "Luke Stuyvenberg"  wrote:
> Hi Alexis,
> 
> > some more tests and hopefully some progresses. I scanned a 82Mb stack (1024*1024*41 @ 16bits)
> >
> > - RAM usage by Fiji and stack writing to disk:
> > ~20Mb used at start (out of 1Gb allocated)
> > as the scan progresses in ramps up to ~110Mb and then slowly decreases as the size of the file on disk increases; finally the garbage collector resets to 20Mb
> > it takes up to 2 minutes to write the 84Mb and at least 1 minute more to write the remaining 2Mb (the metadata, I presume...)
> Okay; although the writing times are not at all ideal (my 400MB stacks were written synchronously and in their entirety in a little over a minute and a half), the RAM usage is more or less as expected.
> 
> 
> > If you open that stack & metadata, you will see that the planes are not written in the right order. Look at the sequence (toward the end of the file above):
> 
> I have some mixed feelings on this; this is the definitive proof I was looking for, but I'm genuinely at a loss as to how this may have come about. My best guess is that the synchronization is too loose, and somehow the acquisition ("main") thread is writing slices (because it feels it needs to) before the writing thread.
> 
> 
> > I have the feeling that my problems are somehow linked to Bio-Format	not behaving properly.
> 
> I've tried the new Bio-Formats daily builds update site through the Fiji updater, and haven't noticed any problems from it; perhaps try enabling that update site and downloading the very latest versions?
> 
> 
> Finally, I have most of the promised plugin updates. There are two main additions:
> - First, it does some timing over the acquisition, outputting the results to the log; although it isn't very detailed, it gives a breakdown of how time is used while acquiring (for example, if the async isn't appropriately asynchronous, the acquisition time will be roughly the same as though it were off).
> - Second, and more importantly for you, the async writer has been completely revamped. It now queues the begin/end stack events as well as new slices, so these operations shouldn't *ever* get out of order. It also puts up a monitor showing the state of its queue, which will let us know if it's filling up or not.
> If you're willing to play guinea pig again, please try replacing mmplugins/SPIMAcquisition.jar with the version from: https://dl.dropboxusercontent.com/u/57890359/SPIMAcquisition.jar
> Anyone is free to try this JAR out, but I haven't put it on our Fiji update site for a few reasons -- the new features are only conditionally useful, and can't (yet) be disabled. That said, if the new async fixes your output problems, I'll polish these changes and incorporate them.
> 
> 
> Thanks,
> Luke
> 
>



From Alexis.Maizel at cos.uni-heidelberg.de  Thu Aug 22 10:31:30 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Thu, 22 Aug 2013 17:31:30 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <7690dfdf5d53d.5215c932@wiscmail.wisc.edu>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<l.wisc.edu@wiscmail.wisc.edu> !
	<7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
	<F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>
	<77509ad34a859.5213a0a0@wiscmail.wisc.edu>
	<747093234c4f4.5213a0dc@wiscmail.wisc.edu>
	<76f0c89648b4c.5213a119@wiscmail.wisc.edu>
	<77809ad24bb9a.5213a155@wiscmail.!>
	<wisc.edu@wiscmail.wisc.edu> <7!
	790f9e84a27f.5213a192@wiscmail.wisc.edu> <740096c04dc83.5213a2!
	0b@wiscmail.wisc.edu> <75e0ec504df4a.5213a247@wiscmail.wisc.edu>
	<75b0a0ad4a219.5213a284@wiscmail.wisc.edu>
	<7530ccce48e90.5213a2c0@wiscmail.wisc.edu>
	<75b0891f4e792.5213a2fd@wiscmail.wisc.edu>
	<599178A9-1BFA-4F1D-895B-76A66D43DAB3@cos.uni-heidelberg.de>
	<75f0ddef
Message-ID: <82A3F18D-4C6E-40C0-B674-37C4F04C2DA0@cos.uni-heidelberg.de>

Hi Luke,
thanks a lot! I'll test the new SPIMAcquisition and let you know. 
With my best regards,

Alexis

On 22 Aug 2013, at 15:17, Luke Stuyvenberg <stuyvenberg at wisc.edu> wrote:

> Hi Alexis,
> 
> Here's the email I sent, sans the rest of the conversation thread (which may have been the cause of the rejection).
> 
> 
> Thanks,
> Luke
> 
> 
> P.S. The source code for the linked plugin is available, but split up into two branches on github -- async-fix and profiling -- in case anyone is looking for it. At the moment, neither merges cleanly into the main branch, since I've just merged the new device manager code yesterday.
> 
> On 08/21/13, "Luke Stuyvenberg"  wrote:
>> Hi Alexis,
>> 
>>> some more tests and hopefully some progresses. I scanned a 82Mb stack (1024*1024*41 @ 16bits)
>>>  
>>> - RAM usage by Fiji and stack writing to disk: 
>>> ~20Mb used at start (out of 1Gb allocated) 
>>> as the scan progresses in ramps up to ~110Mb and then slowly decreases as the size of the file on disk increases; finally the garbage collector resets to 20Mb
>>> it takes up to 2 minutes to write the 84Mb and at least 1 minute more to write the remaining 2Mb (the metadata, I presume...)
>> Okay; although the writing times are not at all ideal (my 400MB stacks were written synchronously and in their entirety in a little over a minute and a half), the RAM usage is more or less as expected.
>> 
>> 
>>> If you open that stack & metadata, you will see that the planes are not written in the right order. Look at the sequence (toward the end of the file above):
>> 
>> I have some mixed feelings on this; this is the definitive proof I was looking for, but I'm genuinely at a loss as to how this may have come about. My best guess is that the synchronization is too loose, and somehow the acquisition ("main") thread is writing slices (because it feels it needs to) before the writing thread.
>> 
>> 
>>> I have the feeling that my problems are somehow linked to Bio-Format	not behaving properly. 
>> 
>> I've tried the new Bio-Formats daily builds update site through the Fiji updater, and haven't noticed any problems from it; perhaps try enabling that update site and downloading the very latest versions?
>> 
>> 
>> Finally, I have most of the promised plugin updates. There are two main additions:
>> - First, it does some timing over the acquisition, outputting the results to the log; although it isn't very detailed, it gives a breakdown of how time is used while acquiring (for example, if the async isn't appropriately asynchronous, the acquisition time will be roughly the same as though it were off).
>> - Second, and more importantly for you, the async writer has been completely revamped. It now queues the begin/end stack events as well as new slices, so these operations shouldn't *ever* get out of order. It also puts up a monitor showing the state of its queue, which will let us know if it's filling up or not.
>> If you're willing to play guinea pig again, please try replacing mmplugins/SPIMAcquisition.jar with the version from: https://dl.dropboxusercontent.com/u/57890359/SPIMAcquisition.jar
>> Anyone is free to try this JAR out, but I haven't put it on our Fiji update site for a few reasons -- the new features are only conditionally useful, and can't (yet) be disabled. That said, if the new async fixes your output problems, I'll polish these changes and incorporate them.
>> 
>> 
>> Thanks,
>> Luke
>> 
>> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130822/0feec6aa/attachment.sig>

From Alexis.Maizel at cos.uni-heidelberg.de  Fri Aug 23 04:34:18 2013
From: Alexis.Maizel at cos.uni-heidelberg.de (Alexis Maizel)
Date: Fri, 23 Aug 2013 11:34:18 +0200
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <7690dfdf5d53d.5215c932@wiscmail.wisc.edu>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<l.wisc.edu@wiscmail.wisc.edu> !
	<7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
	<F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>
	<77509ad34a859.5213a0a0@wiscmail.wisc.edu>
	<747093234c4f4.5213a0dc@wiscmail.wisc.edu>
	<76f0c89648b4c.5213a119@wiscmail.wisc.edu>
	<77809ad24bb9a.5213a155@wiscmail.!>
	<wisc.edu@wiscmail.wisc.edu> <7!
	790f9e84a27f.5213a192@wiscmail.wisc.edu> <740096c04dc83.5213a2!
	0b@wiscmail.wisc.edu> <75e0ec504df4a.5213a247@wiscmail.wisc.edu>
	<75b0a0ad4a219.5213a284@wiscmail.wisc.edu>
	<7530ccce48e90.5213a2c0@wiscmail.wisc.edu>
	<75b0891f4e792.5213a2fd@wiscmail.wisc.edu>
	<599178A9-1BFA-4F1D-895B-76A66D43DAB3@cos.uni-heidelberg.de>
	<75f0ddef
Message-ID: <C652514F-B972-49DE-81BC-0ADBFD36AFEB@cos.uni-heidelberg.de>

Hi Luke,

I did some test and have good news: the new SPIMAcquisition.jar solves the issue. The slices are now reliably written in the right order. I also have the feeling it is a bit faster than the 'original/old' one and the metadata a properly written. 

You can get the stacks and logs of my tests here: http://dl.dropbox.com/u/484859/2013-08-23.zip

I did 3 acquisitions:
- test 1: A  stack of 40 slices (1024*1024)  with the original  SPIMAcquisition.jar; the slices are mixed up. You will also notice in the log.txt file the message "Warning: asynchronous writer may have been cancelled before completing. (0)" I had that everytime the metadata are not written. I did not hit "abort" at any time; after acquisition I simply waited for the file to finish writing to disk (a couple of minutes)  and opened it in Fiji (then the message appeared in the log).

-test2: same thing, but with the new SPIMAcquisition.jar. Everything is fine (slices in the right order & metadata present). I observed the async monitor: it climbed to 27/240 (writing) then slowly decreased to 0/240 (Idle). Overall it felt faster.

-test3: time lapse (5 time steps) of  a 40-slices stack; one stack every 120sec. Absolutely no problems. The async monitor showed the same behaviour as for test 2.    

I'll keep on using this version until the next update. 

Thanks a lot for your help.

With my best regards,

Alexis


>> - First, it does some timing over the acquisition, outputting the results to the log; although it isn't very detailed, it gives a breakdown of how time is used while acquiring (for example, if the async isn't appropriately asynchronous, the acquisition time will be roughly the same as though it were off).
>> - Second, and more importantly for you, the async writer has been completely revamped. It now queues the begin/end stack events as well as new slices, so these operations shouldn't *ever* get out of order. It also puts up a monitor showing the state of its queue, which will let us know if it's filling up or not.
>> If you're willing to play guinea pig again, please try replacing mmplugins/SPIMAcquisition.jar with the version from: https://dl.dropboxusercontent.com/u/57890359/SPIMAcquisition.jar
>> Anyone is free to try this JAR out, but I haven't put it on our Fiji update site for a few reasons -- the new features are only conditionally useful, and can't (yet) be disabled. That said, if the new async fixes your output problems, I'll polish these changes and incorporate them.
>> 
>> 
>> Thanks,
>> Luke
>> 
>> 

-------------- next part --------------
A non-text attachment was scrubbed...
Name: signature.asc
Type: application/pgp-signature
Size: 363 bytes
Desc: Message signed with OpenPGP using GPGMail
URL: <http://openspim.org/pipermail/openspim/attachments/20130823/aee6240a/attachment.sig>

From stuyvenberg at wisc.edu  Fri Aug 23 14:49:30 2013
From: stuyvenberg at wisc.edu (LUKE ADAM STUYVENBERG)
Date: Fri, 23 Aug 2013 14:49:30 -0500
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <C652514F-B972-49DE-81BC-0ADBFD36AFEB@cos.uni-heidelberg.de>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<7720e3744fa3d.52122391@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
	<F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>
	<77509ad34a859.5213a0a0@wiscmail.wisc.edu>
	<747093234c4f4.5213a0dc@wiscmail.wisc.edu>
	<76f0c89648b4c.5213a119@wiscmail.wisc.edu>
	<77809ad24bb9a.5213a155@wiscmail.!>
	<wisc.edu@wiscmail.wisc.edu> <75e0ec504df4a.5213a247@wiscmail.wisc.edu>
	<75b0a0ad4a219.5213a284@wiscmail.wisc.edu>
	<7530ccce48e90.5213a2c0@wiscmail.wisc.edu>
	<75b0891f4e792.5213a2fd@wiscmail.wisc.edu>
	<599178A9-1BFA-4F1D-895B-76A66D43DAB3@cos.uni-heidelberg.de>
	<75f0ddef@wiscmail.wisc.edu>
	<C652514F-B972-49DE-81BC-0ADBFD36AFEB@cos.uni-heidelberg.de>
Message-ID: <4b0909c42b9b27fdb84a5be0b48f742e@wiscmail.wisc.edu>

Hi Alexis,

> I did some test and have good news: the new SPIMAcquisition.jar solves the
> issue. The slices are now reliably written in the right order. I also have
> the feeling it is a bit faster than the 'original/old' one and the metadata a
> properly written.
Excellent!

> -test2: same thing, but with the new SPIMAcquisition.jar. Everything is fine
> (slices in the right order & metadata present). I observed the async monitor:
> it climbed to 27/240 (writing) then slowly decreased to 0/240 (Idle). Overall
> it felt faster.
My async monitor still rarely exceeds 1 before writing it out, though I've
noticed that sometimes it snowballs (i.e. if it doesn't get one written quickly
enough, they start to pile up until a break in the acquisition). Still, 27/240
gives lots of room, and that limit is pretty conservative. It's good to know
things are working properly now, and the associated speed boost is also a big
plus.

> I'll keep on using this version until the next update.
If I don't get the update uploaded today, it will almost certainly be Monday. At
this point my biggest obstacle is deciding where to put the checkboxes to make
it show the monitor. ;-)

I should warn you in advance, though I'll try to make it clear when I announce
the update: the next update *will* obsolete the changes you made to the Cobalt
adapter. The reason is because it will be designed to work with the Cobalt
adapter in MM's nightly build -- I'm afraid you will need to revert your
changes.

Thanks again for experimenting, and for bringing this to light!

Luke

On Fri, 23 Aug 2013 11:34:18 +0200 Alexis Maizel
<Alexis.Maizel at cos.uni-heidelberg.de> wrote

> Hi Luke,
> 
> I did some test and have good news: the new SPIMAcquisition.jar solves the
> issue. The slices are now reliably written in the right order. I also have
> the feeling it is a bit faster than the 'original/old' one and the metadata a
> properly written.
> 
> You can get the stacks and logs of my tests here:
> http://dl.dropbox.com/u/484859/2013-08-23.zip
> 
> I did 3 acquisitions:
> - test 1: A  stack of 40 slices (1024*1024)  with the original 
> SPIMAcquisition.jar; the slices are mixed up. You will also notice in the
> log.txt file the message "Warning: asynchronous writer may have been
> cancelled before completing. (0)" I had that everytime the metadata are not
> written. I did not hit "abort" at any time; after acquisition I simply waited
> for the file to finish writing to disk (a couple of minutes)  and opened it
> in Fiji (then the message appeared in the log).
> 
> -test2: same thing, but with the new SPIMAcquisition.jar. Everything is fine
> (slices in the right order & metadata present). I observed the async monitor:
> it climbed to 27/240 (writing) then slowly decreased to 0/240 (Idle). Overall
> it felt faster.
> 
> -test3: time lapse (5 time steps) of  a 40-slices stack; one stack every
> 120sec. Absolutely no problems. The async monitor showed the same behaviour
> as for test 2.
> 
> I'll keep on using this version until the next update.
> 
> Thanks a lot for your help.
> 
> With my best regards,
> 
> Alexis
> 
> 
> >> - First, it does some timing over the acquisition, outputting the results
> >> to the log; although it isn't very detailed, it gives a breakdown of how
time
> >> is used while acquiring (for example, if the async isn't appropriately
> >> asynchronous, the acquisition time will be roughly the same as though it
were
> >> off).
> >> - Second, and more importantly for you, the async writer has been
> >> completely revamped. It now queues the begin/end stack events as well as
new
> >> slices, so these operations shouldn't *ever* get out of order. It also puts
> >> up a monitor showing the state of its queue, which will let us know if it's
> >> filling up or not.
> >> If you're willing to play guinea pig again, please try replacing
> >> mmplugins/SPIMAcquisition.jar with the version from:
> >> https://dl.dropboxusercontent.com/u/57890359/SPIMAcquisition.jar
> >> Anyone is free to try this JAR out, but I haven't put it on our Fiji
> >> update site for a few reasons -- the new features are only conditionally
> >> useful, and can't (yet) be disabled. That said, if the new async fixes your
> >> output problems, I'll polish these changes and incorporate them.
> >>
> >>
> >> Thanks,
> >> Luke
> >>
> >>





From stuyvenberg at wisc.edu  Mon Aug 26 08:59:00 2013
From: stuyvenberg at wisc.edu (LUKE ADAM STUYVENBERG)
Date: Mon, 26 Aug 2013 08:59:00 -0500
Subject: [OpenSPIM] asynchronous writing of stacks bug?
In-Reply-To: <7CD33490-933E-4455-805D-11F755A6C170@cos.uni-heidelberg.de>
References: <C9336363-B38B-49A9-BE0C-D34965A0F55B@cos.uni-heidelberg.de>
	<B9EA2084-CD9B-4788-AE3A-68B34E689F99@mpi-cbg.de>
	<698586BA-C7E4-479D-8D6C-DB66B50208E4@cos.uni-heidelberg.de>
	<761094d849279.52121d6c@wiscmail.wisc.edu>
	<74e0e6274ffe4.52121da8@wiscmail.wisc.edu>
	<7700c402485ab.52121de5@wiscmail.wisc.edu>
	<7790cfc249ae6.52121e21@wiscmail.wisc.edu>
	<7700e19e4eea9.52121e5d@wiscmail.wisc.edu>
	<7700c6584b3a7.52121f4e@wiscmail.wisc.edu>
	<76509acd48e55.52121f8a@wiscmail.wisc.edu>
	<7780e9e54ae49.52121fc6@wiscmail.wisc.edu>
	<74f0908149b5c.52122003@wiscmail.wisc.edu>
	<773086104b284.5212203f@wiscmail.wisc.edu>
	<7780d77e4ef1e.5212207b@wiscmail.wisc.edu>
	<7780fe0049625.521220f7@wiscmail.wisc.edu>
	<77809f6b4c67b.52122133@wiscmail.wisc.edu>
	<7620d8bb4c942.5212216f@wiscmail.wisc.edu>
	<7660c1354d2bf.521221ab@wiscmail.wisc.edu>
	<7700db414cf27.52122224@wiscmail.wisc.edu>
	<76109ec848c86.52122260@wiscmail.wisc.edu>
	<7650d96c4d050.5212229c@wiscmail.wisc.edu>
	<7720e3744fa3d.52122391@wiscmai!> <l.wisc.edu@wiscmail.wisc.edu>
	<76608f484dc66.52122ed2@wiscmail.wisc.edu>
	<7660b1dd4b463.52122f0e@wiscmail.wisc.edu>
	<74f0c1334b3f1.52122f88@wiscmail.wisc.edu>
	<7780e8604f40b.521230bb@wiscmail.wisc.edu>
	<7620994a4d0be.521230f7@wiscmail.wisc.edu>
	<74e0bb444ee7d.52123134@wiscmail.wisc.edu>
	<7780dac64d38f.52123171@wiscmail.wisc.edu>
	<762097a349c95.5212523c@wiscmail.wisc.edu>
	<76609e8b49945.5212527a@wiscmail.wisc.edu>
	<7720ebd148a9e.521252b6@wiscmail.wisc.edu>
	<7610d67e4aa56.5212532f@wiscmail.wisc.edu>
	<7610f1524f771.5212536b@wiscmail.wisc.edu>
	<7550b82d496f2.521253a8@wiscmail.wisc.edu>
	<74d0e0a24e25e.521253e4@wiscmail.wisc.edu>
	<75f08c45483b1.52125421@wiscmail.wisc.edu>
	<7530b15b4bd9a.52120df0@wiscmail.wisc.edu>
	<F8F8892C-7FD7-4534-BF1B-96B33AB13A3E@cos.uni-heidelberg.de>
	<77509ad34a859.5213a0a0@wiscmail.wisc.edu>
	<747093234c4f4.5213a0dc@wiscmail.wisc.edu>
	<76f0c89648b4c.5213a119@wiscmail.wisc.edu>
	<77809ad24bb9a.5213a155@wiscmail.!>
	<wisc.edu@wiscmail.wisc.edu> <75e0ec504df4a.5213a247@wiscmail.!>
	<wisc.edu@wiscmail.wisc.edu> <75b0a0ad4a219.5213a284@wiscmail.wisc.edu>
	<75b0891f4e792.5213a2fd@wiscmail.wisc.edu>
	<599178A9-1BFA-4F1D-895B-76A66D43DAB3@cos.uni-heidelberg.de>
	<75f0ddef@wiscmail.wisc.edu>
	<C652514F-B972-49DE-81BC-0ADBFD36AFEB@cos.uni-heidelberg.de>
	<4b0909c42b9b27fdb84a5be0b48f742e@wi>
	<7CD33490-933E-4455-805D-11F755A6C170@cos.uni-heidelberg.de>
Message-ID: <1e1d0fea36bf731d5f52f2bb936256c1@wiscmail.wisc.edu>

Hi Alexis,

On Mon, 26 Aug 2013 08:44:26 0200 Alexis Maizel wrote
> The next OpenSPIM update will use a different code base than the current one
> (1.4.x dev)? So unless I commit my update to the MM repository and get them
> part of the nightly build, it won't work, right?
Actually, what I meant was that from now on, the OpenSPIM plugin will try to
interact with the Cobolt device as though it were unchanged -- it will not
treat it as a shutter, and when altering laser power, it will attempt to set
the "Power" property, not "PowerSetpoint" (and so, having the property be named
"PowerSetpoint" even though the laser is a Cobolt will greatly confuse the
plugin). Essentially, the changes I'm making mean that the plugin will interact
with each device adapter differently.

By all means, though, commit your changes -- adapting the plugin to changes in
device adapters is very little work (which is the part of the point of this
update), so if your changes become 'canonical', I can easily change the
SPIMAcquisition plugin to support them.

I'm not planning to update the MM code base just yet -- as far as I am aware,
the nightly build's device adapters still work with the OpenSPIM version.

Thanks,
Luke

On Mon, 26 Aug 2013 08:44:26 0200 Alexis Maizel wrote

> Hi Luke,
> 
> > I should warn you in advance, though I'll try to make it clear when I
> > announce
> > the update: the next update *will* obsolete the changes you made to the
> > Cobalt
> > adapter. The reason is because it will be designed to work with the Cobalt
> > adapter in MM's nightly build -- I'm afraid you will need to revert your
> > changes.
> 
> I am not sure, I am following you here
> The next OpenSPIM update will use a different code base than the current one
> (1.4.x dev)? So unless I commit my update to the MM repository and get them
> part of the nightly build, it won't work, right?
> My amendments to the Cobolt code are now tested and stable, so I feel
> confident to have them added to the repository. I'll talk to Karl Llave.
> 
> With my best regards,
> 
> Alexis





From j.krieger at dkfz-heidelberg.de  Tue Aug 27 08:05:15 2013
From: j.krieger at dkfz-heidelberg.de (Jan Krieger)
Date: Tue, 27 Aug 2013 15:05:15 +0200
Subject: [OpenSPIM] beadscans
Message-ID: <521CA40B.3020606@dkfz-heidelberg.de>

HI everybody!

I put some details on our homepage, how we perform beadscans to look at 
the PSF of our SPIM (we do SPIM-FCS, so that's rather important) in our 
group:

    http://www.dkfz.de/Macromol/quickfit/beadscan.html

We do not have an openSPIM, but a homebuilt, standard-SPIM 
(http://www.dkfz.de/Macromol/research/spim.html), specialized for 
single-cell measurements, but still the script should also work for data 
from an openSPIM.

I also also added a link to the openSPIM wiki.

Best from Heidelberg,
JAN





From stuyvenberg at wisc.edu  Wed Aug 28 08:49:25 2013
From: stuyvenberg at wisc.edu (Luke Stuyvenberg)
Date: Wed, 28 Aug 2013 08:49:25 -0500
Subject: [OpenSPIM] Plugin update
Message-ID: <8e80694b79413fd56a5e25c8259ffeda@wiscmail.wisc.edu>

Hi everyone,

 I've just uploaded a new version of the plugin to the Fiji update site. Some
quick features:

 - Rebuilt device interactions to be less dependent on specific
devices/properties/et cetera
 - Overhauled asynchronous output to be more reliable and transparent
 - Added some rudimentary acquisition profiling, for the curious

 As always, I do my best to make sure that the updates are free of any
use-impairing bugs or issues. However, and especially with this update, this
is not always possible. Therefore, if you have any project-critical imaging to
do, please do not update until you have finished.

 I mention this because this update completely revises the way the plugin
interacts with Micro-Manager devices, letting us easily add new features and
supported hardware. However, this also means that, for certain features, the
plugin now expects a limited subset of devices, and might cause problems with
others -- in particular, anyone not using the OpenSPIM 1.0 specification might
run into trouble.

 On the other hand, if you _don't_ have any project-critical imaging, I would
greatly appreciate it if you could test the plugin on your setup. As I can
personally only test the OpenSPIM 1.0 setup, I don't yet know if the plugin
will operate seamlessly with variants on the setup. If you can, please test
the new plugin and get back to me!

 The rest of this e-mail describes the pros, cons, capabilities, and
limitations of the new code, and might only be of interest if the update is
giving you trouble (or if you're just curious about these sorts of things. ;-)
).

 Thanks,
 Luke Stuyvenberg

-------------------------

The plugin should still work with any hardware compatible with Micro-Manager,
but it will not be able to use any features not provided by the MM API. For
example, it will not be able to determine the maximum position of most stages,
and will be unable to control their velocity, or the laser power of most
lasers. (These will need to be controlled another way, i.e. through the device
property browser in Micro-Manager.)

 _What it _can_ do_: The plugin knows the attributes (minimum, maximum, and
step size) of Picard stages, and can control the Z-stage's velocity. It also
knows the minimum and maximum laser power of Coherent Cube laser devices, and
can control that.

 _What it _should_ be able to do_: Additionally, it should be able to control
the laser power of Cobolt laser devices (although the range is hard-coded to 0
- 50 mW).

 I don't mention cameras because, as yet, the plugin doesn't take advantage of
any camera features not available through the MM API. As a result, it should
be able to use any camera MM can use without trouble.

 If your hardware is having problems with the update, feel free to e-mail the
list with whatever issues you're running into. You can also add support for
your device yourself: download our development environment from
http://openspim.org/How_to_build_the_software -- from there, take a look at
plugins/SPIMAcquisition/src/main/java/spim/setup/PicardStage.java for an
example of the code the new device manager runs on. I only ask that such
daring individuals as do so eventually submit a pull request on github, or
contact the list with their new code, so we can extend this support to
everyone using the software.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130828/29561f6e/attachment-0002.html>
-------------- next part --------------
An embedded and charset-unspecified text was scrubbed...
Name: message_1.1.txt
URL: <http://openspim.org/pipermail/openspim/attachments/20130828/29561f6e/attachment-0002.txt>

From stuyvenberg at wisc.edu  Wed Aug 28 09:02:43 2013
From: stuyvenberg at wisc.edu (Luke Stuyvenberg)
Date: Wed, 28 Aug 2013 09:02:43 -0500
Subject: [OpenSPIM] Plugin update (again)
Message-ID: <521E0303.50704@wisc.edu>

Sorry about the cut-off email; here is (hopefully) the message in its 
entirety:
------------------------------------------------------------------------
Hi everyone,

I've just uploaded a new version of the plugin to the Fiji update site. 
Some quick features:

- Rebuilt device interactions to be less dependent on specific 
devices/properties/et cetera
- Overhauled asynchronous output to be more reliable and transparent
- Added some rudimentary acquisition profiling, for the curious

As always, I do my best to make sure that the updates are free of any 
use-impairing bugs or issues. However, and especially with this update, 
this is not always possible. Therefore, if you have any project-critical 
imaging to do, please do not update until you have finished.

I mention this because this plugin update completely revises the way 
OpenSPIM interacts with Micro-Manager devices, letting us easily add new 
features and supported hardware. However, this also means that, for 
certain features, the plugin now expects a limited subset of devices, 
and might cause problems with others -- in particular, anyone not using 
the OpenSPIM 1.0 specification on the site might run into trouble.

On the other hand, if you /don't/ have any project-critical imaging, I 
would greatly appreciate it if you could test the plugin on your setup. 
As I can personally only test the OpenSPIM 1.0 setup, I don't yet know 
if the plugin will operate seamlessly with variants on the setup. If you 
can, please test the new plugin and get back to me!

The rest of this e-mail describes the pros, cons, capabilities, and 
limitations of the new code, and might only be of interest if the update 
is giving you trouble (or if you're just curious about these sorts of 
things. ;-) ).

Thanks,
Luke Stuyvenberg
------------------------------------------------------------------------
The plugin should still work with any hardware compatible with 
Micro-Manager, but it will not be able to use any features not provided 
by the MM API. For example, it will not be able to determine the maximum 
position of most stages, and will be unable to control their velocity, 
or the laser power of most lasers. (These will need to be controlled 
another way, i.e. through the device property browser in Micro-Manager.)

/What it /can/do/: The plugin knows the attributes (minimum, maximum, 
and step size) of Picard stages, and can control the Z-stage's velocity. 
It also knows the minimum and maximum laser power of Coherent Cube laser 
devices, and can control that.

/What it /should/be able to do/: Additionally, it should be able to 
control the laser power of Cobolt laser devices (although the range is 
hard-coded to 0 - 50 mW).

I don't mention cameras because, as yet, the plugin doesn't take 
advantage of any camera features not available through the MM API. As a 
result, it should be able to use any camera MM can use without trouble.

If your hardware is having problems with the update, feel free to e-mail 
the list with whatever issues you're running into. You can also add 
support for your device yourself: download our development environment 
from http://openspim.org/How_to_build_the_software -- from there, take a 
look at 
plugins/SPIMAcquisition/src/main/java/spim/setup/PicardStage.java for an 
example of the code the new device manager runs on. I only ask that such
daring individuals as do so eventually submit a pull request on github, 
or contact the list with their new code, so we can extend this support 
to everyone using the software.
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://openspim.org/pipermail/openspim/attachments/20130828/6cc9f960/attachment-0002.html>

From j.krieger at dkfz-heidelberg.de  Mon Aug 26 10:56:51 2013
From: j.krieger at dkfz-heidelberg.de (Jan Krieger)
Date: Mon, 26 Aug 2013 17:56:51 +0200
Subject: [OpenSPIM] beadscans
Message-ID: <521B7AC3.4010409@dkfz-heidelberg.de>

HI everybody!

I put some details on our homepage, how we perform beadscans to look at 
the PSF of our SPIM (we do SPIM-FCS, so that's rather important) in our 
group:

   http://www.dkfz.de/Macromol/quickfit/beadscan.html

We do not have an openSPIM, but a homebuilt, standard-SPIM 
(http://www.dkfz.de/Macromol/research/spim.html), specialized for 
single-cell measurements, but still the script should also work for data 
from an openSPIM.

I also also added a link to the openSPIM wiki.

Best from Heidelberg,
JAN

-------------- next part --------------
A non-text attachment was scrubbed...
Name: j_krieger.vcf
Type: text/x-vcard
Size: 408 bytes
Desc: not available
URL: <http://openspim.org/pipermail/openspim/attachments/20130826/53d1728a/attachment-0002.vcf>

From j.krieger at Dkfz-Heidelberg.de  Sat Aug 31 02:22:52 2013
From: j.krieger at Dkfz-Heidelberg.de (Krieger, Jan)
Date: Sat, 31 Aug 2013 09:22:52 +0200
Subject: [OpenSPIM] lasers for SPIM
Message-ID: <4F91222C659D5B4E8C89CE4D710BB08EF1440C604F@DKFZEX01.ad.dkfz-heidelberg.de>

Hi everybody!

as one of our lasers (491nm) died down yesterday, I wanted to ask, which lasers you prefer for the openSPIM:

- So we use Cobolt DPSS lasers, which can not be modulated
- If we can't repair our laser, I thought about getting one of these:
   1. a Cobolt MLD http://www.cobolt.se/coboltmld.html
   2. Coherent Cube http://www.coherent.com/products/?1007/CUBE
   3. Coherent Saphire http://www.coherent.com/products/?1638/Sapphire-Lasers
   4. Coherent OBIS http://www.coherent.com/products/?1884/OBIS-Lasers

I would appreciate any help/hints you can give me on these (and possibly other) lasers, that you think suitable for SPIM.

Best,
JAN


Dipl.-Phys. Jan Krieger
German Cancer Research Center (dkfz)
Department B040 - Biophysics of Macromolecules (Prof. J. Langowski)
Im Neuenheimer Feld 580
69120 Heidelberg

fon: +49 / 6221 / 42-3395
fax: +49 / 6221 / 42-3391
e-mail: j.krieger at dkfz.de
www: http://www.dkfz.de/Macromol/


